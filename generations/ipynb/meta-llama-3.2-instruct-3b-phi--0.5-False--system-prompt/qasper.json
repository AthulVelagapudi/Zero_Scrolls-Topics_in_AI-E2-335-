{
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Unif and Stopword.",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No, the article does not mention the analysis of transformer-based architectures.",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The 8,640 sentences were generated using eleven sentence templates, each with two variables: a person noun phrase and an emotion word. The person noun phrase could be instantiated with ten different noun phrases for females and ten different noun phrases for males, and the emotion word could be one of five words for each of four emotions.",
    "518dae6f936882152c162058895db4eca815e649": "The UTCNN model has three convolutional layers.",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Yes",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "The three levels of the annotation scheme are: \n\n1. Level A: Offensive language detection, distinguishing between offensive (OFF) and non-offensive (NOT) tweets.\n2. Level B: Categorization of Offensive Language, distinguishing between targeted (TIN) and untargeted (UNT) insults and threats, and between targeted insults/threats and general profanity.\n3. Level C: Offensive Language Target Identification, distinguishing between targets as individual (IND), group (GRP), and others (OTH).",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "They show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods.",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "The baseline model was the system that answered \"Yes\" to all questions in the first three test batches.",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "The dataset comprises a total of 353 conversations from 40 speakers, covering 41 hours of data.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "The baseline classification uses support vector machine (SVM) with radial basis function (RBF) kernel.",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "The supervised scores of the words are calculated using the formula: $w_{t} = \\frac{N_{t} + 1}{N + N^{\\prime}}$, where $N_{t}$ is the number of documents in which the word $t$ occurs in the positive polarity corpus, $N$ is the number of words in the positive polarity corpus, $N^{\\prime}_{t}$ is the number of documents in which the word $t$ occurs in the negative polarity corpus, and $N^{\\prime}$ is the number of words in the negative polarity corpus.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "The article introduces several data simulation techniques, including: \n\n1. Back-Translation (BT): generating artificial parallel data by translating monolingual target texts into the source language.\n2. Copy: generating pseudo-source texts by copying target sentences.\n3. Copy-marked: augmenting the source vocabulary with a copy of the target vocabulary, and marking the target words with a special language identifier.\n4. Copy-dummies: replacing words with \"dummy\" tokens.\n5. GANs (Generative Adversarial Networks): using a generator to encode pseudo-source sentences and a discriminator to distinguish between natural and artificial encodings.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "Information extraction-oriented metrics.",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "The article mentions the following linguistics features used:\n\n1. Part-Of-Speech (POS) tags\n2. Gender\n3. Number\n4. Morphological patterns\n5. Affixes\n6. Stem templates\n7. Syntactic dependencies\n8. Word segmentation information\n9. Stem-templates\n10. Prefixes\n11. Suffixes\n12. POS tagging\n13. Syntactic features\n14. Morphological features\n15. Bigram language model\n16. Unigram language model\n17. Named entity recognition (NER)\n18. Indeclinability detection\n19. Dependency parsing",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "The training data was translated using the machine translation platform Apertium.",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "The proposed model is compared to a comprehensive set of baseline models, including traditional models (such as TF-IDF, Naive Bayes, and random forest) and deep models (such as LSTM and CNN).",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "Unanswerable",
    "b970f48d30775d3468952795bc72976baab3438e": "They tackle issues related to the interpretation and validation of results, such as the lack of a clear notion of ground truth, the importance of considering the limitations and biases of computational approaches, and the need for human insight and judgment in the analysis process.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "No"
}