{
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Unif and Stopword.",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No, the article does not analyze transformer-based architectures.",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The 8,640 English sentences were selected based on the following criteria: they involve at least one race- or gender-associated word, are short and grammatically simple, and include expressions of sentiment and emotion.",
    "518dae6f936882152c162058895db4eca815e649": "The text does not explicitly state the number of layers in the UTCNN model.",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "No, the paper does not provide any case studies to illustrate how to use Macaw for CIS research.",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "Level A: Offensive language Detection, \nLevel B: Categorization of Offensive Language, \nLevel C: Offensive Language Target Identification",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods.",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "The baseline model was not explicitly mentioned in the article, but it can be inferred that the baseline model was the one that was used for comparison with the proposed system, which was the BioBERT model fine-tuned on the SQuAD 2.0 dataset.",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "The dataset comprises a total of 353 conversations from 40 speakers.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "The baseline classification uses a combination of n-grams, bag-of-words, common words, and hashtags.",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "The supervised scores of the words are calculated as the minimum, maximum, and average polarity scores of the words occurring in the same contexts as the target word, as well as the self score of the target word itself, using the formula: $w_{t}$ denotes the sentiment score of word $t$, $N_{t}$ is the number of documents (reviews or tweets) in which $t$ occurs in the dataset of positive polarity, $N$ is the number of all the words in the corpus of positive polarity, $N^{\\prime }$ denotes the corpus of negative polarity, $N^{\\prime }_{t}$ and $N^{\\prime }$ denote similar values for the negative polarity corpus.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "The article introduces the following data simulation techniques:\n\n1. Back-Translation (BT): generating artificial parallel data by translating monolingual target texts into the source language.\n2. Copy: copying the target side of the corpus into the source side, with optional noise addition.\n3. Copy-Marked: copying the target side of the corpus into the source side, with the target vocabulary marked to avoid confusion with source vocabulary.\n4. Copy-Dummies: replacing words with \"dummy\" tokens to simulate noisy source sentences.\n5. Generative Adversarial Networks (GANs): using a generator to encode pseudo-source sentences to fool a discriminator trained to distinguish natural from artificial sources.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "Information extraction-oriented metrics.",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "The article mentions the following linguistics features used in the proposed DNN models for recovering core word diacritics and case endings:\n\nFor Core Word Diacritics:\n\n1. CHAR (characters)\n2. SEG (segmentation information)\n3. PRIOR (diacritics seen in the training set per segment)\n4. CASE (whether the letter expects a core word diacritic or a case ending)\n\nFor Case Ending Diacritics:\n\n1. word surface forms\n2. POS (Part-of-Speech) tags\n3. stem templates\n4. surface-level features\n5. morphological features\n6. syntactic features\n7. word and stem leading and trailing character unigrams and bigrams\n8. sukun words and named entities\n\nThese features are used to inform the DNN models and improve the accuracy of diacritics recovery.",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "The training data was translated into Spanish using the machine translation platform Apertium.",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "The proposed model is compared to a base model (LSTM with self-attention) and two other models: ELMo Only (using only the pre-trained ELMo model) and ELMo+Topic (combining the pre-trained ELMo model with the proposed topic-attention model).",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "Unanswerable",
    "b970f48d30775d3468952795bc72976baab3438e": "They tackle issues that are \"thick\" and \"thorny\" at the heart of some of the most pressing contemporary cultural and social issues.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "No, the answers are triple annotated."
}