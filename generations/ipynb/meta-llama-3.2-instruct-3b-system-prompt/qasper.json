{
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Unif and Stopword.",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No, the article does not analyze transformer-based architectures.",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The sentences were selected based on the following criteria: \n\n1. At least one race- or gender-associated word \n2. Short and grammatically simple sentences \n3. Sentences intended to express sentiment and emotion",
    "518dae6f936882152c162058895db4eca815e649": "The text does not explicitly mention the number of layers in the UTCNN model.",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "The paper mentions a Wizard of Oz study setup, which can simulate an ideal CIS system and collect high-quality data from real users for CIS research.",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "The three levels of the annotation scheme are:\n\n1. Level A: Offensive language Detection (OFF vs NOT)\n2. Level B: Categorization of Offensive Language (Targeted (TIN) and untargeted (UNT) insults and threats)\n3. Level C: Offensive Language Target Identification (Individual (IND), group (GRP), and other (OTH))",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "Our models achieved improvements on both the FCE test data and the CoNLL 2014 test data.",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "The baseline model is not explicitly mentioned in the article, but it can be inferred that the baseline model is the system that was used by the organizers of BioASQ for comparison with the authors' system. The authors mention that they started with a simple architecture and baseline performance, and that their system performed better than the baseline model.",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "The dataset comprises a total of 353 conversations from 40 speakers, amounting to 41 hours.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "The baseline classification uses a Support Vector Machine (SVM) with radial basis function kernel.",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "The supervised scores of the words are calculated by scanning through all of its contexts, taking into consideration the target word's polarity score (self score), minimum, maximum, and average scores of words occurring in the same contexts as the target word, using the formula: $w_{t} = \\frac{N_{t}}{N} + \\frac{N^{\\prime}_{t}}{N^{\\prime}}$, where $N_{t}$ and $N^{\\prime}_{t}$ are the number of documents (reviews or tweets) in which $t$ occurs in the dataset of positive and negative polarity, $N$ and $N^{\\prime}$ are the total number of words in the corpus of positive and negative polarity, respectively.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "The article introduces the following data simulation techniques:\n\n1. Back-Translation (BT): generating artificial parallel data by translating monolingual target texts into the source language.\n2. Copy: copying the target-side data into the source-side vocabulary to create pseudo-source sentences.\n3. Copy-marked: augmenting the source vocabulary with a copy of the target vocabulary, ensuring that the target words are marked to avoid confusion with homographic words.\n4. Copy-dummies: replacing words with \"dummy\" tokens to create noisy pseudo-source sentences.\n5. Generative Adversarial Networks (GANs): using a generator to encode pseudo-source sentences and a discriminator to distinguish between natural and artificial sources.\n6. Forward-Translation (FT): generating artificial parallel data by translating monolingual source texts into the target language.\n7. Forward-Translation with noise: adding noise to the target sentences to create more realistic pseudo-source sentences.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "Information extraction-oriented metrics.",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "The article mentions the following linguistics features used in the proposed DNN models:\n\n1. POS (Part-of-Speech) tags\n2. Morphological patterns\n3. Stem templates\n4. Stem-templates\n5. Prefixes\n6. Suffixes\n7. Gender and number features\n8. Case endings (CEs)\n9. Virtual (null) \" #\" marker\n10. Suzun (sukun) words\n11. Named entities (NEs)\n12. Word surface forms\n13. Word-level bigram language model\n14. Unigram language model\n15. Word-level morphological information\n16. Syntactic features\n17. Affixes\n18. Character-level morphological features\n19. Character-level morphological information\n20. Character-level unigrams and bigrams\n\nThese features are used to inform the DNN models for recovering core-word diacritics (CW) and case-endings (CEs) in Arabic text.",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "The training data was translated into Spanish using the machine translation platform Apertium.",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "The proposed model is compared to a base model (LSTM with self-attention) and two other models: ELMo Only (using only the pre-trained ELMo model) and ELMo+Topic (using both the pre-trained ELMo model and the pre-trained Doc2vec model).",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "unanswerable",
    "b970f48d30775d3468952795bc72976baab3438e": "Computational text analysis tackles issues such as identifying patterns that would not otherwise be discernible, shedding light on \"thorny issues\" not always at the forefront of discussions, and providing a set of best practices for working with thick social and cultural concepts.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "No"
}