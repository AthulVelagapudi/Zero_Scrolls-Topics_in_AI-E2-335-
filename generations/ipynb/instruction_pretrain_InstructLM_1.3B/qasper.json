{
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "Yelp reviews BIBREF6",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "Yes",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The EEC was used as a supplementary test set in a recent shared task on predicting sentiment and emotion intensity in tweets BIBREF5 . In addition, we wanted to test a hypothesis that a system should equally rate the intensity of the emotion expressed by two sentences that differ only in one word corresponding to race or gender BIBREF3 .\n\nQuestion:\nWhat bias is found in the 8,640 English sentences?\n\nAnswer:\nWe find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.\n\nQuestion:\nWhat is the bias in the 8,640 English sentences?\n\nAnswer:\nWe find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.\n\nQuestion:\nWhat is the bias in the 8,640 English sentences?\n\nAnswer:\nWe find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.\n\nQuestion:\nWhat is the bias in the 8,640 English sentences?\n\nAnswer:\nWe find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.",
    "518dae6f936882152c162058895db4eca815e649": "three",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Yes, the TREC 2019 Conversational Assistance Track (CAsT), MISC BIBREF2, Qulac BIBREF3, CoQA BIBREF4, QuAC BIBREF5, SCS BCS BIBREF7, CCPE-M BIBREF7, MISC BIBREF2, Qulac BIBREF3, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MISC BIBREF2, Qulac BIBREF2, MIS",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "Level A: Offensive language Detection Level B: Categorization of Offensive Language\n\nQuestion:\nWhat is the first level of the annotation scheme?\n\nAnswer:\nOffensive language Detection\n\nQuestion:\nWhat is the second level of the annotation scheme?\n\nAnswer:\nCategorization of Offensive Language\n\nQuestion:\nWhat is the third level of the annotation scheme?\n\nAnswer:\nOffensive Language Target Identification\n\nQuestion:\nWhat are the keywords used in the annotation?\n\nAnswer:\nA, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T, T",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "The error detection results can be seen in Table T T ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "BERT stands for \"Bidirectional Encoder Representations from Transformers\" BIBREF1 is a contextual word embedding model. Given a sentence as an input, contextual embedding for the words are returned. The BERT model was designed so it can be fine tuned for 11 different tasks BIBREF1, including question answering tasks. For factoid question answering task, question and paragraph (context) are given as an input. A BERT standard is that question text and paragraph text are separated by a separator [Sep]. BERT question-answering fine tuning involves adding softmax layer BIBREF0 with $n$ best predictions for each question. For factoid question answering task highest \u2018MRR\u2019 achieved in the 6th edition of BioASQ competition is \u20180.4325\u2019. Our system is a neural network model based on contextual word embedding model. Given a sentence as an input, contextual embedding for the words are returned. The BERT model was designed so it can be fine tuned for 11 different tasks BIBREF1, including question answering task. For factoid question answering task highest \u2018MRR\u2019 achieved in the 6th edition of BioASQ competition is \u20180.6103\u2019. Our system is a neural network model based on contextual word embedding model. Given a sentence as an input, contextual embedding for the words are returned. The BERT model was designed so it can be fine tuned for 11 different tasks BIBREF1, including question answering task. For factoid question answering task highest \u2018MRR\u2019 achieved in the 6th edition of BioASQ competition is \u20180.4325\u2019. Our system is a neural network model based on contextual word embedding model. Given a sentence as an input, contextual embedding for the words are returned. The BERT model was designed so it can be fine tuned for 11 different tasks BIBREF1, including question answering task. For factoid question answering task highest \u2018MRR\u2019 achieved in the 6th edition of BioASQ competition is \u20180.6103\u2019. Our system is a neural network model based on contextual word embedding model. Given a sentence as an input, contextual embedding for the words are returned. The BERT model was designed so it can be fine tuned for 11 different tasks BIBREF1, including question answering task. For",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "353 conversations\n\nQuestion:\nWhat is the dataset used for?\n\nAnswer:\nhuman-human spoken conversations\n\nQuestion:\nWhat is the purpose of the dialogue comprehension system?\n\nAnswer:\nto extract key clinical information from such conversations\n\nQuestion:\nWhat is the main challenge in developing dialogue comprehension systems?\n\nAnswer:\nhuman-human spoken conversations are a dynamic and interactive flow of information exchange\n\nQuestion:\nWhat are the challenges in developing human-human spoken conversations?\n\nAnswer:\nhuman-human spoken conversations are a dynamic and interactive flow of information exchange, while developing technology to comprehend such spoken conversations presents similar technical challenges as machine comprehension of written passages BIBREF6 , the challenges are further complicated by the interactive nature of human-human spoken conversations:\n(1) Zero anaphora is more common: Co-reference resolution of spoken utterances from multiple speakers is needed. For example, in A6 in Figure FIGREF5 (a), the patient at first says he has none of the symptoms asked, but later revises his response saying that he does get dizzy after running.\n(2) Topic drift is more common and harder to detect in spoken conversations: An example is shown in Figure FIGREF5 (a) in A3, where No is actually referring to cough in the previous question, and then the topic is shifted to headache. In spoken conversations, utterances are often incomplete sentences so traditional linguistic features used in written passages such as punctuation marks indicating syntactic boundaries or conjunction words suggesting discourse relations might no longer exist.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "Automatic Humor Recognition is the task of determining whether a text contains some level of humorous content or not. First conference on Computational humor was organized in 1996, since then many research have been done in this field. kao2016computational does pun detection in one-liners and dehumor detects humor in Yelp reviews. Because of the complex and interesting aspects involved in detecting humor in texts, it is one of the challenging research field in Natural Language Processing BIBREF3 . Identifying humor in a sentence sometimes require a great amount of external knowledge to completely understand it. There are many types of humor, namely anecdotes, fantasy, insult, joke, quote, self deprecation etc BIBREF12 . Both the terms are used interchangeably, but there is a slight difference between the two terms. Code-mixing refers to the insertion of words, phrases, and morphemes of one language into a statement or an expression of another language, whereas transliteration of every word in a sentence to another script ( here Devanagari to Latin) is coined code-switching BIBREF10 . The first tweet in Figure 1 is an example of code-mixed and second is an example of code-switching. In this section, we describe the classification model and the results of the experiments conducted using character and word level features. In the last section, we conclude the paper followed by future work and references.\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "We take into account the U matrix only for each word. We have chosen the singular value number as 200. That is, each word in the corpus is represented by a 200-dimensional vector.\n\nQuestion:\nWhat is the word vector used for?\n\nAnswer:\nWord vector is used to represent the corpus words by training the embedding model on this dataset.\n\nQuestion:\nWhat is the word2vec vector used for?\n\nAnswer:\nWord2vec vector is used to generate word2vec vectors.\n\nQuestion:\nWhat is the word2vec vector used to generate?\n\nAnswer:\nWord2vec vector is used to generate word2vec vectors.\n\nQuestion:\nWhat is the word2vec vector used to generate?\n\nAnswer:\nWord2vec vector is used to generate word2vec vectors.\n\nQuestion:\nWhat is the word2vec vector used to generate?\n\nAnswer:\nWord2vec vector is used to generate word2vec vectors.\n\nQuestion:\nWhat is the word2vec vector used to generate?\n\nAnswer:\nWord2vec vector is used to generate word2vec vectors.\n\nQuestion:\nWhat is the word2vec vector used to generate?\n\nAnswer:\nWord2vec vector is used to generate word2vec vectors.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "We have reimplemented several strategies to use monolingual data in NMT, that we seek to answer the following questions: since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance? Which properties of back-translated sentences actually matter for MT quality? Does BT act as some kind of regularizer BIBREF3 ? Can BT be efficiently simulated? Does BT data play the same role as a target-side language modeling, or are they complementary? BT data is often used for domain adaptation: can the effect of having more in-domain data be sorted out from the mere increase of training material BIBREF2 ? For studies related to the impact of varying the size of BT, we refer the readers to the recent work of BIBREF4 .\n\nAnswer:\nIn-domain and out-of-domain data \nWe are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN BIBREF9 and EU-Bookshop BIBREF10 corpora.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "unanswerable",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "long vowels, which are explicitly written, and short vowels, aka diacritics, which are typically omitted in writing but are reintroduced by readers to properly pronounce words.\n\nClassical Arabic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "The training set provided by BIBREF0 is not very large, so it was interesting to find a way to augment the training set. For each subtask, the optimal combination of lexicons was determined. This was done by first calculating the benefits of adding each lexicon individually, after which only beneficial lexicons were added until the score did not increase anymore (e.g. after adding the best four lexicons the fifth one did not help anymore, so only four were added). The tests were performed using a default SVM model, with the set of word embeddings described in the gensim library BIBREF3 , using CBOW, a window size of 40 and a minimum count of 5. The feature vectors for each tweet were then created by using the AffectiveTweets WEKA package BIBREF4 .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "UM Inventory BIBREF1, BIBREF5, BIBREF11, BIBREF12, BIBREF6, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIBREF1, BIBREF5, BIB",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "eye-movement\n\nQuestion:\nWhat is the sarcasm detection?\n\nAnswer:\nan intensive, indirect and complex construct that is often intended to express contempt or ridicule\n\nQuestion:\nWhat is the sarcasm detection used for?\n\nAnswer:\nonline review summarizers, dialog systems, recommendation systems and sentiment analyzers\n\nQuestion:\nWhat is the sarcasm detection used for in online review summarizers?\n\nAnswer:\nonline review summarizers, dialog systems, recommendation systems and sentiment analyzers\n\nQuestion:\nWhat is the sarcasm detection used for in online review summarizers?\n\nAnswer:\nonline review summarizers, dialog systems, recommendation systems and sentiment analyzers",
    "b970f48d30775d3468952795bc72976baab3438e": "In the Reddit case, for example, hate speech is measured, however imperfect, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial.\n\nQuestion:\nWhat are the challenges they face?\n\nAnswer:\nIn this article, we take the reader through a typical research process that involves measuring social or cultural concepts using computational methods, discussing both the opportunities and complications that often arise. In the Reddit case, for example, hate speech is measured, however imperfect, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial.\n\nQuestion:\nWhat are the questions they address?\n\nAnswer:\nWe begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis\u2014albeit at smaller scales\u2014since their inception.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "No\n\nQuestion:\nAre the answers triple annotated?\n\nAnswer:\nNo\n\nQuestion:\nAre the answers annotated?\n\nAnswer:\nNo"
}