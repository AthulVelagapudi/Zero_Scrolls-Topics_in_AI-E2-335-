{
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "The baselines used are 52.16% and 77.37% retention rates.",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "No, the article does not mention transformer-based architectures.",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The 8,640 English sentences were compiled from the Equity Evaluation Corpus, which is a dataset used for evaluating bias in natural language processing systems.",
    "518dae6f936882152c162058895db4eca815e649": "The article does not explicitly state the number of layers in the UTCNN model.",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Yes",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "The three levels of the annotation scheme are: \n\n1. Insult\n2. Untargeted\n3. Profanity",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "Their results show improved performance on the CoNLL 2014 dataset, with 41% error detection accuracy, and no results are mentioned for the FCE test dataset.",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "The baseline model was BERT.",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "The dataset consists of 353 conversations.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "The baseline classification uses a radial basis kernel extra tree classifier.",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "The supervised scores of the words are calculated by multiplying the polarity scores of each word with its co-occurrence matrix, and then normalizing the result using a formula that takes into account the number of documents and the number of positive and negative reviews.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "The article mentions the use of pseudo-source data, artificial data, and simulated data, as well as Generative Adversarial Networks (GANs) and pseudo-source embeddings, to simulate monolingual data for training and testing NMT systems.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "BLEU, RG, and CS.",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "Arabic morphological, syntactic, semantic, and lexical features.",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "The training data was translated from Spanish to English for the English version of the AffectiveTweets corpus, and from English to Spanish for the Spanish version of the AffectiveTweets corpus.",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "The proposed model is compared to TF-IDF, Naive Bayes, random forest, and baseline models using Doc2vec and Gensim embeddings.",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "Lexical features, semantic incongruity, and readability.",
    "b970f48d30775d3468952795bc72976baab3438e": "They tackle issues related to the limitations and challenges of computational text analysis, such as the need for more nuanced understanding of social and cultural contexts, the importance of human interpretation and expertise, and the potential for biases in machine learning models, as well as issues related to data quality, annotation, and the interpretation of results.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "No"
}