{
    "3fad42be0fb2052bb404b989cc7d58b440cd23a0": "The baselines used are Unif and Stopword. Unif randomly keeps tokens with probability \u03b4, while Stopword keeps all tokens except those labeled as stopwords. Both baselines are optimized using gradient descent to minimize the reconstruction error.",
    "8bf7f1f93d0a2816234d36395ab40c481be9a0e0": "Unanswerable. The passage discusses LSTM-based models such as InferSent, SSE, PWIM, DecAtt, and ESIM, but does not mention any analysis of Transformer-based architectures.",
    "0f12dc077fe8e5b95ca9163cea1dd17195c96929": "The 8,640 English sentences were carefully chosen to tease out biases towards certain races and genders. They were selected from eleven sentence templates designed to cover both emotional and neutral sentences, ensuring grammatical simplicity and including expressions of sentiment and emotion. The templates were constructed after multiple discussions and consensus-building sessions, incorporating specific nouns related to race and gender from a list developed by the researchers. The selection process aimed to create a balanced dataset that could effectively measure bias in sentiment analysis systems.",
    "518dae6f936882152c162058895db4eca815e649": "Three convolutional layers are applied on the concatenated transformed word embeddings in the UTCNN model. \n\nExplanation: According to the UTCNN model description provided in the article, the model consists of three convolutional layers. Each convolutional layer processes the concatenated transformed word embeddings, resulting in the extraction of unigram, bigram, and trigram n-gram representations. After the convolutional layers, an average pooling layer is applied to combine the max-pooled outputs, leading to the final stance label prediction.",
    "58ef2442450c392bfc55c4dc35f216542f5f2dbb": "Unanswerable\n\nThe provided text does not mention any specific case studies illustrating how Macaw can be used for CIS research. Instead, it focuses on describing the capabilities and benefits of Macaw without detailing any particular applications or examples.",
    "290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30": "The three levels of the annotation model are:\n\n1. **Level A**: Offensive language Detection\n   - Discriminates between offensive (OFF) and non-offensive (NOT) tweets.\n\n2. **Level B**: Categorization of Offensive Language\n   - Categorizes the type of offense and uses two labels: targeted (TIN) and untargeted (INT) insults and threats.\n\n3. **Level C**: Offensive Language Target Identification\n   - Categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH).\n\nThese three levels together provide a comprehensive framework for annotating offensive content across different dimensions.",
    "ab9b0bde6113ffef8eb1c39919d21e5913a05081": "Improvements were observed across both datasets, with the pattern-based method consistently outperforming the system by Felice2014a and combining the pattern-based method with the machine translation approach giving further substantial improvements and achieving the best performance on all datasets.",
    "ff338921e34c15baf1eae0074938bf79ee65fdd2": "unanswerable\n\nExplanation:\nThe article does not provide enough information to determine what the baseline model was. The text discusses various experiments and improvements made to the system, but does not specify which model served as the baseline.",
    "1b1a30e9e68a9ae76af467e60cefb180d135e285": "Their created dataset consists of 353 conversations from 40 speakers.",
    "dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596": "Support Vector Machine with Radial Basis Function Kernel.",
    "3355918bbdccac644afe441f085d0ffbbad565d7": "Supervised scores of words are calculated using the formula provided in the text, involving the counts of positive and negative occurrences within the context of each word, normalized against the total count of positive and negative words in the corpus. Specifically, for each word \\( t \\):\n\n\\[ w_t = \\frac{N_t}{N} \\]\n\\[ N' = \\frac{N'}{N'} \\]\n\nwhere \\( N_t \\) is the number of positive reviews containing \\( t \\), \\( N \\) is the total number of positive reviews, and \\( N' \\) is the number of negative reviews containing \\( t \\). Similarly:\n\n\\[ w'_t = \\frac{N'^{-}}{N'} \\]\n\\[ N'' = \\frac{N''}{N''} \\]\n\nwhere \\( N'^{-} \\) is the number of negative reviews containing \\( t \\).\n\nThese scores are derived from the training data and provide a measure of the sentiment associated with each word, facilitating the creation of more meaningful and discriminative word embeddings.",
    "d9980676a83295dda37c20cfd5d58e574d0a4859": "Back-translation (BT) and generative adversarial networks (GANs) were introduced as data simulation techniques. Specifically:\n\n1. **Back-translation** (BT):\n   - Introduced by BIBREF2, BT involves translating text from one language to another and then translating it back to the original language. This creates synthetic parallel data that can be used to train neural machine translation (NMT) models.\n   - BT is computationally expensive but has shown promise in improving translation quality.\n\n2. **Generative Adversarial Networks (GANs)**:\n   - Proposed by BIBREF28, GANs are a class of machine learning algorithms that consist of two neural networks: a generator and a discriminator. The generator creates synthetic data that the discriminator tries to distinguish from real data.\n   - In the context of NMT, GANs are used to simulate monolingual data by encoding and decoding sequences to create a dataset that mimics natural language patterns.\n\nThese techniques were introduced to address the limitations of traditional monolingual data and to enhance the performance of NMT models by providing more varied and structured training data.",
    "79a44a68bb57b375d8a57a0a7f522d33476d9f33": "Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. \n\nNote: The question asks specifically about qualitative metrics used for evaluation, and the answer provides the name of the metric (\"Content Selection\") along with a brief explanation of what it measures. The provided answer is concise and directly addresses the question asked.",
    "76ed74788e3eb3321e646c48ae8bf6cdfe46dca1": "Surface level features, morphological features, and syntactic features are used. \n\nThis answer summarizes the key linguistic features discussed in the article, focusing on the surface level features (characters), morphological features (stem templates, affixes), and syntactic features (POS tagging, gender and number features, morphological patterns, and affixes). These features are described as contributing to improving diacritization performance.",
    "8e52637026bee9061f9558178eaec08279bf7ac6": "All tweets from the English datasets were translated into Spanish. This new set of \"Spanish\" data was then added to our original training set.",
    "3116453e35352a3a90ee5b12246dc7f2e60cfc59": "The proposed model is compared with several baseline models, including traditional models like naive bayes, logistic regression, and support vector machines, as well as deep models like convolutional neural networks and recurrent neural networks. It also compares against a few-shot learning approach that incorporates topic information and contextualized word embeddings. Additionally, it contrasts with state-of-the-art models like BioBERT and BERT.",
    "4e748cb2b5e74d905d9b24b53be6cfdf326e8054": "They used traditional linguistic features including unigrams, pragmatic features, stylistic patterns, and Hastag interpretations. Additionally, they incorporated readability and word count features.",
    "b970f48d30775d3468952795bc72976baab3438e": "They tackle thorny issues not always at the forefront of discussions about computational text analysis methods. Specifically, they discuss:\n\n1. Cultural and social nuances that are hard to incorporate in computational approaches due to subtleties of meaning and interpretation.\n2. Highly contested concepts like hate speech, which can raise serious concerns about conceptual validity and may lead to shallow or obvious conclusions instead of deep findings.\n\nAdditionally, they mention that computational approaches are not a panacea and face significant challenges in addressing these issues effectively. They emphasize the importance of bridging interdisciplinary insights and partnerships to realize the full potential of computational text analysis involving social and cultural concepts. \n\nSo, the kind of issues they tackle are those that are deeply rooted in cultural and social realities but are often overlooked or misunderstood in computational text analysis. They aim to bring these issues to the forefront by providing insights and best practices for working with such concepts.",
    "c70bafc35e27be9d1efae60596bc0dd390c124c0": "Unanswerable\n\nExplanation:\nThe passage mentions that the answers are formulated by domain experts with legal training, but it doesn't specify if they are double or triple annotated. Therefore, the correct answer is \"unanswerable.\" \n\nNote: The term \"double\" refers to the fact that the answers are formulated by domain experts, not that they are double-annotated. The latter would require specifying the exact number of annotators involved in the process. Since the question specifically asks about the number of annotations, and the passage doesn't provide this information, the answer remains \"unanswerable.\""
}