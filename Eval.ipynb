{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/athul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/athul/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/athul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, remove_stopwords=False):\n",
    "    \"\"\"Normalize text by lowercasing, removing punctuation, and optionally removing stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(reference, hypothesis):\n",
    "    \"\"\"Compute ROUGE-1, ROUGE-2, and ROUGE-L scores.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    for key in hypothesis.keys():\n",
    "        result = scorer.score(reference[key], hypothesis[key])\n",
    "        scores[\"rouge1\"].append(result[\"rouge1\"].fmeasure)\n",
    "        scores[\"rouge2\"].append(result[\"rouge2\"].fmeasure)\n",
    "        scores[\"rougeL\"].append(result[\"rougeL\"].fmeasure)\n",
    "\n",
    "    Rouge1 = sum(scores[\"rouge1\"]) / len(scores[\"rouge1\"])\n",
    "    Rouge2 = sum(scores[\"rouge2\"]) / len(scores[\"rouge2\"])\n",
    "    RougeL = sum(scores[\"rougeL\"]) / len(scores[\"rougeL\"])\n",
    "\n",
    "    Rouge_geo = (Rouge1 * Rouge2 * RougeL) ** (1/3)\n",
    "    \n",
    "    return {\"Rouge1\": Rouge1, \"Rouge2\": Rouge2, \"RougeL\": RougeL, \"Rouge_geo\": Rouge_geo}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(reference, hypothesis):\n",
    "    \"\"\"Compute F1 score (unigram overlap without stopwords).\"\"\"\n",
    "    scores = []\n",
    "    for key in hypothesis.keys():\n",
    "        ref_tokens = set(reference[key].split())\n",
    "        hyp_tokens = set(hypothesis[key].split())\n",
    "        common = ref_tokens & hyp_tokens\n",
    "        num_same = len(common)\n",
    "        if num_same == 0:\n",
    "            scores.append(0.0)\n",
    "        else:\n",
    "            precision = num_same / len(hyp_tokens)\n",
    "            recall = num_same / len(ref_tokens)\n",
    "            scores.append(2 * (precision * recall) / (precision + recall))\n",
    "    return {\"f1_score\":sum(scores) / len(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(reference, hypothesis):\n",
    "    \"\"\"Compute Exact Match score across all samples.\"\"\"\n",
    "    scores = []\n",
    "    for key in hypothesis.keys():\n",
    "        ref_text = reference[key]\n",
    "        hyp_text = hypothesis[key]\n",
    "        scores.append(1.0 if normalize_text(ref_text, remove_stopwords=True) == normalize_text(hyp_text, remove_stopwords=True) else 0.0)\n",
    "    return {\"EM_score\" : sum(scores) / len(scores)} if scores else {\"EM_score\" : 0.0}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scores(reference_file, hypothesis_file, metric):\n",
    "    with open(reference_file, 'r') as ref_f, open(hypothesis_file, 'r') as hyp_f:\n",
    "        references = json.load(ref_f)\n",
    "        hypotheses = json.load(hyp_f)\n",
    "    \n",
    "    results = 0\n",
    "    if metric == \"rouge\":\n",
    "        results = compute_rouge(references, hypotheses)\n",
    "    elif metric == \"f1\":\n",
    "        results = compute_f1(references, hypotheses)\n",
    "    elif metric == \"exact_match\":\n",
    "        results = compute_exact_match(references, hypotheses)\n",
    "            \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/assets/models/meta-llama-3.2-instruct-3b gov_report\n",
      "/assets/models/meta-llama-3.2-instruct-3b summ_screen_fd\n",
      "/assets/models/meta-llama-3.2-instruct-3b qmsum\n",
      "/assets/models/meta-llama-3.2-instruct-3b qasper\n",
      "/assets/models/meta-llama-3.2-instruct-3b narrative_qa\n",
      "/assets/models/meta-llama-3.2-instruct-3b quality\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"gov_report\", \"summ_screen_fd\", \"qmsum\", \"qasper\",\"narrative_qa\", \"quality\"]\n",
    "model_names = [\"/assets/models/meta-llama-3.2-instruct-3b\"]#[\"Qwen/Qwen2.5-1.5B-Instruct\",\"MBZUAI/LaMini-GPT-1.5B\",\"instruction-pretrain/InstructLM-1.3B\",\"nvidia/AceInstruct-1.5B\",\"/assets/models/meta-llama-3.2-instruct-3b\"]\n",
    "ipynb = \"generations/ipynb\"\n",
    "metrics = {\"gov_report\":\"rouge\", \"summ_screen_fd\":\"rouge\", \"qmsum\":\"rouge\", \"qasper\":\"f1\",\"narrative_qa\":\"f1\", \"quality\":\"exact_match\"}\n",
    "\n",
    "\n",
    "\n",
    "for model in model_names:\n",
    "    output_file = os.path.join(f\"Evaluation_results\", model.replace(\"/\", \"_\").replace(\"-\", \"_\"), \"results.json\")\n",
    "    results = {}\n",
    "    for dataset in datasets:\n",
    "        reference_file = f\"generations/ipynb/Input_output_json/output_{dataset}.json\"\n",
    "        hypothesis_file = os.path.join(ipynb, model.replace(\"/\", \"_\").replace(\"-\", \"_\"), f\"{dataset}.json\")\n",
    "        result = evaluate_scores(reference_file, hypothesis_file,metrics[dataset])\n",
    "        results[dataset] = result\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)    \n",
    "    \n",
    "        with open(output_file, 'w') as out_f:\n",
    "            json.dump(results, out_f, indent=4)\n",
    "        print(model, dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scrolls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
