{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoConfig\n",
    "from transformers import set_seed as hf_set_seed\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "from wtpsplit import SaT\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"gov_report\", \"summ_screen_fd\",\n",
    "            \"qmsum\", \"qasper\", \"narrative_qa\", \"quality\"]\n",
    "\n",
    "\n",
    "chunking = os.getenv(\"CHUNKING\", \"False\").lower() == \"true\"\n",
    "\n",
    "if chunking:\n",
    "    print(\"Using chunking\")\n",
    "\n",
    "CHUNK_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "max_examples_per_task = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_to_max_input_tokens = {\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": 8192,\n",
    "    \"MBZUAI/LaMini-GPT-1.5B\" : 512,\n",
    "    \"/assets/models/meta-llama-2-chat-7b\" : 8192,\n",
    "    \"instruction-pretrain/InstructLM-1.3B\":2048,\n",
    "    \"nvidia/AceInstruct-1.5B\": 8192,\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": 8192 * 32   \n",
    "}\n",
    "\n",
    "def llama3_prompt(user_message):\n",
    "    BEGIN = \"<|begin_of_text|>\"\n",
    "    START = \"<|start_header_id|>\"\n",
    "    END = \"<|end_header_id|>\"\n",
    "    EOT = \"<|eot_id|>\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Always follow the task instruction carefully. \"\n",
    "        \"The first paragraph before the first double line break contains the task instruction. \"\n",
    "        \"Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}system{END}\\n\\n{system_prompt}{EOT}\\n\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model_to_chat_template = {\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": llama3_prompt \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = SaT(\"sat-3l\")\n",
    "sat.half().to(\"cuda\")\n",
    "\n",
    "model_name = \"/assets/models/meta-llama-3.2-instruct-3b\"\n",
    "\n",
    "print(\"Loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"Loading model: {model_name}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_input_length = model_to_max_input_tokens[model_name]\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.rope_scaling = {\n",
    "    \"factor\": 64.0,\n",
    "    \"high_freq_factor\": 8.0,\n",
    "    \"low_freq_factor\": 1.0,\n",
    "    \"original_max_position_embeddings\": 8192,\n",
    "    \"rope_type\": \"llama3\"\n",
    "}\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", config=config, torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# summary_model_name = \"google/flan-t5-xl\"\n",
    "# summary_tokenizer = AutoTokenizer.from_pretrained(summary_model_name)\n",
    "# summary_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     summary_model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "summary_model = model\n",
    "summary_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(text, chunksize=CHUNK_SIZE):\n",
    "    sentences = sat.split(text)\n",
    "    segments = []\n",
    "    for i in range(0, len(sentences), chunksize):\n",
    "        segment = \" \".join(sentences[i:i + chunksize])\n",
    "        segments.append(segment.strip())\n",
    "    # Remove empty segments\n",
    "    segments = [s for s in segments if s]\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "def batch_concise_rewrite_chunks(chunks, query = None, min_length=50):\n",
    "    # Tokenize each chunk individually to count tokens\n",
    "    token_lens = [len(summary_tokenizer(\n",
    "        chunk, return_tensors=\"pt\").input_ids[0]) for chunk in chunks]\n",
    "\n",
    "    to_rewrite = []\n",
    "    indices = []\n",
    "    passthrough = {}\n",
    "\n",
    "    for i, (chunk, tok_len) in enumerate(zip(chunks, token_lens)):\n",
    "        if tok_len < min_length:\n",
    "            passthrough[i] = chunk  # Skip rewriting\n",
    "        else:\n",
    "            indices.append(i)\n",
    "            if query:\n",
    "                prompt = f\"Make the following text shorter while preserving information that is relevant to the following query.\\n\\nQuery: {query}.\\n\\nText: {chunk}\\n\\nShort version:\"\n",
    "            else:\n",
    "                prompt = f\"Make the following text shorter while preserving its core meaning.\\n\\nText: {chunk}\\n\\nShort version:\"\n",
    "            prompt = model_to_chat_template.get(model_name, lambda x: x)(prompt)\n",
    "            \n",
    "            to_rewrite.append(prompt)\n",
    "\n",
    "    rewritten_chunks = [\"\"] * len(chunks)\n",
    "\n",
    "    if to_rewrite:\n",
    "        prompt_lens = [len(summary_tokenizer(\n",
    "            p, return_tensors=\"pt\").input_ids[0]) for p in to_rewrite]\n",
    "        max_new_tokens = max(prompt_lens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = summary_tokenizer(\n",
    "                to_rewrite, return_tensors=\"pt\", padding=True).to(device)\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            outputs = summary_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.1,\n",
    "                early_stopping=True,\n",
    "                num_beams=2,\n",
    "                eos_token_id=summary_tokenizer.eos_token_id,\n",
    "                pad_token_id=summary_tokenizer.pad_token_id,\n",
    "            )\n",
    "            compressed = summary_tokenizer.batch_decode(\n",
    "                outputs[:, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            compressed = [text for text in compressed]\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            rewritten_chunks[idx] = compressed[i]\n",
    "\n",
    "    for i, original in passthrough.items():\n",
    "        rewritten_chunks[i] = original\n",
    "\n",
    "   \n",
    "    return rewritten_chunks\n",
    "\n",
    "\n",
    "\n",
    "def process_model_input_chunking(tokenizer, example, max_tokens, device, dataset):\n",
    "\n",
    "    instruction = example[\"input\"][:example['document_start_index']]\n",
    "    truncation_seperator = example['truncation_seperator']\n",
    "\n",
    "    query = example[\"input\"][example['query_start_index']:]\n",
    "    if len(query) == 0:\n",
    "        query = None\n",
    "    doc = example[\"input\"][example['document_start_index']\n",
    "        :example['document_end_index']]\n",
    "\n",
    "    # Apply semantic chunking\n",
    "    chunks = extract_segments(doc)\n",
    "\n",
    "    # Compress in batches of 5 in parallel\n",
    "    compressed_chunks = []\n",
    "    for i in range(0, len(chunks), BATCH_SIZE):\n",
    "        batch = chunks[i:i + BATCH_SIZE]\n",
    "        compressed_batch = batch_concise_rewrite_chunks(batch, query)\n",
    "        compressed_chunks.extend(compressed_batch)\n",
    "\n",
    "\n",
    "    compressed_doc = \"\\n\".join(compressed_chunks)\n",
    "\n",
    "    # Compute ratio of compressed doc to original doc\n",
    "    ratio = len(compressed_doc) / len(doc)\n",
    "    print(f\"Compression ratio: {ratio:.2f}\")\n",
    "    if ratio >= 0.95:\n",
    "        print(\"Compression ratio is too high, skipping compression\")\n",
    "        compressed_doc = doc\n",
    "\n",
    "    input_text = f\"{instruction}{compressed_doc}{truncation_seperator}{query or ''}\"\n",
    "\n",
    "    compressed_input = model_to_chat_template.get(model_name, lambda x: x)(input_text)\n",
    "\n",
    "    # Write compressed input to file\n",
    "    with open(f\"compressed_input/{dataset}/{example[\"id\"]}.txt\", \"w\") as f:\n",
    "        f.write(compressed_input)\n",
    "\n",
    "    # Write original input to file\n",
    "    with open(f\"original_input/{dataset}/{example[\"id\"]}.txt\", \"w\") as f:\n",
    "        f.write(example[\"input\"])\n",
    "\n",
    "    tokenized_input_full = tokenizer(\n",
    "        compressed_input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokenized_input_full\n",
    "\n",
    "\n",
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    instruction = example[\"input\"][:example['document_start_index']]\n",
    "    truncation_seperator = example['truncation_seperator']\n",
    "\n",
    "    query = example[\"input\"][example['query_start_index']:]\n",
    "    if len(query) == 0:\n",
    "        query = None\n",
    "    doc = example[\"input\"][example['document_start_index']\n",
    "        :example['document_end_index']]\n",
    "    \n",
    "    input_text = f\"{instruction}{doc}{truncation_seperator}{query or ''}\"\n",
    "    input = model_to_chat_template.get(model_name, lambda x: x)(input_text)\n",
    "\n",
    "    tokenized_input = tokenizer(\n",
    "        input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_dir = \"generations/ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "hf_set_seed(seed)\n",
    "print(\"Params:\")\n",
    "print(f\"model: {model_name}\")\n",
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "if chunking:\n",
    "    model_suffix = f\"{model_suffix}-chunking\"\n",
    "generations_dir = os.path.join(generations_dir, model_suffix)\n",
    "print(f\"generations_dir: {generations_dir}\")\n",
    "print(f\"max_examples_per_task: {max_examples_per_task}\")\n",
    "print(\"=\" * 50)\n",
    "time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(f\"time as start: {time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "\n",
    "mid_layer_index = 16\n",
    "print(f\"model loaded!, device:{model.device}\")\n",
    "\n",
    "print(\"Will write to:\", generations_dir)\n",
    "os.makedirs(generations_dir, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    generations = dict()\n",
    "    print(f\"Processing {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time as start {dataset}: {time}\")\n",
    "    print(f\"Loading {dataset}\")\n",
    "    data = load_dataset(\"tau/zero_scrolls\", dataset, trust_remote_code=True)\n",
    "    print(f\"Loaded {dataset}\")\n",
    "    # Create dir compressed_input if it doesn't exist\n",
    "    compressed_dir = os.path.join(\"compressed_input\", dataset)\n",
    "    original_dir = os.path.join(\"original_input\", dataset)\n",
    "    if not os.path.exists(compressed_dir):\n",
    "        os.makedirs(compressed_dir)\n",
    "    if not os.path.exists(original_dir):\n",
    "        os.makedirs(original_dir)\n",
    "\n",
    "    for i, example in tqdm(enumerate(data[\"validation\"])):\n",
    "        print(\"Processing example:\", example[\"id\"])\n",
    "\n",
    "        if 0 < max_examples_per_task == i:\n",
    "            print(f\"Reached {max_examples_per_task} for {dataset}. Breaking\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            if chunking:\n",
    "                model_input = process_model_input_chunking(\n",
    "                    tokenizer, example, max_input_length, device, dataset)\n",
    "            else:\n",
    "                model_input = process_model_input(\n",
    "                    tokenizer, example, max_input_length, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i} in {dataset}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Get hidden states from the 16th layer\n",
    "        with torch.no_grad():\n",
    "            prediction_token_ids = model.generate(model_input,\n",
    "                                                  max_new_tokens=512,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_p=0.9,\n",
    "                                                  top_k=0,\n",
    "                                                  temperature=0.5,\n",
    "                                                  pad_token_id=tokenizer.eos_token_id,\n",
    "                                                  )\n",
    "\n",
    "            predicted_text = tokenizer.decode(\n",
    "                prediction_token_ids[0][model_input.shape[1]:], skip_special_tokens=True)\n",
    "            del prediction_token_ids, model_input\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        generations[example[\"id\"]] = predicted_text\n",
    "\n",
    "    out_file_path_pred = os.path.join(generations_dir, f\"{dataset}.json\")\n",
    "    with open(out_file_path_pred, 'w') as f_out:\n",
    "        json.dump(generations, f_out, indent=4)\n",
    "\n",
    "    print(f\"Done generating {len(generations)} examples from {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time at end: {time}\")\n",
    "    print(f\"Look for predictions in {generations_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeroscrolls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
