{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers import set_seed as hf_set_seed\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "from wtpsplit import SaT\n",
    "from tqdm.notebook import tqdm\n",
    "from llmlingua import PromptCompressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8d15bfda0c4fce80cf8a7ae6af06f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "# compressor = PromptCompressor(\n",
    "#     model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
    "#     use_llmlingua2=True\n",
    "# )\n",
    "# use_llmlingua = True  # Toggle compressor on/off\n",
    "\n",
    "compressor = PromptCompressor(\"microsoft/phi-2\")\n",
    "use_llmlingua = True\n",
    "\n",
    "# llm_lingua_model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# compressor = PromptCompressor(\n",
    "#     model_name=\"/assets/models/meta-llama-3.2-instruct-3b\",\n",
    "#     use_llmlingua2=False,        # disable llmlingua-2 to use coarse-to-fine pipeline\n",
    "#     device_map=\"auto\",          # distribute across available devices\n",
    "# )\n",
    "# use_llmlingua = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using chunking with LLMLingua default compressor\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"gov_report\", \"summ_screen_fd\",\n",
    "            \"qmsum\", \"qasper\", \"narrative_qa\", \"quality\"]\n",
    "\n",
    "\n",
    "chunking = True\n",
    "system_prompt = True\n",
    "\n",
    "if chunking:\n",
    "    print(\"Using chunking with LLMLingua default compressor\")\n",
    "\n",
    "CHUNK_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "max_examples_per_task = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_to_max_input_tokens = {\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": 8192,\n",
    "    \"MBZUAI/LaMini-GPT-1.5B\" : 512,\n",
    "    \"/assets/models/meta-llama-2-chat-7b\" : 8192,\n",
    "    \"instruction-pretrain/InstructLM-1.3B\":2048,\n",
    "    \"nvidia/AceInstruct-1.5B\": 8192,\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": 8192 * 32   \n",
    "}\n",
    "\n",
    "def llama3_prompt(user_message):\n",
    "    BEGIN = \"<|begin_of_text|>\"\n",
    "    START = \"<|start_header_id|>\"\n",
    "    END = \"<|end_header_id|>\"\n",
    "    EOT = \"<|eot_id|>\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Always follow the task instruction carefully. \"\n",
    "        \"The first paragraph before the first double line break contains the task instruction. \"\n",
    "        \"Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.\"\n",
    "    )\n",
    "\n",
    "    if system_prompt:\n",
    "        prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}system{END}\\n\\n{system_prompt}{EOT}\\n\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    else:\n",
    "        prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model_to_chat_template = {\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": llama3_prompt \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Loading model: /assets/models/meta-llama-3.2-instruct-3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538d0bb2aa55468ca81b0213050c2184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sat = SaT(\"sat-3l\")\n",
    "sat.half().to(\"cuda\")\n",
    "\n",
    "model_name = \"/assets/models/meta-llama-3.2-instruct-3b\"\n",
    "\n",
    "print(\"Loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"Loading model: {model_name}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_input_length = model_to_max_input_tokens[model_name]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# summary_model_name = \"google/flan-t5-xl\"\n",
    "# summary_tokenizer = AutoTokenizer.from_pretrained(summary_model_name)\n",
    "# summary_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     summary_model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "summary_model = model\n",
    "summary_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(text, chunksize=CHUNK_SIZE):\n",
    "    sentences = sat.split(text)\n",
    "    segments = []\n",
    "    for i in range(0, len(sentences), chunksize):\n",
    "        segment = \" \".join(sentences[i:i + chunksize])\n",
    "        segments.append(segment.strip())\n",
    "    # Remove empty segments\n",
    "    segments = [s for s in segments if s]\n",
    "\n",
    "    return segments\n",
    "\n",
    "def batch_concise_rewrite_chunks(chunks, query=None, min_length=50):\n",
    "    # (keep your existing code for computing token_lens, to_rewrite, passthrough, etc.)\n",
    "    token_lens = [\n",
    "        len(tokenizer(chunk, return_tensors=\"pt\").input_ids[0])\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    to_rewrite = []\n",
    "    indices = []\n",
    "    passthrough = {}\n",
    "    for i, (chunk, tok_len) in enumerate(zip(chunks, token_lens)):\n",
    "        if tok_len < min_length:\n",
    "            passthrough[i] = chunk\n",
    "        else:\n",
    "            indices.append(i)\n",
    "            to_rewrite.append(chunk)\n",
    "\n",
    "    rewritten_chunks = [\"\"] * len(chunks)\n",
    "\n",
    "    if to_rewrite:\n",
    "        # ← HERE: call your PromptCompressor instead of HF generate()\n",
    "        # I’m assuming its API looks like: compress_batch(texts, query=None) -> List[str]\n",
    "        compressed_texts = llm_lingua.compress_batch(to_rewrite, query=query)\n",
    "\n",
    "        for k, idx in enumerate(indices):\n",
    "            rewritten_chunks[idx] = compressed_texts[k]\n",
    "\n",
    "    # put back any chunks we skipped\n",
    "    for i, original in passthrough.items():\n",
    "        rewritten_chunks[i] = original\n",
    "\n",
    "    return rewritten_chunks\n",
    "\n",
    "\n",
    "\n",
    "def process_model_input_chunking_withoutlingua(tokenizer, example, max_tokens, device, dataset):\n",
    "\n",
    "    instruction = example[\"input\"][:example['document_start_index']]\n",
    "    truncation_seperator = example['truncation_seperator']\n",
    "\n",
    "    query = example[\"input\"][example['query_start_index']:]\n",
    "    if len(query) == 0:\n",
    "        query = None\n",
    "    doc = example[\"input\"][example['document_start_index']\n",
    "        :example['document_end_index']]\n",
    "\n",
    "    # Apply semantic chunking\n",
    "    chunks = extract_segments(doc)\n",
    "\n",
    "    # Compress in batches of 5 in parallel\n",
    "    compressed_chunks = []\n",
    "    for i in range(0, len(chunks), BATCH_SIZE):\n",
    "        batch = chunks[i:i + BATCH_SIZE]\n",
    "        compressed_batch = batch_concise_rewrite_chunks(batch, query)\n",
    "        compressed_chunks.extend(compressed_batch)\n",
    "\n",
    "\n",
    "    compressed_doc = \"\\n\".join(compressed_chunks)\n",
    "\n",
    "    # Compute ratio of compressed doc to original doc\n",
    "    ratio = len(compressed_doc) / len(doc)\n",
    "    print(f\"Compression ratio: {ratio:.2f}\")\n",
    "    if ratio >= 0.95:\n",
    "        print(\"Compression ratio is too high, skipping compression\")\n",
    "        compressed_doc = doc\n",
    "\n",
    "    input_text = f\"{instruction}{compressed_doc}{truncation_seperator}{query or ''}\"\n",
    "\n",
    "    compressed_input = model_to_chat_template.get(model_name, lambda x: x)(input_text)\n",
    "\n",
    "    # Write compressed input to file\n",
    "    with open(f\"compressed_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(compressed_input)\n",
    "\n",
    "    # Write original input to file\n",
    "    with open(f\"original_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(example[\"input\"])\n",
    "\n",
    "    tokenized_input_full = tokenizer(\n",
    "        compressed_input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokenized_input_full\n",
    "\n",
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    instr = example['input'][:example['document_start_index']]\n",
    "    sep = example['truncation_seperator']\n",
    "    query = example['input'][example['query_start_index']:] or ''\n",
    "    doc = example['input'][example['document_start_index']:example['document_end_index']]\n",
    "    prompt = f\"{instr}{doc}{sep}{query}\"\n",
    "    chat = model_to_chat_template.get(model_name, lambda x:x)(prompt)\n",
    "    return tokenizer(chat, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_chunks_llmlingua(chunks, max_ratio=None):\n",
    "    \"\"\"\n",
    "    Compresses a list of text chunks using LLMLingua, but only if\n",
    "    the compressed length is within the specified max_ratio of the original.\n",
    "    If the ratio is exceeded, returns the original chunk.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): List of text segments to compress.\n",
    "        max_ratio (float or None): Maximum allowed compressed/original token ratio.\n",
    "            e.g., 0.5 means compressed chunk must be <= 50% of original tokens.\n",
    "            If None, no ratio enforcement.\n",
    "    \"\"\"\n",
    "    compressed = []\n",
    "    for chunk in chunks:\n",
    "        # Count original tokens (simple whitespace split)\n",
    "        orig_tokens = len(chunk.split())\n",
    "        target = orig_tokens // 2\n",
    "        # Perform LLMLingua compression\n",
    "        res = compressor.compress_prompt(\n",
    "            chunk,\n",
    "            instruction=\"\",\n",
    "            question=\"\",\n",
    "            target_token=target\n",
    "        )\n",
    "        short = res.get('compressed_prompt', chunk)\n",
    "        # Enforce max_ratio if provided\n",
    "        if max_ratio is not None and orig_tokens > 0:\n",
    "            comp_tokens = len(short.split())\n",
    "            if comp_tokens / orig_tokens > max_ratio:\n",
    "                # Skip compression if ratio too high\n",
    "                compressed.append(chunk)\n",
    "                continue\n",
    "        compressed.append(short)\n",
    "    return compressed\n",
    "\n",
    "# Build model input when chunking\n",
    "def process_model_input_chunking(tokenizer, example, max_tokens, device, dataset):\n",
    "    instr = example['input'][:example['document_start_index']]\n",
    "    sep = example['truncation_seperator']\n",
    "    query = example['input'][example['query_start_index']:] or ''\n",
    "    doc = example['input'][example['document_start_index']:example['document_end_index']]\n",
    "\n",
    "    # Split into segments and compress each chunk, enforcing a max ratio\n",
    "    chunks = extract_segments(doc)\n",
    "    if use_llmlingua:\n",
    "        compressed_chunks = compress_chunks_llmlingua(chunks, max_ratio=0.95)\n",
    "    else:\n",
    "        compressed_chunks = chunks\n",
    "\n",
    "    # Join compressed chunks with newlines\n",
    "    compressed_doc = \"\".join(compressed_chunks)\n",
    "\n",
    "    # Build the prompt by combining instruction, compressed doc, separator, and query\n",
    "    prompt = f\"{instr}{compressed_doc}{sep}{query}\"\n",
    "    chat = model_to_chat_template.get(model_name, lambda x: x)(prompt)\n",
    "\n",
    "    # Save compressed and original inputs for inspection\n",
    "    os.makedirs(f\"compressed_input/{dataset}\", exist_ok=True)\n",
    "    with open(f\"compressed_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(chat)\n",
    "    os.makedirs(f\"original_input/{dataset}\", exist_ok=True)\n",
    "    with open(f\"original_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(example['input'])\n",
    "\n",
    "    # Tokenize and return the input ids for generation\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "    return inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_dir = \"generations/ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:\n",
      "model: /assets/models/meta-llama-3.2-instruct-3b\n",
      "generations_dir: generations/ipynb/meta-llama-3.2-instruct-3b-phi-.5--system-prompt\n",
      "max_examples_per_task: -1\n",
      "==================================================\n",
      "time as start: 26_04_2025_12_34_51\n"
     ]
    }
   ],
   "source": [
    "llm_name = \"phi-.5\"\n",
    "seed = 43\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "hf_set_seed(seed)\n",
    "print(\"Params:\")\n",
    "print(f\"model: {model_name}\")\n",
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "model_suffix = f\"{model_suffix}-{llm_name}\"\n",
    "if system_prompt:\n",
    "    model_suffix = f\"{model_suffix}--system-prompt\"\n",
    "generations_dir = os.path.join(generations_dir, model_suffix)\n",
    "print(f\"generations_dir: {generations_dir}\")\n",
    "print(f\"max_examples_per_task: {max_examples_per_task}\")\n",
    "print(\"=\" * 50)\n",
    "time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(f\"time as start: {time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!, device:cuda:0\n",
      "Will write to: generations/ipynb/meta-llama-3.2-instruct-3b-phi-.5--system-prompt\n",
      "Processing narrative_qa\n",
      "time as start narrative_qa: 26_04_2025_12_34_52\n",
      "Loading narrative_qa\n",
      "Loaded narrative_qa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95838833d3774295bde13bf9a2a6e5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3858\n",
      "Processing example: 5947\n",
      "Processing example: 5947\n",
      "Processing example: 12164\n",
      "Processing example: 12164\n",
      "Processing example: 23148\n",
      "Processing example: 23148\n",
      "Processing example: 25278\n",
      "Processing example: 25278\n",
      "Processing example: 33068\n",
      "Processing example: 33068\n",
      "Processing example: 35134\n",
      "Processing example: 35134\n",
      "Processing example: 38322\n",
      "Processing example: 38322\n",
      "Processing example: 42586\n",
      "Processing example: 42586\n",
      "Processing example: 16886\n",
      "Processing example: 16886\n",
      "Done generating 10 examples from narrative_qa\n",
      "time at end: 26_04_2025_12_49_16\n",
      "Look for predictions in generations/ipynb/meta-llama-3.2-instruct-3b-phi-.5--system-prompt\n",
      "Processing quality\n",
      "time as start quality: 26_04_2025_12_49_16\n",
      "Loading quality\n",
      "Loaded quality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a48c23673b945a2a24a39f0b69cb04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 62139_J05FWZR6_1\n",
      "Processing example: 52855_MV65I88C_9\n",
      "Processing example: 62085_C1SL2YBE_3\n",
      "Processing example: 63616_MQ1O9T2Q_6\n",
      "Processing example: 63833_V187YO4H_2\n",
      "Processing example: 63392_7YS4HHFI_6\n",
      "Processing example: 63473_1VIHQ8TY_4\n",
      "Processing example: 51650_B3KKWWD1_7\n",
      "Processing example: 51274_8Q2YNHG5_6\n",
      "Processing example: 20077_ZF5G55FD_1\n",
      "Processing example: 22579_RQ3GB4A1_3\n",
      "Processing example: 22867_TJ9SPIHC_9\n",
      "Processing example: 22875_L821878U_6\n",
      "Processing example: 22967_0XT2L7PI_7\n",
      "Processing example: 22867_IZGAWLCJ_4\n",
      "Processing example: 22462_BUA2LH2S_5\n",
      "Processing example: 31736_TV0CUXDH_4\n",
      "Processing example: 99927_EVLEI3Q2_6\n",
      "Processing example: 31282_BQYW9TCH_4\n",
      "Processing example: 99914_0Q5X8VEX_4\n",
      "Processing example: 32665_VRYQXG3Y_9\n",
      "Done generating 21 examples from quality\n",
      "time at end: 26_04_2025_12_51_50\n",
      "Look for predictions in generations/ipynb/meta-llama-3.2-instruct-3b-phi-.5--system-prompt\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "\n",
    "mid_layer_index = 16\n",
    "print(f\"model loaded!, device:{model.device}\")\n",
    "\n",
    "print(\"Will write to:\", generations_dir)\n",
    "os.makedirs(generations_dir, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    generations = dict()\n",
    "    print(f\"Processing {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time as start {dataset}: {time}\")\n",
    "    print(f\"Loading {dataset}\")\n",
    "    data = load_dataset(\"tau/zero_scrolls\", dataset, trust_remote_code=True)\n",
    "    print(f\"Loaded {dataset}\")\n",
    "    # Create dir compressed_input if it doesn't exist\n",
    "    compressed_dir = os.path.join(\"compressed_input\", dataset)\n",
    "    original_dir = os.path.join(\"original_input\", dataset)\n",
    "    if not os.path.exists(compressed_dir):\n",
    "        os.makedirs(compressed_dir)\n",
    "    if not os.path.exists(original_dir):\n",
    "        os.makedirs(original_dir)\n",
    "\n",
    "    for i, example in tqdm(enumerate(data[\"validation\"])):\n",
    "        print(\"Processing example:\", example[\"id\"])\n",
    "\n",
    "        if 0 < max_examples_per_task == i:\n",
    "            print(f\"Reached {max_examples_per_task} for {dataset}. Breaking\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            if chunking:\n",
    "                model_input = process_model_input_chunking(\n",
    "                    tokenizer, example, max_input_length, device, dataset)\n",
    "            else:\n",
    "                model_input = process_model_input(\n",
    "                    tokenizer, example, max_input_length, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i} in {dataset}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Get hidden states from the 16th layer\n",
    "        with torch.no_grad():\n",
    "            prediction_token_ids = model.generate(model_input,\n",
    "                                                  max_new_tokens=512,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_p=0.9,\n",
    "                                                  top_k=0,\n",
    "                                                  temperature=0.5,\n",
    "                                                  pad_token_id=tokenizer.eos_token_id,\n",
    "                                                  )\n",
    "\n",
    "            predicted_text = tokenizer.decode(\n",
    "                prediction_token_ids[0][model_input.shape[1]:], skip_special_tokens=True)\n",
    "            del prediction_token_ids, model_input\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        generations[example[\"id\"]] = predicted_text\n",
    "\n",
    "    out_file_path_pred = os.path.join(generations_dir, f\"{dataset}.json\")\n",
    "    with open(out_file_path_pred, 'w') as f_out:\n",
    "        json.dump(generations, f_out, indent=4)\n",
    "\n",
    "    print(f\"Done generating {len(generations)} examples from {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time at end: {time}\")\n",
    "    print(f\"Look for predictions in {generations_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
