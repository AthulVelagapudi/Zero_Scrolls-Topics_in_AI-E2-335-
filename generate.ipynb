{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4591569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athul/Scrolls/zero_scrolls/env_scrolls/lib/python3.10/site-packages/torch/cuda/memory.py:489: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "/home/athul/Scrolls/zero_scrolls/env_scrolls/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import set_seed as hf_set_seed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_infinite_root = os.path.abspath('..')\n",
    "\n",
    "# Prepend it so Python finds 'models/llama.py' first\n",
    "sys.path.insert(0, lm_infinite_root)\n",
    "\n",
    "# Now you can import the converter\n",
    "from models.llama import convert_llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee836408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_prompt(user_message):\n",
    "    BEGIN = \"<|begin_of_text|>\"\n",
    "    START = \"<|start_header_id|>\"\n",
    "    END = \"<|end_header_id|>\"\n",
    "    EOT = \"<|eot_id|>\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"Always follow the task instruction carefully.\"\n",
    "        \"The first paragraph before the first double line break contains the task instruction.\"\n",
    "        \"Generate text as a natural continuation of the user message.\"\n",
    "        \"Do not include any meta-commentary or explanations or your own thoughts.\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}system{END}\\n\\n{system_prompt}{EOT}\\n\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model_to_chat_template = {\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": llama3_prompt \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2993c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =[\"gov_report\", \"summ_screen_fd\", \"qmsum\", \"qasper\",\"narrative_qa\", \"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1568739",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_max_input_tokens = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebfa16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa5a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/assets/models/meta-llama-3.2-instruct-3b\"\n",
    "model_print_name = \"llama-basic_4096\"\n",
    "max_examples_per_task = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c2548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    instruction = example[\"input\"][:example['document_start_index']]\n",
    "    truncation_seperator = example['truncation_seperator']\n",
    "\n",
    "    query = example[\"input\"][example['query_start_index']:]\n",
    "    if len(query) == 0:\n",
    "        query = None\n",
    "    doc = example[\"input\"][example['document_start_index']\n",
    "        :example['document_end_index']]\n",
    "    \n",
    "    input_text = f\"{instruction}{doc}{truncation_seperator}{query or ''}\"\n",
    "    input = model_to_chat_template.get(model_name, lambda x: x)(input_text)\n",
    "    # print(f\"Input: {input}\")\n",
    "    tokenized_input = tokenizer(\n",
    "        input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed80d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:\n",
      "model: /assets/models/meta-llama-3.2-instruct-3b\n",
      "generations_dir: generations/ipynb/llama-basic_4096\n",
      "max_examples_per_task: -1\n",
      "==================================================\n",
      "time as start: 27_04_2025_21_40_14\n",
      "Loading tokenizer\n",
      "Loading model: /assets/models/meta-llama-3.2-instruct-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
    "\n",
    "generations_dir = \"generations/ipynb\"\n",
    "seed = 43\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "hf_set_seed(seed)\n",
    "print(\"Params:\")\n",
    "print(f\"model: {model_name}\")\n",
    "generations_dir = os.path.join(generations_dir, model_print_name.replace(\"/\", \"_\"))\n",
    "print(f\"generations_dir: {generations_dir}\")\n",
    "print(f\"max_examples_per_task: {max_examples_per_task}\")\n",
    "print(\"=\" * 50)\n",
    "time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(f\"time as start: {time}\")\n",
    "\n",
    "print(\"Loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"Loading model: {model_name}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_input_length = model_to_max_input_tokens\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_flash_attention_2=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model.past_key_values = None\n",
    "\n",
    "#model = convert_llama_model(model, local_branch=8192, global_branch=256, safe_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86112e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ") model loaded!, device:cuda:0\n",
      "Will write to: generations/ipynb/llama-basic_4096\n",
      "Processing gov_report\n",
      "time as start gov_report: 27_04_2025_21_40_16\n",
      "Loading gov_report\n",
      "Loaded gov_report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/athul/Scrolls/zero_scrolls/env_scrolls/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/athul/Scrolls/zero_scrolls/env_scrolls/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R45461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:17, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R44668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:29, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R45546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:45, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R45732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:01, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R45237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:11, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_RS21212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:24, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_RL30478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:56, 19.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_RL32665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:23, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_RS22373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:59, 26.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-19-207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [03:10, 21.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:28, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-19-220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [03:37, 16.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-19-148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [03:48, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [04:07, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [04:23, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [04:39, 16.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-17-781T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [04:49, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [05:03, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [05:23, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: gao_GAO-18-403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [05:34, 16.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 20 examples from gov_report\n",
      "time at end: 27_04_2025_21_45_54\n",
      "Look for predictions in generations/ipynb/llama-basic_4096\n",
      "Processing summ_screen_fd\n",
      "time as start summ_screen_fd: 27_04_2025_21_45_54\n",
      "Loading summ_screen_fd\n",
      "Loaded summ_screen_fd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_FRIENDS_07x04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Buffy_the_Vampire_Slayer_04x14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:23, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_The_Office_05x24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:28,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Angel_03x07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:35,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_The_Office_03x20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:39,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Doctor_Who_1963_14x21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:44,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Frasier_07x04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:50,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Frasier_10x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:55,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Justified_06x12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:01,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Charmed_06x13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:07,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Pretty_Little_Liars_01x03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:13,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Buffy_the_Vampire_Slayer_06x14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:23,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Justified_04x01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:27,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Doctor_Who_01x02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:36,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_The_Office_08x23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:41,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Justified_06x11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:52,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_CSI__Crime_Scene_Investigation_06x02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [02:00,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_Alias_01x04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [02:06,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_The_Vampire_Diaries_01x11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [02:12,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_The_O.C._03x01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [02:28,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 20 examples from summ_screen_fd\n",
      "time at end: 27_04_2025_21_48_26\n",
      "Look for predictions in generations/ipynb/llama-basic_4096\n",
      "Processing qmsum\n",
      "time as start qmsum: 27_04_2025_21_48_26\n",
      "Loading qmsum\n",
      "Loaded qmsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:09,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:12,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:21,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-gq-66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:28,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-gq-93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:39,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:48,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:57,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:02,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:03,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-gq-166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:13,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:16,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:23,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:31,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:33,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:35,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:47,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:54,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:59,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [02:03,  6.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 20 examples from qmsum\n",
      "time at end: 27_04_2025_21_50_32\n",
      "Look for predictions in generations/ipynb/llama-basic_4096\n",
      "Processing qasper\n",
      "time as start qasper: 27_04_2025_21_50_32\n",
      "Loading qasper\n",
      "Loaded qasper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3fad42be0fb2052bb404b989cc7d58b440cd23a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3fad42be0fb2052bb404b989cc7d58b440cd23a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 0f12dc077fe8e5b95ca9163cea1dd17195c96929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 0f12dc077fe8e5b95ca9163cea1dd17195c96929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 518dae6f936882152c162058895db4eca815e649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 58ef2442450c392bfc55c4dc35f216542f5f2dbb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:07,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 58ef2442450c392bfc55c4dc35f216542f5f2dbb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:08,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:09,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: ab9b0bde6113ffef8eb1c39919d21e5913a05081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: ff338921e34c15baf1eae0074938bf79ee65fdd2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:12,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 1b1a30e9e68a9ae76af467e60cefb180d135e285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:13,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:14,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:15,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3355918bbdccac644afe441f085d0ffbbad565d7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:19,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: d9980676a83295dda37c20cfd5d58e574d0a4859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:22,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: d9980676a83295dda37c20cfd5d58e574d0a4859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:26,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 79a44a68bb57b375d8a57a0a7f522d33476d9f33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:27,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 79a44a68bb57b375d8a57a0a7f522d33476d9f33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:28,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 76ed74788e3eb3321e646c48ae8bf6cdfe46dca1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:33,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 8e52637026bee9061f9558178eaec08279bf7ac6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:33,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 8e52637026bee9061f9558178eaec08279bf7ac6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:34,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3116453e35352a3a90ee5b12246dc7f2e60cfc59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:36,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3116453e35352a3a90ee5b12246dc7f2e60cfc59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:38,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 4e748cb2b5e74d905d9b24b53be6cfdf326e8054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: b970f48d30775d3468952795bc72976baab3438e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:40,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: c70bafc35e27be9d1efae60596bc0dd390c124c0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:41,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 19 examples from qasper\n",
      "time at end: 27_04_2025_21_51_15\n",
      "Look for predictions in generations/ipynb/llama-basic_4096\n",
      "Processing narrative_qa\n",
      "time as start narrative_qa: 27_04_2025_21_51_15\n",
      "Loading narrative_qa\n",
      "Loaded narrative_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:10,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:22,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:34,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 12164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:14, 20.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 12164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:55, 27.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 23148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:58, 19.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 23148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:02, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 25278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:03, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 25278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:04,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 33068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:21, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 33068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:38, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 35134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:42,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 35134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:45,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 38322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [02:47,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 38322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [02:50,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 42586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [02:52,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 42586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [02:54,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 16886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [03:03,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 16886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [03:11,  9.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 10 examples from narrative_qa\n",
      "time at end: 27_04_2025_21_54_29\n",
      "Look for predictions in generations/ipynb/llama-basic_4096\n",
      "Processing quality\n",
      "time as start quality: 27_04_2025_21_54_29\n",
      "Loading quality\n",
      "Loaded quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 62139_J05FWZR6_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 52855_MV65I88C_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 62085_C1SL2YBE_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 63616_MQ1O9T2Q_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 63833_V187YO4H_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 63392_7YS4HHFI_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 63473_1VIHQ8TY_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:06,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 51650_B3KKWWD1_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:06,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 51274_8Q2YNHG5_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:07,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 20077_ZF5G55FD_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 22579_RQ3GB4A1_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:08,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 22867_TJ9SPIHC_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 22875_L821878U_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:10,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 22967_0XT2L7PI_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:10,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 22867_IZGAWLCJ_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:11,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 22462_BUA2LH2S_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:12,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 31736_TV0CUXDH_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:13,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 99927_EVLEI3Q2_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:13,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 31282_BQYW9TCH_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 99914_0Q5X8VEX_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:15,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 32665_VRYQXG3Y_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:15,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 21 examples from quality\n",
      "time at end: 27_04_2025_21_54_47\n",
      "Look for predictions in generations/ipynb/llama-basic_4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "\n",
    "print(f\"{model} model loaded!, device:{model.device}\")\n",
    "\n",
    "print(\"Will write to:\", generations_dir)\n",
    "os.makedirs(generations_dir, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    generations = dict()\n",
    "    input_task = dict()\n",
    "    output_task = dict()\n",
    "    print(f\"Processing {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time as start {dataset}: {time}\")\n",
    "    print(f\"Loading {dataset}\")\n",
    "    data = load_dataset(\"tau/zero_scrolls\", dataset, cache_dir=\"/home/athul/datasets_cache\")\n",
    "    print(f\"Loaded {dataset}\")\n",
    "\n",
    "    for i, example in tqdm(enumerate(data[\"validation\"])):\n",
    "        print(\"Processing example:\", example[\"id\"])\n",
    "\n",
    "        if 0 < max_examples_per_task == i:\n",
    "            print(f\"Reached {max_examples_per_task} for {dataset}. Breaking\")\n",
    "            break\n",
    "\n",
    "        model_input = process_model_input(tokenizer, example, max_input_length, device)\n",
    "\n",
    "        prediction_token_ids = model.generate(model_input,\n",
    "                                                  max_new_tokens=512,\n",
    "                                                  do_sample=False,\n",
    "                                                  top_p=0,\n",
    "                                                  top_k=0,\n",
    "                                                  temperature=1,\n",
    "                                                  pad_token_id=tokenizer.eos_token_id, )\n",
    "        model.past_key_values = None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        predicted_text = tokenizer.decode(prediction_token_ids[0][model_input.shape[1]:], skip_special_tokens=True)\n",
    "        generations[example[\"id\"]] = predicted_text\n",
    "        input_task[example[\"id\"]] = example[\"input\"]\n",
    "        output_task[example[\"id\"]] = example[\"output\"]\n",
    "        #break\n",
    "\n",
    "    out_file_path_pred = os.path.join(generations_dir, f\"{dataset}.json\")\n",
    "    with open(out_file_path_pred, 'w') as f_out:\n",
    "        json.dump(generations, f_out, indent=4)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Done generating {len(generations)} examples from {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time at end: {time}\")\n",
    "    print(f\"Look for predictions in {generations_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8bf65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
