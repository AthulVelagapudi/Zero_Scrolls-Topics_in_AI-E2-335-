{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers import set_seed as hf_set_seed\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "from wtpsplit import SaT\n",
    "from tqdm.notebook import tqdm\n",
    "from llmlingua import PromptCompressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4827e36149450289b43cf650e5ae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "# compressor = PromptCompressor(\n",
    "#     model_name=''#\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
    "#     use_llmlingua2=True\n",
    "# )\n",
    "# use_llmlingua = True  # Toggle compressor on/off\n",
    "\n",
    "# llm_lingua = PromptCompressor(\"microsoft/phi-2\")\n",
    "\n",
    "# llm_lingua_model_name = \"microsoft/phi-2\"\n",
    "\n",
    "compressor = PromptCompressor(\n",
    "    model_name=\"/assets/models/meta-llama-3.2-instruct-3b\",\n",
    "    use_llmlingua2=False,        # disable llmlingua-2 to use coarse-to-fine pipeline\n",
    "    device_map=\"auto\",          # distribute across available devices\n",
    ")\n",
    "use_llmlingua = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using chunking with LLMLingua default compressor\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"gov_report\", \"summ_screen_fd\",\n",
    "            \"qmsum\", \"qasper\", \"narrative_qa\", \"quality\"]\n",
    "\n",
    "\n",
    "chunking = True\n",
    "system_prompt = True\n",
    "\n",
    "if chunking:\n",
    "    print(\"Using chunking with LLMLingua default compressor\")\n",
    "\n",
    "CHUNK_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "max_examples_per_task = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_to_max_input_tokens = {\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": 8192,\n",
    "    \"MBZUAI/LaMini-GPT-1.5B\" : 512,\n",
    "    \"/assets/models/meta-llama-2-chat-7b\" : 8192,\n",
    "    \"instruction-pretrain/InstructLM-1.3B\":2048,\n",
    "    \"nvidia/AceInstruct-1.5B\": 8192,\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": 8192 * 32   \n",
    "}\n",
    "\n",
    "def llama3_prompt(user_message):\n",
    "    BEGIN = \"<|begin_of_text|>\"\n",
    "    START = \"<|start_header_id|>\"\n",
    "    END = \"<|end_header_id|>\"\n",
    "    EOT = \"<|eot_id|>\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Always follow the task instruction carefully. \"\n",
    "        \"The first paragraph before the first double line break contains the task instruction. \"\n",
    "        \"Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.\"\n",
    "    )\n",
    "\n",
    "    if system_prompt:\n",
    "        prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}system{END}\\n\\n{system_prompt}{EOT}\\n\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    else:\n",
    "        prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model_to_chat_template = {\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": llama3_prompt \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Loading model: /assets/models/meta-llama-3.2-instruct-3b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3d64cb79f6480aa736d77950f3a0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sat = SaT(\"sat-3l\")\n",
    "sat.half().to(\"cuda\")\n",
    "\n",
    "model_name = \"/assets/models/meta-llama-3.2-instruct-3b\"\n",
    "\n",
    "print(\"Loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"Loading model: {model_name}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_input_length = model_to_max_input_tokens[model_name]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# summary_model_name = \"google/flan-t5-xl\"\n",
    "# summary_tokenizer = AutoTokenizer.from_pretrained(summary_model_name)\n",
    "# summary_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     summary_model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "summary_model = model\n",
    "summary_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(text, chunksize=CHUNK_SIZE):\n",
    "    sentences = sat.split(text)\n",
    "    segments = []\n",
    "    for i in range(0, len(sentences), chunksize):\n",
    "        segment = \" \".join(sentences[i:i + chunksize])\n",
    "        segments.append(segment.strip())\n",
    "    # Remove empty segments\n",
    "    segments = [s for s in segments if s]\n",
    "\n",
    "    return segments\n",
    "\n",
    "def batch_concise_rewrite_chunks(chunks, query=None, min_length=50):\n",
    "    # (keep your existing code for computing token_lens, to_rewrite, passthrough, etc.)\n",
    "    token_lens = [\n",
    "        len(tokenizer(chunk, return_tensors=\"pt\").input_ids[0])\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    to_rewrite = []\n",
    "    indices = []\n",
    "    passthrough = {}\n",
    "    for i, (chunk, tok_len) in enumerate(zip(chunks, token_lens)):\n",
    "        if tok_len < min_length:\n",
    "            passthrough[i] = chunk\n",
    "        else:\n",
    "            indices.append(i)\n",
    "            to_rewrite.append(chunk)\n",
    "\n",
    "    rewritten_chunks = [\"\"] * len(chunks)\n",
    "\n",
    "    if to_rewrite:\n",
    "        # ← HERE: call your PromptCompressor instead of HF generate()\n",
    "        # I’m assuming its API looks like: compress_batch(texts, query=None) -> List[str]\n",
    "        compressed_texts = llm_lingua.compress_batch(to_rewrite, query=query)\n",
    "\n",
    "        for k, idx in enumerate(indices):\n",
    "            rewritten_chunks[idx] = compressed_texts[k]\n",
    "\n",
    "    # put back any chunks we skipped\n",
    "    for i, original in passthrough.items():\n",
    "        rewritten_chunks[i] = original\n",
    "\n",
    "    return rewritten_chunks\n",
    "\n",
    "\n",
    "\n",
    "def process_model_input_chunking_withoutlingua(tokenizer, example, max_tokens, device, dataset):\n",
    "\n",
    "    instruction = example[\"input\"][:example['document_start_index']]\n",
    "    truncation_seperator = example['truncation_seperator']\n",
    "\n",
    "    query = example[\"input\"][example['query_start_index']:]\n",
    "    if len(query) == 0:\n",
    "        query = None\n",
    "    doc = example[\"input\"][example['document_start_index']\n",
    "        :example['document_end_index']]\n",
    "\n",
    "    # Apply semantic chunking\n",
    "    chunks = extract_segments(doc)\n",
    "\n",
    "    # Compress in batches of 5 in parallel\n",
    "    compressed_chunks = []\n",
    "    for i in range(0, len(chunks), BATCH_SIZE):\n",
    "        batch = chunks[i:i + BATCH_SIZE]\n",
    "        compressed_batch = batch_concise_rewrite_chunks(batch, query)\n",
    "        compressed_chunks.extend(compressed_batch)\n",
    "\n",
    "\n",
    "    compressed_doc = \"\\n\".join(compressed_chunks)\n",
    "\n",
    "    # Compute ratio of compressed doc to original doc\n",
    "    ratio = len(compressed_doc) / len(doc)\n",
    "    print(f\"Compression ratio: {ratio:.2f}\")\n",
    "    if ratio >= 0.95:\n",
    "        print(\"Compression ratio is too high, skipping compression\")\n",
    "        compressed_doc = doc\n",
    "\n",
    "    input_text = f\"{instruction}{compressed_doc}{truncation_seperator}{query or ''}\"\n",
    "\n",
    "    compressed_input = model_to_chat_template.get(model_name, lambda x: x)(input_text)\n",
    "\n",
    "    # Write compressed input to file\n",
    "    with open(f\"compressed_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(compressed_input)\n",
    "\n",
    "    # Write original input to file\n",
    "    with open(f\"original_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(example[\"input\"])\n",
    "\n",
    "    tokenized_input_full = tokenizer(\n",
    "        compressed_input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokenized_input_full\n",
    "\n",
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    instr = example['input'][:example['document_start_index']]\n",
    "    sep = example['truncation_seperator']\n",
    "    query = example['input'][example['query_start_index']:] or ''\n",
    "    doc = example['input'][example['document_start_index']:example['document_end_index']]\n",
    "    prompt = f\"{instr}{doc}{sep}{query}\"\n",
    "    chat = model_to_chat_template.get(model_name, lambda x:x)(prompt)\n",
    "    return tokenizer(chat, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_chunks_llmlingua_2(chunks, max_ratio=None):\n",
    "    \"\"\"\n",
    "    Compresses each text chunk via LLMLingua, but only keeps the compressed\n",
    "    version if its token-count ratio (compressed/original) is <= max_ratio.\n",
    "    \"\"\"\n",
    "    compressed = []\n",
    "    for chunk in chunks:\n",
    "        orig_tokens = len(chunk.split())\n",
    "        # target ≈ ⅓ of original tokens, capped at 200\n",
    "        target = min(200, orig_tokens // 3)\n",
    "        res = llm_lingua.compress_prompt(\n",
    "            chunk,\n",
    "            instruction=\"\",\n",
    "            question=\"\",\n",
    "            target_token=target\n",
    "        )\n",
    "        short = res.get('compressed_prompt', chunk)\n",
    "\n",
    "        # if a max_ratio is set, enforce it per chunk\n",
    "        if max_ratio is not None and orig_tokens > 0:\n",
    "            comp_tokens = len(short.split())\n",
    "            if (comp_tokens / orig_tokens) > max_ratio:\n",
    "                # reject over-compressed chunk\n",
    "                compressed.append(chunk)\n",
    "                continue\n",
    "\n",
    "        compressed.append(short)\n",
    "    return compressed\n",
    "\n",
    "def process_model_input_LLMlingua_2(tokenizer, example, max_tokens, device, dataset):\n",
    "    instr = example['input'][:example['document_start_index']]\n",
    "    sep   = example['truncation_seperator']\n",
    "    query = example['input'][example['query_start_index']:] or ''\n",
    "    doc   = example['input'][example['document_start_index']\n",
    "                              :example['document_end_index']]\n",
    "\n",
    "    # 1) split into semantic segments\n",
    "    chunks = extract_segments(doc)\n",
    "\n",
    "    # 2) compress each chunk, enforcing a 0.95 ratio cap\n",
    "    if use_llmlingua:\n",
    "        compressed_chunks = compress_chunks_llmlingua(chunks, max_ratio=0.95)\n",
    "    else:\n",
    "        compressed_chunks = chunks\n",
    "\n",
    "    # 3) re-join and build final prompt\n",
    "    compressed_doc = \"\\n\".join(compressed_chunks)\n",
    "    prompt = f\"{instr}{compressed_doc}{sep}{query}\"\n",
    "    chat_input = model_to_chat_template.get(model_name, lambda x: x)(prompt)\n",
    "\n",
    "    # 4) save for inspection\n",
    "    os.makedirs(f\"compressed_input/{dataset}\", exist_ok=True)\n",
    "    open(f\"compressed_input/{dataset}/{example['id']}.txt\",\"w\").write(chat_input)\n",
    "    os.makedirs(f\"original_input/{dataset}\", exist_ok=True)\n",
    "    open(f\"original_input/{dataset}/{example['id']}.txt\",\"w\").write(example['input'])\n",
    "\n",
    "    # 5) tokenize and return\n",
    "    inputs = tokenizer(chat_input, return_tensors=\"pt\").to(device)\n",
    "    return inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_chunks_llmlingua(chunks, max_ratio=None):\n",
    "    \"\"\"\n",
    "    Compresses a list of text chunks using LLMLingua, but only if\n",
    "    the compressed length is within the specified max_ratio of the original.\n",
    "    If the ratio is exceeded, returns the original chunk.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): List of text segments to compress.\n",
    "        max_ratio (float or None): Maximum allowed compressed/original token ratio.\n",
    "            e.g., 0.5 means compressed chunk must be <= 50% of original tokens.\n",
    "            If None, no ratio enforcement.\n",
    "    \"\"\"\n",
    "    compressed = []\n",
    "    for chunk in chunks:\n",
    "        # Count original tokens (simple whitespace split)\n",
    "        orig_tokens = len(chunk.split())\n",
    "        # Determine target tokens: at most 200 or one-third of original\n",
    "        target = min(200, orig_tokens // 3)\n",
    "        # Perform LLMLingua compression\n",
    "        res = compressor.compress_prompt(\n",
    "            chunk,\n",
    "            instruction=\"\",\n",
    "            question=\"\",\n",
    "            target_token=target\n",
    "        )\n",
    "        short = res.get('compressed_prompt', chunk)\n",
    "        # Enforce max_ratio if provided\n",
    "        if max_ratio is not None and orig_tokens > 0:\n",
    "            comp_tokens = len(short.split())\n",
    "            if comp_tokens / orig_tokens > max_ratio:\n",
    "                # Skip compression if ratio too high\n",
    "                compressed.append(chunk)\n",
    "                continue\n",
    "        compressed.append(short)\n",
    "    return compressed\n",
    "\n",
    "# Build model input when chunking\n",
    "def process_model_input_chunking(tokenizer, example, max_tokens, device, dataset):\n",
    "    instr = example['input'][:example['document_start_index']]\n",
    "    sep = example['truncation_seperator']\n",
    "    query = example['input'][example['query_start_index']:] or ''\n",
    "    doc = example['input'][example['document_start_index']:example['document_end_index']]\n",
    "\n",
    "    # Split into segments and compress each chunk, enforcing a max ratio\n",
    "    chunks = extract_segments(doc)\n",
    "    if use_llmlingua:\n",
    "        compressed_chunks = compress_chunks_llmlingua(chunks, max_ratio=0.95)\n",
    "    else:\n",
    "        compressed_chunks = chunks\n",
    "\n",
    "    # Join compressed chunks with newlines\n",
    "    compressed_doc = \"\".join(compressed_chunks)\n",
    "\n",
    "    # Build the prompt by combining instruction, compressed doc, separator, and query\n",
    "    prompt = f\"{instr}{compressed_doc}{sep}{query}\"\n",
    "    chat = model_to_chat_template.get(model_name, lambda x: x)(prompt)\n",
    "\n",
    "    # Save compressed and original inputs for inspection\n",
    "    os.makedirs(f\"compressed_input/{dataset}\", exist_ok=True)\n",
    "    with open(f\"compressed_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(chat)\n",
    "    os.makedirs(f\"original_input/{dataset}\", exist_ok=True)\n",
    "    with open(f\"original_input/{dataset}/{example['id']}.txt\", \"w\") as f:\n",
    "        f.write(example['input'])\n",
    "\n",
    "    # Tokenize and return the input ids for generation\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "    return inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations_dir = \"generations/ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:\n",
      "model: /assets/models/meta-llama-3.2-instruct-3b\n",
      "generations_dir: generations/ipynb/meta-llama-3.2-instruct-3b-llama3-chunking--system-prompt\n",
      "max_examples_per_task: -1\n",
      "==================================================\n",
      "time as start: 26_04_2025_10_16_23\n"
     ]
    }
   ],
   "source": [
    "llm_name = \"llama3\"\n",
    "seed = 43\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "hf_set_seed(seed)\n",
    "print(\"Params:\")\n",
    "print(f\"model: {model_name}\")\n",
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "model_suffix = f\"{model_suffix}-{llm_name}\"\n",
    "if chunking:\n",
    "    model_suffix = f\"{model_suffix}-chunking\"\n",
    "if system_prompt:\n",
    "    model_suffix = f\"{model_suffix}--system-prompt\"\n",
    "generations_dir = os.path.join(generations_dir, model_suffix)\n",
    "print(f\"generations_dir: {generations_dir}\")\n",
    "print(f\"max_examples_per_task: {max_examples_per_task}\")\n",
    "print(\"=\" * 50)\n",
    "time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(f\"time as start: {time}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!, device:cuda:0\n",
      "Will write to: generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n",
      "Processing gov_report\n",
      "time as start gov_report: 25_04_2025_19_57_29\n",
      "Loading gov_report\n",
      "Loaded gov_report\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7895c377ec4979818a2e6ea36a897a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R45461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: crs_R44668\n",
      "Processing example: crs_R45546\n",
      "Processing example: crs_R45732\n",
      "Processing example: crs_R45237\n",
      "Processing example: crs_RS21212\n",
      "Processing example: crs_RL30478\n",
      "Processing example: crs_RL32665\n",
      "Processing example: crs_RS22373\n",
      "Processing example: gao_GAO-19-207\n",
      "Processing example: gao_GAO-18-486\n",
      "Processing example: gao_GAO-19-220\n",
      "Processing example: gao_GAO-19-148\n",
      "Processing example: gao_GAO-18-271\n",
      "Processing example: gao_GAO-18-78\n",
      "Processing example: gao_GAO-18-420\n",
      "Processing example: gao_GAO-17-781T\n",
      "Processing example: gao_GAO-18-249\n",
      "Processing example: gao_GAO-18-7\n",
      "Processing example: gao_GAO-18-403\n",
      "Done generating 20 examples from gov_report\n",
      "time at end: 25_04_2025_20_05_15\n",
      "Look for predictions in generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n",
      "Processing summ_screen_fd\n",
      "time as start summ_screen_fd: 25_04_2025_20_05_15\n",
      "Loading summ_screen_fd\n",
      "Loaded summ_screen_fd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfe3d75bf404e7b8639e0b79c5f17a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: fd_FRIENDS_07x04\n",
      "Processing example: fd_Buffy_the_Vampire_Slayer_04x14\n",
      "Processing example: fd_The_Office_05x24\n",
      "Processing example: fd_Angel_03x07\n",
      "Processing example: fd_The_Office_03x20\n",
      "Processing example: fd_Doctor_Who_1963_14x21\n",
      "Processing example: fd_Frasier_07x04\n",
      "Processing example: fd_Frasier_10x16\n",
      "Processing example: fd_Justified_06x12\n",
      "Processing example: fd_Charmed_06x13\n",
      "Processing example: fd_Pretty_Little_Liars_01x03\n",
      "Processing example: fd_Buffy_the_Vampire_Slayer_06x14\n",
      "Processing example: fd_Justified_04x01\n",
      "Processing example: fd_Doctor_Who_01x02\n",
      "Processing example: fd_The_Office_08x23\n",
      "Processing example: fd_Justified_06x11\n",
      "Processing example: fd_CSI__Crime_Scene_Investigation_06x02\n",
      "Processing example: fd_Alias_01x04\n",
      "Processing example: fd_The_Vampire_Diaries_01x11\n",
      "Processing example: fd_The_O.C._03x01\n",
      "Done generating 20 examples from summ_screen_fd\n",
      "time at end: 25_04_2025_20_11_31\n",
      "Look for predictions in generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n",
      "Processing qmsum\n",
      "time as start qmsum: 25_04_2025_20_11_31\n",
      "Loading qmsum\n",
      "Loaded qmsum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea103abb9c744c1f8600ae8a3a8c69f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: va-sq-11\n",
      "Processing example: va-sq-50\n",
      "Processing example: va-sq-57\n",
      "Processing example: va-sq-58\n",
      "Processing example: va-gq-66\n",
      "Processing example: va-gq-93\n",
      "Processing example: va-sq-108\n",
      "Processing example: va-sq-117\n",
      "Processing example: va-sq-125\n",
      "Processing example: va-sq-145\n",
      "Processing example: va-gq-166\n",
      "Processing example: va-sq-179\n",
      "Processing example: va-sq-184\n",
      "Processing example: va-sq-186\n",
      "Processing example: va-sq-216\n",
      "Processing example: va-sq-219\n",
      "Processing example: va-sq-225\n",
      "Processing example: va-sq-250\n",
      "Processing example: va-sq-258\n",
      "Processing example: va-sq-264\n",
      "Done generating 20 examples from qmsum\n",
      "time at end: 25_04_2025_20_18_26\n",
      "Look for predictions in generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n",
      "Processing qasper\n",
      "time as start qasper: 25_04_2025_20_18_26\n",
      "Loading qasper\n",
      "Loaded qasper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e445e38633b42a883b43e7261a2d585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3fad42be0fb2052bb404b989cc7d58b440cd23a0\n",
      "Processing example: 3fad42be0fb2052bb404b989cc7d58b440cd23a0\n",
      "Processing example: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0\n",
      "Processing example: 8bf7f1f93d0a2816234d36395ab40c481be9a0e0\n",
      "Processing example: 0f12dc077fe8e5b95ca9163cea1dd17195c96929\n",
      "Processing example: 0f12dc077fe8e5b95ca9163cea1dd17195c96929\n",
      "Processing example: 518dae6f936882152c162058895db4eca815e649\n",
      "Processing example: 58ef2442450c392bfc55c4dc35f216542f5f2dbb\n",
      "Processing example: 58ef2442450c392bfc55c4dc35f216542f5f2dbb\n",
      "Processing example: 290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30\n",
      "Processing example: ab9b0bde6113ffef8eb1c39919d21e5913a05081\n",
      "Processing example: ff338921e34c15baf1eae0074938bf79ee65fdd2\n",
      "Processing example: 1b1a30e9e68a9ae76af467e60cefb180d135e285\n",
      "Processing example: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596\n",
      "Processing example: dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596\n",
      "Processing example: 3355918bbdccac644afe441f085d0ffbbad565d7\n",
      "Processing example: d9980676a83295dda37c20cfd5d58e574d0a4859\n",
      "Processing example: d9980676a83295dda37c20cfd5d58e574d0a4859\n",
      "Processing example: 79a44a68bb57b375d8a57a0a7f522d33476d9f33\n",
      "Processing example: 79a44a68bb57b375d8a57a0a7f522d33476d9f33\n",
      "Processing example: 76ed74788e3eb3321e646c48ae8bf6cdfe46dca1\n",
      "Processing example: 8e52637026bee9061f9558178eaec08279bf7ac6\n",
      "Processing example: 8e52637026bee9061f9558178eaec08279bf7ac6\n",
      "Processing example: 3116453e35352a3a90ee5b12246dc7f2e60cfc59\n",
      "Processing example: 3116453e35352a3a90ee5b12246dc7f2e60cfc59\n",
      "Processing example: 4e748cb2b5e74d905d9b24b53be6cfdf326e8054\n",
      "Processing example: b970f48d30775d3468952795bc72976baab3438e\n",
      "Processing example: c70bafc35e27be9d1efae60596bc0dd390c124c0\n",
      "Done generating 19 examples from qasper\n",
      "time at end: 25_04_2025_20_20_50\n",
      "Look for predictions in generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n",
      "Processing narrative_qa\n",
      "time as start narrative_qa: 25_04_2025_20_20_50\n",
      "Loading narrative_qa\n",
      "Loaded narrative_qa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab95413c34cb44588f3ceecec3ff21a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 3858\n",
      "Processing example: 3858\n",
      "Processing example: 5947\n",
      "Processing example: 5947\n",
      "Processing example: 12164\n",
      "Processing example: 12164\n",
      "Processing example: 23148\n",
      "Processing example: 23148\n",
      "Processing example: 25278\n",
      "Processing example: 25278\n",
      "Processing example: 33068\n",
      "Processing example: 33068\n",
      "Processing example: 35134\n",
      "Processing example: 35134\n",
      "Processing example: 38322\n",
      "Processing example: 38322\n",
      "Processing example: 42586\n",
      "Processing example: 42586\n",
      "Processing example: 16886\n",
      "Processing example: 16886\n",
      "Done generating 10 examples from narrative_qa\n",
      "time at end: 25_04_2025_20_37_35\n",
      "Look for predictions in generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n",
      "Processing quality\n",
      "time as start quality: 25_04_2025_20_37_35\n",
      "Loading quality\n",
      "Loaded quality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8391f8fcc9cc40208cff33cbcfe73060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 62139_J05FWZR6_1\n",
      "Processing example: 52855_MV65I88C_9\n",
      "Processing example: 62085_C1SL2YBE_3\n",
      "Processing example: 63616_MQ1O9T2Q_6\n",
      "Processing example: 63833_V187YO4H_2\n",
      "Processing example: 63392_7YS4HHFI_6\n",
      "Processing example: 63473_1VIHQ8TY_4\n",
      "Processing example: 51650_B3KKWWD1_7\n",
      "Processing example: 51274_8Q2YNHG5_6\n",
      "Processing example: 20077_ZF5G55FD_1\n",
      "Processing example: 22579_RQ3GB4A1_3\n",
      "Processing example: 22867_TJ9SPIHC_9\n",
      "Processing example: 22875_L821878U_6\n",
      "Processing example: 22967_0XT2L7PI_7\n",
      "Processing example: 22867_IZGAWLCJ_4\n",
      "Processing example: 22462_BUA2LH2S_5\n",
      "Processing example: 31736_TV0CUXDH_4\n",
      "Processing example: 99927_EVLEI3Q2_6\n",
      "Processing example: 31282_BQYW9TCH_4\n",
      "Processing example: 99914_0Q5X8VEX_4\n",
      "Processing example: 32665_VRYQXG3Y_9\n",
      "Done generating 21 examples from quality\n",
      "time at end: 25_04_2025_20_41_29\n",
      "Look for predictions in generations/ipynb/llama-i3b-microsoft_phi-2-chunking--system-prompt\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "\n",
    "mid_layer_index = 16\n",
    "print(f\"model loaded!, device:{model.device}\")\n",
    "\n",
    "print(\"Will write to:\", generations_dir)\n",
    "os.makedirs(generations_dir, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    generations = dict()\n",
    "    print(f\"Processing {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time as start {dataset}: {time}\")\n",
    "    print(f\"Loading {dataset}\")\n",
    "    data = load_dataset(\"tau/zero_scrolls\", dataset, trust_remote_code=True)\n",
    "    print(f\"Loaded {dataset}\")\n",
    "    # Create dir compressed_input if it doesn't exist\n",
    "    compressed_dir = os.path.join(\"compressed_input\", dataset)\n",
    "    original_dir = os.path.join(\"original_input\", dataset)\n",
    "    if not os.path.exists(compressed_dir):\n",
    "        os.makedirs(compressed_dir)\n",
    "    if not os.path.exists(original_dir):\n",
    "        os.makedirs(original_dir)\n",
    "\n",
    "    for i, example in tqdm(enumerate(data[\"validation\"])):\n",
    "        print(\"Processing example:\", example[\"id\"])\n",
    "\n",
    "        if 0 < max_examples_per_task == i:\n",
    "            print(f\"Reached {max_examples_per_task} for {dataset}. Breaking\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            if chunking:\n",
    "                model_input = process_model_input_chunking(\n",
    "                    tokenizer, example, max_input_length, device, dataset)\n",
    "            else:\n",
    "                model_input = process_model_input(\n",
    "                    tokenizer, example, max_input_length, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i} in {dataset}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Get hidden states from the 16th layer\n",
    "        with torch.no_grad():\n",
    "            prediction_token_ids = model.generate(model_input,\n",
    "                                                  max_new_tokens=512,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_p=0.9,\n",
    "                                                  top_k=0,\n",
    "                                                  temperature=0.5,\n",
    "                                                  pad_token_id=tokenizer.eos_token_id,\n",
    "                                                  )\n",
    "\n",
    "            predicted_text = tokenizer.decode(\n",
    "                prediction_token_ids[0][model_input.shape[1]:], skip_special_tokens=True)\n",
    "            del prediction_token_ids, model_input\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        generations[example[\"id\"]] = predicted_text\n",
    "\n",
    "    out_file_path_pred = os.path.join(generations_dir, f\"{dataset}.json\")\n",
    "    with open(out_file_path_pred, 'w') as f_out:\n",
    "        json.dump(generations, f_out, indent=4)\n",
    "\n",
    "    print(f\"Done generating {len(generations)} examples from {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time at end: {time}\")\n",
    "    print(f\"Look for predictions in {generations_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
