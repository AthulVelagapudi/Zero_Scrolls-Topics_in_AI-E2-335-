<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction   The new generation of Neural Machine Translation (NMT) systems is known to be extremely data hungry BIBREF0 .  Yet, most existing NMT training pipelines fail to fully take advantage of the very large volume of monolingual source and/or parallel data that is often available.Making a better use of data is particularly critical in domain adaptation scenarios, where parallel adaptation data is usually assumed to be small in comparison to out-of-domain parallel data, or to in-domain monolingual texts.  This situation sharply contrasts with the previous generation of statistical MT engines BIBREF1 , which could seamlessly integrate very large amounts of non-parallel documents, usually with a large positive effect on translation quality.  Such observations have been made repeatedly and have led to many innovative techniques to integrate monolingual data in NMT, that we review shortly.The most successful approach to date is the proposal of BIBREF2 , who use monolingual target texts to generate artificial parallel data via backward translation (BT).  This technique has since proven effective in many subsequent studies.  It is however very computationally costly, typically requiring to translate large sets of data.  Determining the “right” amount (and quality) of BT data is another open issue, but we observe that experiments reported in the literature only use a subset of the available monolingual resources.This suggests that standard recipes for BT might be sub-optimal.  This paper aims to better understand the strengths and weaknesses of BT and to design more principled techniques to improve its effects.  More specifically, we seek to answer the following questions:since there are many ways to generate pseudo parallel corpora, how important is the quality of this data for MT performance?  Which properties of back-translated sentences actually matter for MT quality?  Does BT act as some kind of regularizer BIBREF3 ?  Can BT be efficiently simulated?Does BT data play a-side language modeling?   for adaptation   the of more be the mere training material BIBREF?  For studies related to the impact of varying the size of BT data, we refer the readers to the recent work of BIBREF4.To have reimmented several to useolingual data in NMT and have run experiments on two language pairs in a very controlled setting (see § SECREF2 ).  Our main results (see § SECREF4 and § SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than conventional BT.In-domain and out-of-domain data   We are mostly interested with the following training scenario:  a large out-of-domain parallel corpus, and limited monolingual in-domain data.We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions:  English INLINEFORM0  German and English  INLINEFORM1 French. we the ofual, our experiments the target of this. for this to) to perform of natural; of domain- both thel from for, keeping test 2006 for development.When measuring out-of-domain performance, we will use the WMT newstest 2014.  NMT setups and performanceOur baseline NMT system implements the attentional encoder-decoder approach BIBREF6 , BIBREF7 as implemented in Nematus BIBREF8 on 4 million out-of-domain parallel sentences.  For French we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN BIBREF9 and EU-Bookshop BIBREF10 corpora.  For German, we use samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and Multi-UN (see table TABREF5 ).  Bilingual BPE units BIBREF11 are learned with 50k merge operations, yielding vocabularies of about respectively 32k and 36k for English INLINEFORM0 French and 32k and 44k for English INLINEFORM1 German.Both systems use 512-dimensional word embeddings and a single hidden layer with 1024 cells.  They are optimized using Adam BIBREF12 and early stopped according to the validation performance.  Training lasted for about three weeks on an Nvidia K80 GPU card.s generating- are same, where we simply target.   are further in § SECREF we also train a system that has access to a large batch of in-domain parallel data following the strategy often referred to as “fine-tuning”:upon convergence of the baseline model, we resume training with a 2M sentence in-domain corpus mixed with an equal amount of randomly selected out-of-domain natural sentences, with the same architecture and training parameters, running validation every 2000 updates with a patience of 10.  Since BPE units are selected based only on the out-of-domain statistics, fine-tuning is performed on sentences that are slightly longer (ie. they contain more units) than for the initial training.  This system defines an upper-bound of the translation performance and is denoted below as natural.Our baseline and topline results are in Table TABREF6 , where we measure translation performance using BLEU BIBREF13 , BEER BIBREF14 (higher is better) and characTER BIBREF15 (smaller is better).  As they are trained from much smaller amounts of data than current systems, these baselines are not quite competitive to today's best system, but still represent serious baselines for these datasets.  Given our setups, fine-tuning with in-domain natural data improves BLEU by almost 4 points for both translation directions on in-domain tests; it also improves, albeit by a smaller margin, the BLEU score of the out-of-domain tests.Using artificial parallel data in NMT   A simple way to use monolingual data in MT is to turn it into synthetic parallel data and let the training procedure run as usual BIBREF16 .  In this section, we explore various ways to implement this strategy.We first reproduce results of BIBREF2 with BT of various qualities, that we then analyze thoroughly.  The quality of Back-Translation  BT requires the availability of an MT system in the reverse translation direction.We consider three:  back- poor system onlyk the additional system use Moses BIBREF17, computing withalign a pre-processing (basic tokenization).This a pessimistic in.  back-: are larger same parallel data as the baseline NMTs (see § SECREF4 ) and all the English monolingual data available for the WMT 2017 shared tasks,  totalling approximately 174M sentences.These systems are strong, yet relatively cheap to build.  backtrans-nmt:  these are the best NMT systems we could train,using settings that replicate the forward translation NMTs.  Note that we do not use any in-domain (Europarl) data to train these systems.  Their performance is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best systems (for both languages).As noted in BIBREF,  ificial data through forward-translation (FT) can also prove advantageous and we also consider a FT system (fwdtrans-nmt):  in this case the target side of the corpus is artificial and is generated using the baseline NMT applied to a natural source.Our results (see Table TABREF6 )  replicate the findings of BIBREF2 :  large gains can be obtained from BT (nearly INLINEFORM0 BLEU in French and German);better artificial data yields better translation systems.  Interestingly, our best Moses system is almost as good as the NMT and an order of magnitude faster to train.  Improvements obtained with the bad system are much smaller;  contrary to the better MTs, this system is even detrimental for the out-of-domain test.Gains with forward are in BIBREF, albeit as with and in the tests.  Experiments combining forward and backward translation (backfwdtrans-nmt), each using a half of the available artificial data, do not outperform the best BT results.We finally note the large remaining difference between BT data and natural data, even though they only their source side.  This shows that at least in our domain-adaptation settings, BT does not really act as a regularizer, contrarily to the findings of BIBREF4, BIBREF11.  Figure FIGREF13 displays the learning curves of these two systems.  We observe that backtrans-nmt improves quickly in the earliest updates and then stays horizontal, whereas natural continues improving, even after 400k updates.Therefore BT does not help to avoid overfitting, it actually encourages it, which may be due “easier” training examples (cf. § SECREF15 ).  Properties of back-translated dataComparing the natural and artificial sources of our parallel data wrt.  several linguistic and distributional properties, we observe that (see Fig. FIGREF21 - FIGREF22 ):  artificial sources are on average shorter than natural ones:when using BT,  cases where the source is shorter than the target are rarer;  cases when they have the same length are more frequent.automatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average  Kendall INLINEFORM0 of source-target alignments  BIBREF22 :  for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial);for German-English 0.068 and 0.053.  Using more monotonic sentence pairs turns out to be a facilitating factor for NMT, as also noted by BIBREF20 .  syntactically, artificial sources are simpler than real data;We observe significant differences in the distributions of tree depths.  distributionally, plain word occurrences in artificial sources are more concentrated;  this also translates into both a slower increase of the number of types wrt. of sentences and a of rareThe that properties should help translation as to natural ( should be detrimental.  We checked (ii) by building systems with only 10M words from the natural parallel data selecting these data either randomly or based on the regularity of their word alignments.Results in Table TABREF23 show that the latter is much preferable for the overall performance.  This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline.  Stupid Back-TranslationWe now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine.  SetupsWe use the following cheap ways to generate pseudo-source texts:  copy:in this the source a mere target.   source vocabulary the, target sentences can the, BIBREF24 decom target into copy  EachOV is into units the resulting are in the source vocabulary.copy-marked:  another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary.  In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier.Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume.  copy-dummies:  instead of using actual copies, we replace each word with “dummy” tokens. this unrealistic to the training over and informative source the in § SEC4, the embeddings in the copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining parameters are frozen.  This prevents random parameters from hurting the already trained model.Copy+marking+noise is not so stupid We observe that the copy setup has only a small impact on the English-French system, for which the baseline is already strong.  This is less true for English-German where simple copies yield a significant improvement.Performance drops for both language the-dummies.  We achieve best- best to copy target ( the performance thedomain is at most  Such gains may look surprising, since the NMT model does not need to learn to translate but only to copy the source. is indeed:  to we a fake set having identical target side (inThe average., 0, be with average of.52 we actual source to with no even sentences not seen in training.A whether a task task the improvement:   theMT was  To measure this, we introduce noise in the target sentences copied onto the source, following the procedure of BIBREF26 :  it deletes random words and performs a small random permutation of the remaining words.Results (+ Source)  show the French- test but the   on (5 last is even the backtrans-nmt condition (see § SECREF8 ) for German.This shows that learning to reorder and predict missing words can more effectively serve our purposes than simply learning to copy.  Towards more natural pseudo-sourcesrating into target which already some improvement adding noise.We now study make-sources look more like natural data, using the framework of Generative Adversarial Networks (GANs)  BIBREF27, an idea borrowed from BIBREF26.GAN setups  In our setups, we use a marked target copy, viewed as a fake source, which a generator encodes so as to fool a discriminator trained to distinguish a fake from a natural source.  Our architecture contains two distinct encoders, one for the natural source and one for the pseudo-source.The latter acts as the generator ( INLINEFORM0 ) in the GAN framework, computing a representation of the pseudo-source that is then input to a discriminator ( INLINEFORM1 ), which has to sort natural from artificial encodings.  INLINEFORM2 assigns a probability of a sentence being natural.  During training, the cost of the discriminator is computed over two batches, one with natural (out-of-domain) sentences INLINEFORM0 and one with (in-domain) pseudo-sentencesINLINEFORM1The aal- ofAaged is applied.  It inputs encodings of natural ( INLINEFORM2 ) and pseudo-sentences ( INLINEFORM3 ) and is trained to optimize:INLINEFORM0  INLINEFORM0's parameters are updated to maximally foolINLINEFORM1, thus the loss  INLINEFORM2 :  INLINEFORM3Finally, we keep the usual MT objective.  ( INLINEFORM0 is a real or pseudo-sentence):  INLINEFORM1We thus need to train three sets of parameters:  INLINEFORM0 and INLINEFORM1 (MT parameters), with INLINEFORM2.  The pseudo-source encoder and embeddings are updated wrt.  both INLINEFORM3 and INLINEFORM4.Following BIBREF28,  INLINEFORM is6sFORM , INLINEFORM8 is not updated when its accuracy exceeds INLINEFORM9.  At each update, two batches are generated for each type of data, which are encoded with the real or pseudo-encoder.The encoder outputs serve to compute INLINEFORM10 and INLINEFORM11 .  Finally, the pseudo-source is encoded again (once INLINEFORM12 is updated), both encoders are plugged into the translation model and the MT cost is back-propagated down to the real and pseudo-word embeddings.  Pseudo-encoder and discriminator parameters are pre-trained for 10k updates.  At test time, the pseudo-encoder is ignored and inference is run as usual.GANs can help  Results are in Table TABREF32, assuming the same fine-tuning procedure as above.On top of the copy-marked setup, our GANs do not provide any improvement in both language pairs, with the exception of a small improvement for English-French on the out-of-domain test, which we understand as a sign that the model is more robust to domain variations, just like when adding pseudo-source noise.  When combined with noise, the French model yields the best performance we could obtain with stupid BT on the in-domain tests, at least in terms of BLEU and BEER.  On the News domain, we remain close to the baseline level, with slight improvements in German.A first this method brings stupid BT to conventional aWhile French remains0 B below good, for German - because former system than for the latter.  Finally note that the GAN architecture has two differences with basic copy-marked: a distinct for and-; differentTo we the with BT, of  -source the backtrans-nmt setup can be detrimental to performance (see Table TABREF32 ):translation degrades in French all  Addings the- able to make up for the degradation observed in French, but allowed the German system to slightly outperform backtrans-nmt.  Even though this setup is unrealistic and overly costly, it shows that GANs are actually helping even good systems.Using Target Language Models    we the previous methods with the use of a target side Language Model (LM).  Several proposals exist in the literature to integrate LMs in NMT: for instance, BIBREF3 strengthen the decoder by integrating an extra, source independent, RNN layer in a conventional NMT architecture.Training is performed either with parallel, or monolingual data.  In the latter case, word prediction only relies on the source independent part of the network.  LM SetupWe have followed BIBREF29 and reimplemented their deep-fusion technique.  It requires to first independently learn a RNN-LM on the in-domain target data with a cross-entropy objective; then to train the optimal combination of the translation and the language models by adding the hidden state of the RNN-LM as an additional input to the softmax layer of the decoder.  Our RNN-LMs are trained using dl4mt with the target side of the parallel data and the Europarl corpus (about 6M sentences for both French and German), using a one-layer GRU with the same dimension as the MT decoder (1024).LM Results  Results are in Table TABREF33.They show that deep-fusion hardly improves the Europarl results, while we obtain about +0.6 BLEU over the baseline on newstest-2014 for both languages.  deep-fusion differs from stupid BT in that the model is not directly optimized on the in-domain data, but uses the LM trained on Europarl to maximize the likelihood of the out-of-domain training data.  Therefore, no specific improvement is to be expected in terms of domain adaptation, and the performance increases in the more general domain.  Combining deep-fusion and copy-marked + noise + GANs brings slight improvements on the German in-domain test sets, and performance out of the domain remains near the baseline level.Re-analyzing the effects of BT   As a follow up of previous discussions, we analyze the effect of BT on the internals of the network.Arguably, using a copy of the target sentence instead of a natural source should not be helpful for the encoder, but is it also the case with a strong BT?  What are the effects on the attention model?  Parameter freezing protocolTo we sameing the-marked,trans-mt. that all training have to same targetWe to the the system we selectively freeze certain sets of parameters, meaning that they are not updated during fine-tuning.Results BLEU scores are in Table TABREF39.  The backtrans-nmt setup is hardly impacted by selective updates:updating the only decoder leads to a degradation of at most 0.2 BLEU.  For copy-marked, we were not able to freeze the source embeddings, since these are initialized when fine-tuning begins and therefore need to be trained.  We observe that freezing the encoder and/or the attention parameters has no impact on the English-German system, whereas it slightly degrades the English-French one.  This suggests that using artificial sources, even of the poorest quality, has a positive impact on all the components of the network, which makes another big difference with the LM integration scenario.The largest degradation is for natural, where the model is prevented from learning from informative source sentences, which leads to a decrease of 0.4 to over 1.0 BLEU.  We assume from these experiments that BT impacts most of all the decoder, and learning to encode a pseudo-source, be it a copy or an actual back-translation, only marginally helps to significantly improve the quality.  Finally, in the fwdtrans-nmt setup, freezing the decoder does not seem to harm learning with a natural source.Related work  The literature devoted to the use of monolingual data is large, and quickly expanding.We already alluded to several possible ways to use such data: using back- or forward-translation or using a target language model.  The former approach is mostly documented in BIBREF2, and recently analyzedIBREF19, which focus on fully artificial settings as well as pivot-based artificial data; and BIBREF4, which studies the effects of increasing the size of BT data.  The studies of BIBREF20, BIBREF19 also consider forward translation and BIBREF21 expand these results to domain adaptation scenarios.  Our results are complementary to these earlier studies.As shown above, many alternatives to BT exist.  The most obvious is to use target LMs BIBREF3, BIBREF29, as we have also done here; but attempts to improve the encoder using multi-task learning also exist BIBREF30.This investigation is also related to recent attempts to consider supplementary data with a valid target side, such as multi-lingual NMT BIBREF31 , where source texts in several languages are fed in the same encoder-decoder architecture, with partial sharing of the layers.  This is another realistic scenario where additional resources can be used to selectively improve parts of the model.  Round trip training is another important source of inspiration, as it can be viewed as a way to use BT to perform semi-unsupervised BIBREF32 or unsupervised BIBREF33 training of NMT.The most convincing date has been proposed by BIBREF26, who propose to use GANs to mitigate the difference between artificial and natural data.  Conclusion  In this paper, we have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation.While the of, also proposed significantly the a modified the target of full.  When no high quality BT is available, using GANs to make the pseudo-source sentences closer to natural source sentences is an efficient solution for domain adaptation.  To recap our answers to our initial questions:the quality of BT actually matters for NMT (cf. § SECREF8 ) and it seems that, even though artificial source are lexically less diverse and syntactically complex than real sentence, their monotonicity is a facilitating factor.  We have studied cheaper alternatives and found out that copies of the target, if properly noised (§ SECREF4 ), and even better, if used with GANs, could be almost as good as low quality BTs (§ SECREF5 ):  BT is only worth its cost when good BT can be generated.  Finally, BT seems preferable to integrating external LM - at least in our data condition (§ SECREF6 ).Further experiments with larger LMs are needed to confirm this observation, and also to evaluate the complementarity of both strategies.  More work is needed to better understand the impact of BT on subparts of the network (§ SECREF7 ).  In future work, we plan to investigate other cheap ways to generate artificial data.The experimental setup we proposed may also benefit from a refining of the data selection strategies to focus on the most useful monolingual sentences.... [The rest of the article is omitted]

Question:
what data simulation techniques were introduced?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

