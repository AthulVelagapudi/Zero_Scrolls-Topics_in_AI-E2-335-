<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction  Medical text mining is an exciting area and is becoming attractive to natural language processing (NLP) researchers.  Clinical notes are an example of text in the medical area that recent work has focused on BIBREF0, BIBREF1, BIBREF2.This work studies abbreviation disambiguation on clinical notes BIBREF3, BIBREF4, specifically those used commonly by physicians and nurses.  Such clinical abbreviations can have a large number of meanings, depending on the specialty BIBREF5, BIBREF6.  For example, the term MR can mean magnetic resonance, mitral regurgitation, mental retardation, medical record and the general English Mister (Mr.).  Table TABREF1 illustrates such an example.Abbreviation disambiguation is an important task in medical text understanding task BIBREF7.  Successful recognition of the abbreviations in the notes can contribute to downstream tasks such as classification, named entity recognition, and relation extraction BIBREF7.  Recent work focuses on formulating the abbreviation disambiguation task as a classification problem, where the possible senses of a given abbreviation term are pre-defined with the help of domain experts BIBREF6, BIBREF5.Traditional features as-speech) and are widely for clinical notes.  ifiers like are to predictions BIBREF1.  Such methods depend heavily on feature engineering.  Recently, deep features have been investigated in the medical domain.Word embeddings BIBREF8 and Convolutional Neural Networks (CNNs)  BIBREF9, BIBREF10 provide very competitive performance on text classification for clinical notes and abbreviation disambiguation task BIBREF0, BIBREF11, BIBREF12, BIBREF6.  Beyond vanilla embeddings, BIBREF13 utilized contextual features to do abbreviation expansion.  Another challenge is the difficulty in obtaining training data: clinical notes have many restrictions due to privacy issues and require domain experts to develop high-quality annotations, thus leading to limited annotated training and testing data.Another difficulty is that in the real world (and in the existing public datasets), some abbreviation term-sense pairs (for example, AB as abortion) have a very high frequency of occurrence BIBREF5, while others are rarely found.  This long tail issue creates the challenge of training under unbalanced sample distributions.  We tackle these problems in the setting of few-shot learning BIBREF14, BIBREF15 where only a few or a low number of samples can be found in the training dataset to make use of limited resources.  We propose a model that combines deep contextual features based on ELMo BIBREF16 and topic information to solve the abbreviation disambiguation task.Our contributions be:) weined and corrected an existing dataset for training and we collected a test set for evaluation with focus especially for rare senses;  2) we proposed a few-shot learning approach which combines topic information and contextualized word embeddings to solve clinical abbreviation disambiguation task.The implementation is available online;  3) as limited research are conducted on this particular task, we evaluated and compared a number of baseline methods including classical models and deep models comprehensively.  DatasetsTraining Dataset  UM  Inventory  BIBREF5 is a public dataset created by researchers from the University of Minnesota, containing about 37,500 training samples with 75 abbreviation terms.Existing work reportsviation disuation results on 50viation terms BIBREFREF. , after this many where professionals disagree  wrong samples and   these and flaws the and eventually selected 30 as our training dataset that can be made public.Among theviation, have,466 and 93- pairs ( average pair and 38).   are very popular a of samples but (typically 5), we call them rare-sense cases.  More details can be found in Appendix SECREF7.Testinget  Our dataset takes MIII BIBREF and as   are to the notesIII (NOTVENTS table and Reports PubMed as this of contents are close to medical notes.  We provide detailed information in Appendix SECREF8, including the steps to create the testing dataset.Eventually, we have a balanced testing dataset, where each term-sense pair has at least 11 and up to 15 samples for training (on average, each pair has 14.56 samples and the median sample number is 15).  BaselinesWe a comprehensive comparison the baseline and were never investigated theviation.We applied traditional features by the the inputs the classic   also considered  a model BIBREF was using and these were applied to initialize deep models and fine-tuned.TF-IDF:  We applied TF-IDF features as inputs to four classifiers:  support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB) and random forest (RF);CNN:  We followed the same architecture as BIBREF9;  LSTM:  We applied an LSTM model BIBREF20 to classify the sentences with pre-trained word embeddings;LSTM-soft:We then a soft-ention BIBREF layerM model we soft scores over states;  M-self: We layer toWe denote the content vector as $c_i$ for each sentence $i$, as the input to the last classification layer.Topic-attention Model ELMo  ELMo is a new type of word representation introduced by BIBREF16 which considers contextual information captured by pre-trained BiLSTM language models.  Similar works like BERT BIBREF23 and BioBERT BIBREF24 also consider context but with deeper structures. with those, ELMo has less and not to be.  We trained the MIII  Since some sentences also the, one may of performance inflation in weMo the corpusIC in an unsupervised way, so the classification labels were not visible during training.To train the ELMo model, we adapted code from AllenNLP, we set the word embedding dimension to 64 and applied two BiLSTM layers.  For all of our experiments that involved with ELMo, we initialized the word embeddings from the last layer of ELMo.  Topic-attentionWe propose a neural topic-attention model for text classification.  Our assumption is that the topic information plays an important role in doing the disambiguation given a sentence.  For example, the abbreviation of the medical term FISH has two potential senses: general English fish as seafood and the sense of fluorescent in situ hybridization BIBREF17.  The former case always goes with the topic of food, allergies while the other one appears in the topic of some examination test reports.In our model, a topic-attention module is applied to add topic information into the final sentence representation.  This module calculates the distribution of topic-attention weights on a list of topic vectors.  As shown in Figure SECREF5, we took the content vector $c_i$ (from soft-attention BIBREF22) and a topic matrix $T_{topic}=[t_1,t_2,..,t_j]$ (where each $t_i$ is a column vector in the figure and we illustrate four topics) as the inputs to the topic-attention module, and then calculated a weighted summation of the topic vectors.  The final sentence representation $r_i$ for the sentence $i$ was calculated as the following:where $W_{topic}$ and $b _  { topic }$ are trainable parameters,  $\beta _{it}$ represents the topic-attention weights.The final sentence representation is denoted as $r_i$, and $[\cdot ,\cdot ]$ means concatenation.  Here $s_i$ is the representation of the sentence which includes the topic information.  The final sentence representation $r_i$ is the concatenation of $c_i$ and $s_i$, now we have both context-aware sentence representation and topic-related representation.  Then we added a fully-connected layer, followed by a Softmax to do classification with cross-entropy loss function.Topic Matrix  To the matrix_{$ation DIS_FORM we aolution first pre-trained a topic model using the Latent Dirichlet Allocation (LDA) BIBREF25 model on MIMIC-III notes as used by the ELMo model.We set the number of topics to be 50 and ran for 30 iterations.  Then we were able to get a list of top $k$ words (we set $k=100$) for each topic.  To get the topic vector $t$ for a specific topic:where $e_j$ (column vector) is the pre-trained Doc2vec word embedding for the top word $j$ from the current topic, and $Conv(\cdot )$ indicates a convolutional layer;  $maxpool(\cdot )$ is a max pooling layer.  We finally reshaped the output $t$ into a 1-dimension vector.  Eventually we collected all topic vectors as the topic matrix $T_{topic}$ in Figure SECREF5.Evaluation  We did three groups of experiments including two baseline groups and our proposed model.The first group was the TF-IDF features in Section SECREF3 for traditional models.  The Na√Øve Bayesian classifier (NB) has the highest scores among all traditional methods.  The second group of experiments used neural models described in Section SECREF3, where LSTM with self attention model (LSTM-self) has competitive results among this group, we choose this model as our base model.  Notably, this is the first study that compares and evaluates LSTM-based models on the medical term abbreviation disambiguation task.tableExperimental Results:  we report macro F1 in all the experiments.  figureTopic-attention ModelThe last group the of our proposed model with different.  We used Topic Only setting on the, only- layer and the were from ourtrained2 model and wereed duringWe could that, we have of. on and. F1 score.Another model (ELMo Only) is to initialize the word embeddings of the base model using the pre-trained ELMo model, and here no topic information was added.  Here we have higher scores than Topic Only, and the accuracy and F1 score increased by 9.87% and 12.26% respectively compared with the base model.  We then conducted a combination of both(ELMo+Topic), where the word embeddings from the sentences were computed from the pre-trained ELMo model, and the topic representations were from the pre-trained Doc2vec model.  We have a remarkable increase of 12.27% on the accuracy and 14.86% on F1 score.To further compare our proposed topic- model the we report an of curve) score all 30 terms:  7 model (ELMo+Topic) achieves an average AUC of 0.8196.  We provide a case study in Appendix SECREF9.The results the model can from ELMo, which considers contextual features, and the topic-attention module, which brings in topic information.  We can conclude that under the few-shot learning setting, our proposed model can better capture the sentence features when only limited training samples are explored in a small-scale dataset.  ConclusionIn this paper, we propose a neural topic-attention model with few-shot learning for medical abbreviation disambiguation task.  We also manually cleaned and collected training and testing data for this task, which we release to promote related research in NLP with clinical notes.  In addition, we evaluated and compared a comprehensive set of baseline models, some of which had never been applied to the medical term abbreviation disambiguation task.  Future work would be to adapt other models like BioBERT or BERT to our proposed topic-attention model.We will also extend the method into other clinical notes tasks such as drug name recognition and ICD-9 code auto-assigning BIBREF26.  In addition, other LDA-based approach can be investigated.  Dataset DetailsFigure FIGREF11 shows the histogram for the distribution of term-sense pair sample numbers.  The X-axis gives the pair sample numbers and the Y-axis shows the counts.  For example, the first bar shows that there are 43 term-sense pairs that have the sample number in the range of 0-50.We also show histogram of class numbers for all terms in Figure FIGREF11.  The Y-axis gives the counts while the X-axis gives the number of classes.  For instance, the first bar means there are 12 terms contain 2 classes.Testing Dataset  Since the training dataset is unbalanced and relatively small, it is hard to split it further into training and testing.Existing work BIBREF6, BIBREF5 conducted fold validation on the datasets and we found there are extreme rare senses for which only one or two samples exist.  Besides, we believe that it is better to evaluate on a balanced dataset to determine whether it performs equally well on all classes.  While most of the works deal with unbalanced training and testing that which may lead to very high accuracy if there is a dominating class in both training and testing set, the model may have a poor performance in rare testing cases because only a few samples have been seen during training.  To be fair to all the classes, a good performance on these rare cases is also required otherwise it may lead to a severe situation.In this work, we are very interested to see how the model works among all senses especially the rare ones.  Also, we can prevent the model from trivially predicting the dominant class and achieving high accuracy.  As a result, we decided to create a dataset with the same number of samples for each case in the test dataset.Our testing dataset takes MIMIC-III BIBREF18 and PubMed as data sources.  Here we are referring to all the notes data from MIMIC-III (NOTEEVENTS table ) and Case Reports articles from PubMed, as these contents are close to medical notes.  To create the test set, we first followed the approach by BIBREF6 who applied an auto-generating method.  Initially, we built a term-sense dictionary from the training dataset.Then we did matching for the sense words or phrases in the MIMIC-III notes dataset, and once there is a match, we replaced the words or phrases with the abbreviation term .  We then asked two researchers with a medical background to check the matched samples manually with the following judgment:  given this sentence and the abbreviation term and its sense, do you think the content is enough for you to guess the sense and whether this is the right sense?  To estimate the agreement on the annotations, we selected a subset which contains 120 samples randomly and let the two annotators annotate individually.We got a Kappa score BIBREF27 of 0.96, which is considered as a near perfect agreement.  We then distributed the work to the two annotators, and each of them labeled a half of the dataset, which means each sample was only labeled by a single annotator.  For some rare term-sense pairs, we failed to find samples from MIMIC-III.  The annotators then searched these senses via PubMed data source manually, aiming to find clinical notes-like sentences.They picked good sentences from these results as testing samples where the keywords exist and the content is informative.  For those senses that are extremely rare, we let the annotators create sentences in the clinical note style as testing samples according to their experiences.  Eventually, we have a balanced testing dataset, where each term-sense pair has around 15 samples for testing (on average, each pair has 14.56 samples and the median sample number is 15), and a comparison with training dataset is shown in Figure FIGREF11.  Due to the difficulty for collecting the testing dataset, we decided to only collect for a random selection of 30 terms.On average, it took few hours to generate testing samples for each abbreviation term per researcher.  Case StudyWe now select two representative terms AC, IM and plot their receiver operating characteristic(ROC) curves.  The term has a relatively large number of classes and the second one has extremely unbalanced training samples.  We show the details in Table TABREF16.  We have 8 classes in term AC.Figure FIGREF15(a) illustrates the results of our best performed model and Figure FIGREF15(b) shows the results of the base model.  The accuracy and F1 score have an improvement from 0.3898 and 0.2830 to 0.4915 and 0.4059 respectively.  Regarding the rare senses (for example, class 0, 1, 4 and 6), we can observe an increase in the ROC areas.  Class 6 has an obvious improvement from 0.75 to 1.00.Such improvements in the rare senses make a huge difference in the reported average accuracy and F1 score, since we have a nearly equal number of samples for each class in the testing data.  Similarly, we show the plots for IM term in Figure FIGREF15(c) and FIGREF15(d).  IM has only two classes, but they are very unbalanced in training set, as shown in Table TABREF16.  The accuracy and F1 scores improved from 0.6667 and 0.6250 to 0.8667 and 0.8667 respectively.We observe in the ROC both classes.  This observation further that our is more to the class samples the model, even the have , by curves comparingUC we model, which applies ELMo and topic- better representation ability under the setting of few-shot learning.... [The rest of the article is omitted]

Question:
To what baseline models is proposed model compared?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

