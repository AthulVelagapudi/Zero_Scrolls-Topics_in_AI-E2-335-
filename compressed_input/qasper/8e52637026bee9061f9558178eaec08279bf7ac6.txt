<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction Understanding the emotions expressed in a text or message is of high relevance nowadays.  Companies are interested in this to get an understanding of the sentiment of their current customers regarding their products and the sentiment of their potential customers to attract new ones.  Moreover, changes in a product or a company may also affect the sentiment of a customer.However, the intensity of an emotion is crucial in determining the urgency and importance of that sentiment.  If someone is only slightly happy about a product, is a customer willing to buy it again?  Conversely, if someone is very angry about customer service, his or her complaint might be given priority over somewhat milder complaints.BIBREF0 present four tasks in which systems have to automatically determine the intensity of emotions (EI) or the intensity of the sentiment (Valence) of tweets in the languages English, Arabic, and Spanish.  The goal is to either predict a continuous regression (reg) value or to do ordinal classification (oc) based on a number of predefined categories.  The EI tasks have separate training sets for four different emotions: anger, fear, joy and sadness.  Due to the large number of subtasks and the fact that this language does not have many resources readily available, we only focus on the Spanish subtasks.Our work the following:  Our submissions ranked second (E-Reg),-O (), is in automatically determining the intensity of emotions and sentiment of Spanish tweets.  This paper will first focus on the datasets, the data generation procedure, and the techniques and tools used.Then we present the results in detail, after which we perform a small error analysis on the largest mistakes our model made.  We conclude with some possible ideas for future work.  DataFor each the data that was available the organizers is used a of tweets with each label the emotion or sentiment BIBREF1.  Links and usernames were replaced by the general tokens URL and @username, after which the tweets were tokenized by using TweetTokenizer.  All text was lowercased.In a post-processing step, it was ensured that each emoji is tokenized as a single token.  Word EmbeddingsTo be able to train word embeddings, Spanish tweets were scraped between November 8, 2017 and January 12, 2018.  We chose to create our own embeddings instead of using pre-trained embeddings, because this way the embeddings would resemble the provided data set: both are based on Twitter data.  Added to this set was the Affect in Tweets Distant Supervision Corpus (DISC) made available by the organizers BIBREF0 and a set of 4.1 million tweets from 2015, obtained from BIBREF2 .  After removing duplicate tweets and tweets with fewer than ten tokens, this resulted in a set of 58.7 million tweets, containing 1.1 billion tokens.The tweets were the in Section SECREF6.The were usingvec the library BIBREF3, usingOW, a window size of 40 and a minimum count of 5.  The feature vectors for each tweet were then created by using the AffectiveTweets WEKA package BIBREF4.Translating Lexicons  Most lexical resources for sentiment analysis are in English.  To still be able to benefit from these sources, the lexicons in the AffectiveTweets package were translated to Spanish, using the machine translation platform Apertium BIBREF5 .All lexicons from the AffectiveTweets package were translated, except for SentiStrength.  Instead of translating this lexicon, the English version was replaced by the Spanish variant made available by BIBREF6 .For each subtask, the optimal combination of lexicons was determined.  This was done by first calculating the benefits of adding each lexicon individually, after which only beneficial lexicons were added until the score did not increase anymore (e.g. after adding the best four lexicons the fifth one did not help anymore, so only four were added).  The tests were performed using a default SVM model, with the set of word embeddings described in the previous section.  Each subtask thus uses a different set of lexicons (see Table TABREF1 for an overview of the lexicons used in our final ensemble).For each subtask, this resulted in a (modest) increase on the development set, between 0.01 and 0.05.  Translating DataThe training set by BIBREF is not, it was to find to set.  A possible to simply the datasets into, the   the present study on Spanish, tweets the English Spanish.  This new set of “Spanish” data was then added to our original training set.Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets.  Algorithms  UsedThree of models in our system, a anVM. neural were of Prayas BIBREF7 the previous  Different algorithms (., tried the of SeerNet8 but our study to results for Spanish.For both the LM the network, a parameter search was done for the number of layers, the number of nodes and dropout used.  This was done for each subtask, i.e. different tasks can have a different number of layers.  All models were implemented using Keras BIBREF9.After the best parameter settings were found, the results of 10 system runs to produce our predictions were averaged (note that this is different from averaging our different type of models in Section SECREF16 ).  For the SVM (implemented in scikit-learn BIBREF10 ), the RBF kernel was used and a parameter search was conducted for epsilon.  Detailed parameter settings for each subtask are shown in Table TABREF12 .  Each parameter search was performed using 10-fold cross validation, as to not overfit on the development set.Semi-supervised Learning  One of the aims of this study was to see if using semi-supervised learning is beneficial for emotion intensity tasks.For this theC BIBREF0. byying certain words, which it suitable a-   the emotion the tweet not made public.  Therefore, a method was applied to automatically assign the tweets to an emotion by comparing our scraped tweets to this new data set., in an to the query- we the 100 words which in the DISC corpus, in comparison with their frequencies in our own scraped tweets corpus.  Words that were clearly not indicators of emotion were removed.  The rest was annotated per emotion or removed if it was unclear to which emotion the word belonged.This allowed to create silver datasets per emotion, assigning to emotion if anated emotion-word.  Ourvised approach is quite straightforward: first a on the set and then is used to predict the labels of the silver data.  This silver data is then simply added to our training set, after which the model is retrained.However, an extra step is applied to ensure that the silver data is of reasonable quality.  Instead of training a single model initially, ten different models were trained which predict the labels of the silver instances.  If the highest and lowest prediction do not differ more than a certain threshold the silver instance is maintained, otherwise it is discarded.This in two parameters that be:  the threshold and the number of silver instances that would be added.  This method can be applied to both the LSTM and feed-forward networks that were used.  An overview of the characteristics of our data set with the final parameter settings is shown in Table TABREF14.Usually, only a small subset of data was added to our training set, meaning that most of the silver data is not used in the experiments.  Note that since only the emotions were annotated, this method is only applicable to the EI tasks.  EnsemblingTo boost, the SVM, L andforward models were combined into ensemble.  For the andforward, three different trained  The trained the data (regular on and translated (lated third training data and the semi-supervised data (silver). the the S, learning not help, so the regular and translated model were trained in this case.  This results in 8 different models per subtask.  Note that for the valence tasks no silver training data was obtained, meaning that for those tasks the semi-supervised models could not be used.Per task, theM andforward model's predictions over 10 prediction.   the of all were combined into an average , were ensemble a manner the removal the average  This done on their original,. starting out by worst individual model and working our way up to the best model.We only consider it an increase in score if the difference is larger than 0.002 (i.e. the difference between 0.716 and 0.718).  If at some point the score does not increase and we are therefore unable to remove a model, the process is stopped and our best ensemble of models has been found.  This process uses the scores on the development set of different combinations of models.  Note that this means that the ensembles for different subtasks can contain different sets of models.The final model selections can be found in Table TABREF17.  Results and DiscussionTable TABREF18 shows the results on the development set of all individuals models, distinguishing the three types of training:  regular (r), translated (t) and semi-supervised (s).  In Tables TABREF17 and TABREF18 , the letter behind each model (e.g. SVM-r, LSTM-r) corresponds to the type of training used.  Comparing the regular and translated columns for the three algorithms, it shows that in 22 out of 30 cases, using translated instances as extra training data resulted in an improvement.For the-, an improvement is in 15 of. , our best individual model for eachask (ed Table TABREF18 is always a or  18, our network results, having the highest F-score for 8 out of 10 subtasks.However, Table TABREF19 shows that these scores can still be improved by averaging or ensembling the individual models.  On the dev set, averaging our 8 individual models results in a better score for 8 out of 10 subtasks, while creating an ensemble beats all of the individual models as well as the average for each subtask.  On the test set, however, only a small increase in score (if any) is found for stepwise ensembling, compared to averaging.  Even though the results do not get worse, we cannot conclude that stepwise ensembling is a better method than simply averaging.Our official scores (column Ens Test in Table TABREF19 ) have placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.  However, it is evident that the results obtained on the test set are not always in line with those achieved on the development set.  Especially on the anger subtask for both EI-Reg and EI-Oc, the scores are considerably lower on the test set in comparison with the results on the development set.Therefore, a small error analysis was performed on the instances where our final model made the largest errors.  Error Analysis to some large between our results the dev set of this we a small analysis to what.  For E-Reg- the gold were to our we checked 50 our system made the.  Some examples that were indicative of the shortcomings of our system are shown in Table TABREF20.First, our system not account capital.  The are the first sentence whereization the emotion used   the the name Imperator is not understood.  Since our texts were lowercased, our system was unable to capture the named entity and thought the sentence was about an angry emperor instead. the third, our system to capture that when you are so that it you it a reduced the an. , is theative language me infla la vena that  The two error-ategories be by including smart features regarding capitalization and named entity recognition.However, the last two categories are problems of natural language understanding and will be very difficult to fix.  Conclusion To conclude, the present study described our submission for the Semeval 2018 Shared Task on Affect in Tweets.We participated in four Spanish subtasks and our submissions ranked second, second, fourth and fifth place.  Our study aimed to investigate whether the automatic generation of additional training data through translation and semi-supervised learning, as well as the creation of stepwise ensembles, increase the performance of our Spanish-language models.  Strong support was found for the translation and semi-supervised learning approaches; our best models for all subtasks use either one of these approaches.  These results suggest that both of these additional data resources are beneficial when determining emotion intensity (for Spanish). the of a stepwise ensemble from the best models not in performance to simply.  In, somefitting the were found.  In future work, we would like to apply the methods (translation and semi-supervised learning) used on Spanish on other low-resource languages and potentially also on other tasks.... [The rest of the article is omitted]

Question:
How was the training data translated?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

