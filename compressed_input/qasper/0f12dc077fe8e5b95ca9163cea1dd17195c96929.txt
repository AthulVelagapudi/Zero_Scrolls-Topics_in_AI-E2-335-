<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction  [0]leftmargin=*  [0]leftmargin=*omatic systems have significant and on all of.   is their to benefit society by promoting, and , machines not take bribes to they can determine for without the can provide access to information and services without discrimination based on gender or sexual orientation.Nonetheless, as machine learning systems become more human-like in their predictions, they can also perpetuate human biases.  Some learned biases may be beneficial for the downstream application (e.g., learning that humans often use some insect names, such as spider or cockroach, to refer to unpleasant situations).  Other biases can be inappropriate and result in negative experiences for some groups of people.  Examples include, loan eligibility and crime recidivism prediction systems that negatively assess people belonging to a certain pin/zip code (which may disproportionately impact people of a certain race)BIBREF0 and resumé sorting systems that believe that men are more qualified to be programmers than women BIBREF1 .  Similarly, sentiment and emotion analysis systems can also perpetuate and accentuate inappropriate human biases, e.g., systems that consider utterances from one race or gender to be less positive simply because of their race or gender, or customer support systems that prioritize a call from an angry male over a call from the equally angry female.  Predictions of machine learning systems have also been shown to be of higher quality when dealing with information from some groups of people as opposed to other groups of people.For example, in the area of vision, gender classification systems perform particularly poorly for darker skinned females BIBREF2.  Natural language processing (NLP) systems have been shown to be poor in understanding text produced by people belonging to certain races BIBREF3, BIBREF4.  For NLP systems, the sources of the bias often include the training data, other corpora, lexicons, and word embeddings that the machine learning algorithm may leverage to build its prediction model. though is some recent work highlighting such inappropriate ( work), each such has on two and.   no benchmark for examining in we we of,640 sentences to biases certainWe will to the Equity (EEC).We used the EEC as a supplementary test set in a recent shared task on predicting sentiment and emotion intensity in tweets,  SemEval-2018 Task 1: Affect in Tweets BIBREF5 .  In particular, we wanted to test a hypothesis that a system should equally rate the intensity of the emotion expressed by two sentences that differ only in the gender/race of a person mentioned.  Note that here the term system refers to the combination of a machine learning architecture trained on a labeled dataset, and possibly using additional language resources.The bias can from any or these.  We were to the E to examine 219 sentiment that theWe compare emotion sentiment scores systems onEC that word to., `This man made me feel angry' vs. `This woman made me feel angry').We find that the majority of the systems studied show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one race or one gender.  We also find that the bias may be different depending on the particular affect dimension that the natural language system is trained to predict.  Despite the work we describe here and what others have proposed in the past, it should be noted that there are no simple solutions for dealing with inappropriate human biases that percolate into machine learning systems.It seems difficult to ever be able to identify and quantify all of the inappropriate biases perfectly (even when restricted to the scope of just gender and race).  Further, any such mechanism is liable to be circumvented, if one chooses to do so.  Nonetheless, as developers of sentiment analysis systems, and NLP systems more broadly, we cannot absolve ourselves of the ethical implications of the systems we build.  Even if it is unclear how we should deal with the inappropriate biases in our systems, we should be measuring such biases.The Equity Evaluation Corpus is not meant to be a catch-all for all inappropriate biases, but rather just one of the several ways by which we can examine the fairness of sentiment analysis systems.  We make the corpus freely available so that both developers and users can use it, and build on it.  Related WorkRecent studies have demonstrated that the systems trained on the human-written texts learn human-like biases BIBREF1, BIBREF6.  In general, any predictive model built on historical data may inadvertently inherit human biases based on gender, ethnicity, race, or religion BIBREF7, BIBREF8.  Discrimination-aware data mining focuses on measuring discrimination in data as well as on evaluating performance of discrimination-aware predictive models BIBREF9, BIBREF10, BIBREF11, BIBREF12.In the attention been oningsa powerful asdimensional.The of, News or  B in analysis has been in simple thatputed word embeddings BIBREF13.There is no prior work that systematically quantifies the extent of bias in a large number of sentiment analysis systems.  This paper does not examine the differences in accuracies of systems on text produced by different races or genders, as was done by hovy2015demographic,blodgett2016demographic,jens2017incorporating,buolamwini2018gender.  Approaches on how to mitigate inappropriate biases BIBREF14, BIBREF1, BIBREF15, BIBREF16, BIBREF13, BIBREF17, BIBREF18 are also beyond the scope of this paper.See also the position paper by hovy2016social, which identifies socio-ethical implications of the NLP systems in general.  The Equity Evaluation CorpusWe now describe how we compiled a dataset of thousands of sentences to determine whether automatic systems consistently give higher (or lower) sentiment intensity scores to sentences involving a particular race or gender.  There are several ways in which such a dataset may be compiled.  We present below the choices that we made.We use sentences involving one--.The were be and wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems.  We, the authors of this paper, developed eleven sentence templates after several rounds of discussion and consensus building.They are shown in Table TABREF3 .  The templates are divided into two groups.  The first type (templates 1–7) includes emotion words.  The purpose of this set is to have sentences expressing emotions.The second type (templates 8–11) does not include any emotion words.  The purpose of this set is to have non-emotional (neutral) sentences.  The templates include two variables:INLINEFORM0 person INLINEFORM1 and INLINEFORM2 emotion word  INLINEFORM3 .  We generate sentences from the template by instantiating each variable with one of the pre-chosen values that the variable can take.  Each of the eleven templates includes the variable INLINEFORM4 personINLINEFORM5.  INLINEFORM6 person  INLINEFORM7 can be instantiated by any of the following noun phrases: chose ten of byan: TableABREF7 ).  The full lists of noun phrases representing females and males, used in our study, are shown in Table TABREF8.  The second variable, INLINEFORM0 emotion word INLINEFORM1, has two variants.plates through four a for an state.The words to four , for each of the emotions, we selected five words that convey that emotion in varying intensities.  These words were taken from the categories in the Roget's Thesaurus corresponding to the four emotions:category #900  Resentment (for anger),  category #860 Fear (for fear), category #836 Cheerfulness (for joy), and category #837 Dejection (for sadness).  Templates five through seven include emotion words describing a situation or event.These were sameaurus.The ofotional emotional TableABREF.  We generated sentences from the templates by replacing INLINEFORM0 person INLINEFORM1 and INLINEFORM2 emotion word INLINEFORM3 variables with the values they can take.In total, 8,640 sentences were generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates.  We manually examined the sentences to make sure they were grammatically well-formed.  Notably, one can derive pairs of sentences from the EEC such that they differ only in one word corresponding to gender or race (e.g., `My daughter feels devastated' and `My son feels devastated').  We refer to the full set of 8,640 sentences as Equity Evaluation Corpus.Measuring Race and Gender Bias in Automatic Sentiment Analysis Systems  The race and gender bias evaluation was carried out on the output of the 219 automatic systems that participated in SemEval-2018 Task 1: Affect in Tweets BIBREF5 .The shared task included five subtasks on inferring the affectual state of a person from their tweet:  1. emotion intensity regression,  2. emotion intensity ordinal classification,  3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification.For each subtask, labeled data were provided for English, Arabic, and Spanish.  The race and gender bias were analyzed for the system outputs on two English subtasks:  emotion intensity regression (for anger, fear, joy, and sadness) and valence regression.  These regression tasks were formulated as follows: a and anive dimension ( or), the A theeteravaluedast A) and 1 (most A).  Separate training and test datasets were provided for each affective dimension.  Training sets included tweets along with gold intensity scores.Two test sets were provided for each task:  1. a regular tweet test set (for which the gold intensity scores are known but not revealed to the participating systems), and 2. the Equity Evaluation Corpus (for which no gold intensity labels exist).  Participants were told that apart from the usual test set, they are to run their systems on a separate test set of unknown origin.  The participants were instructed to train their system on the tweets training sets provided, and that they could use any other resources they may find or create. were to the same final system the two.The the was the first (twe was to evaluate and the quality the systems' predictions.  The second (EEC) test set was used to perform the bias analysis, which is the focus of this paper.Systems:  Fifty teams submitted their system outputs to one or more of the five emotion intensity regression tasks (for anger, fear, joy, sadness, and valence), resulting in 219 submissions in total.  Many systems were built using two types of features:deep neural network representations of tweets (sentence embeddings) and features derived from existing sentiment and emotion lexicons.  These features were then combined to learn a model using either traditional machine learning algorithms (such as SVM/SVR and Logistic Regression) or deep neural networks.  SVM/SVR, LSTMs, and Bi-LSTMs were some of the most widely used machine learning algorithms.  The sentence embeddings were obtained by training a neural network on the provided training data, a distant supervision corpus (e.g., AIT2018 Distant Supervision Corpus that has tweets with emotion-related query terms), sentiment-labeled tweet corpora (e.g., Semeval-2017 Task4A dataset on sentiment analysis in Twitter), or by using pre-trained models (e.g., DeepMoji BIBREF20 , Skip thoughts BIBREF21 ).Theicon features were the NRCicons BIBREF,N and Bing.We a baselineVM using word features the training data (SVM-Unigrams).  This system is also included in the current analysis.Measuring bias:  To examine gender bias, we compared each system's predicted scores on the EEC sentence pairs as follows:, eleven of ( from ten noun the averages on name each template– word.   were twenty in ( and the 8–11).  In total, INLINEFORM0 pairs of scores were compared.Similarly, to examine race bias, we compared pairs of system predicted scores as follows:  Thus, one pair of scores was examined for each template–emotion word instantiation.  In total, INLINEFORM0 pairs of scores were compared.For each we the paired mean two of scores ( two races two.We set the significance level to 0.05.  However, since we performed 438 assessments (219 submissions evaluated for biases in both gender and race), we applied Bonferroni correction.The null hypothesis that the true mean difference between the paired samples was zero was rejected if the calculated p-value fell below INLINEFORM0 .  ResultsThe two sub-sections below present the results from the analysis for gender bias and race bias, respectively.  Gender Bias ResultsIndividual submission results were communicated to the participants.  Here, we present the summary results across all the teams.  The goal of this analysis is to gain a better understanding of biases across a large number of current sentiment analysis systems.  Thus, we partition the submissions into three groups according to the bias they show:F=M not significant: submissions that showed no statistically significant difference in intensity scores predicted for corresponding female and male noun phrase sentences,  F INLINEFORM0 –MINLINEFORM1 significant: submissions that consistently gave higher scores for sentences with female noun phrases than for corresponding sentences with male noun phrases,  F INLINEFORM0 –M INLINEFORM1 significant: submissions that consistently gave lower scores for sentences with female noun phrases than for corresponding sentences with male noun phrases.For each system and each sentence pair, we calculate the score difference INLINEFORM0 as the score for the female noun phrase sentence minus the score for the corresponding male noun phrase sentence.  Table TABREF24 presents the summary results for each of the bias groups.  It has the following columns:#Subm.: number of submissions in each group.  If all the systems are unbiased, then the number of submissions for the group F=M not significant would be the maximum, and the number of submissions in all other groups would be zero.Avg. score difference  F  INLINEFORM0 –M  INLINEFORM1 :the average INLINEFORM2 for only those pairs where the score for the female noun phrase sentence is higher.  The greater the magnitude of this score, the stronger the bias in systems that consistently give higher scores to female-associated sentences.  Avg. score differenceF  INLINEFORM0 –M  INLINEFORM1 : the average INLINEFORM2 for only those pairs where the score for the female noun phrase sentence is lower.  The greater the magnitude of this score, the stronger the bias in systems that consistently give lower scores to female-associated sentences.Note that these numbers were first calculated separately for each submission, and then averaged over all the submissions within each submission group.  The results are reported separately for submissions to each task (anger, fear, joy, sadness, and sentiment/valence intensity prediction). that the four emotion intensity, only about 12 the 46 () showed no significant.   the the 36 submissions (14% of the submissions) showed no statistically significant score difference.  Thus 75% to 86% of the submissions consistently marked sentences of one gender higher than another.When predicting anger, joy, or valence, the number of systems consistently giving higher scores to sentences with female noun phrases (21–25) is markedly higher than the number of systems giving higher scores to sentences with male noun phrases (8–13).  (Recall that higher valence means more positive sentiment.)  In contrast, on the fear task, most submissions tended to assign higher scores to sentences with male noun phrases (23) as compared to the number of systems giving higher scores to sentences with female noun phrases (12).  When predicting sadness, the number of submissions that mostly assigned higher scores to sentences with female noun phrases (18) is close to the number of submissions that mostly assigned higher scores to sentences with male noun phrases (16). some as and involving male more fearful BIBREF.  Figure FIGREF25 shows the score differences ( INLINEFORM0 ) for individual systems on the valence regression task.  Plots for the four emotion intensity prediction tasks are shown in Figure FIGREF31 in the Appendix.Each point ( ▲, ▼, ●) on the plot corresponds to the difference in scores predicted by the system on one sentence pair.  The systems are ordered by their rank (from first to last) on the task on the tweets test sets, as per the official evaluation metric (Spearman correlation with the gold intensity scores).  We will refer to the difference between the maximal value of INLINEFORM1 and the minimal value of INLINEFORM2 for a particular system as the INLINEFORM3 –spread.  Observe that the INLINEFORM4 –spreads for many systems are rather large, up to 0.57.Depending on the task, the top 10 or top 15 systems as well as some of the worst performing systems tend to have smaller INLINEFORM5 –spreads while the systems with medium to low performance show greater sensitivity to the gender-associated words.  Also, most submissions that showed no statistically significant score differences (shown in green) performed poorly on the tweets test sets.  Only three systems out of the top five on the anger intensity task and one system on the joy and sadness tasks showed no statistically significant score difference.  This indicates that when considering only those systems that performed well on the intensity prediction task, the percentage of gender-biased systems are even higher than those indicated above.These results raise further questions such as `what exactly is the cause of such biases?' and `why is the bias impacted by the emotion task under consideration?'.  Answering these questions will require further information on the resources that the teams used to develop their models, and we leave that for future work.Average score:  For that showed difference F INLINEFORM0 –1 were4   the intensity scores,.The ( ) was high as 0.34.Note, however, that these INLINEFORM6 s are the result of changing just one word in a sentence.  In more complex sentences, several gender-associated words can appear, which may have a bigger impact.  Also, whether consistent score differences of this magnitude will have significant repercussions in downstream applications, depends on the particular application.Analyses on only the neutral sentences in EEC and only the emotional sentences in EEC:  We also performed a separate analysis using only those sentences from the EEC that included no emotion words.  Recall that there are four templates that contain no emotion words.  Tables TABREF26 shows these results.We observe similar trends as in the analysis on the full set.  One noticeable difference is that the number of submissions that showed statistically significant score difference is much smaller for this data subset.  However, the total number of comparisons on the subset (44) is much smaller than the total number of comparisons on the full set (1,584), which makes the statistical test less powerful.  Note also that the average score differences on the subset (columns 3 and 4 in Table TABREF26 ) tend to be higher than the differences on the full set (columns 3 and 4 in Table TABREF24 ).This that-associated words a on system predictions for neutral.   also analysis by to only the the words emotion., submissions to the anger intensity prediction task were evaluated only on sentences with anger words).  The results (not shown here) were similar to the results on the full set.Race Bias Results  We did a similar analysis for race as we did for gender.For each submission on each we the the average score the of sentences with American (AA) names and the average predicted score on the set of sentences with European American (EA) names.  Then, we aggregated the results over all such sentence pairs in the EEC.  Table TABREF29 shows the results.The the same the gender.  Obs of submissions no statistically significant score difference for sentences pertaining to the two races is about 5–11 (about 11% to 24%) for the four emotions and 3 (about 8%) for valence.  These numbers are even lower than what was found for gender.The the systems assigned to with American names on the tasks of anger, fear, and sadness intensity prediction.  On the joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names.  These tendencies reflect some common stereotypes that associate African Americans with more negative emotions BIBREF28.Figure FIGREF28 shows the score differences for individual systems on race sentence pairs on the valence regression task.  Plots for the four emotion intensity prediction tasks are shown in Figure FIGREF32 in the Appendix.  Here, the INLINEFORM0 –spreads are smaller than on the gender sentence pairs—from 0 to 0.15.  As in the gender analysis, on the valence task the top 13 systems as well as some of the worst performing systems have smaller INLINEFORM1 –spread while the systems with medium to low performance show greater sensitivity to the race-associated names., we not the emotion intensity. , the gender, most submissions that no significant differences obtained the tweets test   one system the five the fear and the , this of the exact causes of such biases.We hope to explore this in future work.  Discussion As mentioned in the introduction, bias can originate from any or several parts of a system:the labeled and unlabeled datasets used to learn different parts of the model, the language resources used (e.g., pre-trained word embeddings, lexicons), the learning method used (algorithm, features, parameters), etc.  In our analysis, we found systems trained using a variety of algorithms (traditional as well as deep neural networks) and a variety of language resources showing gender and race biases.  Further experiments may tease out the extent of bias in each of these parts. also the our baselineVM system using word). system any languageWe also shows inThe INLINEFORM0 -spreads were09.2 gender and. on the race sentence pairs.The predicted intensity scores tended to be higher on the sentences with male noun phrases than on the sentences with female noun phrases for the tasks of anger, fear, and sadness intensity prediction.  This tendency was reversed on the task of valence prediction.  On the race sentence pairs, the system predicted higher intensity scores on the sentences with European American names for all four emotion intensity prediction tasks, and on the sentences with African American names for the task of valence prediction.  This indicates that the training data contains some biases (in the form of some unigrams associated with a particular gender or race tending to appear in tweets labeled with certain emotions).The labeled datasets for the shared task were created using a fairly standard approach:  polling Twitter with task-related query terms (in this case, emotion words) and then manually annotating the tweets with task-specific labels.  The SVM-Unigram bias results show that data collected by distant supervision can be a source of bias.  However, it should be noted that different learning methods in combination with different language resources can accentuate, reverse, or mask the bias present in the training data to different degrees.Conclusions and Future Work  We created the Equity Evaluation Corpus (EEC), which consists of 8,640 sentences specifically chosen to tease out gender and race biases in natural language processing systems.We the E to analyze 219 N that recent international task on predicting sentiment emotion. that than systems to mark sentences involving one gender/race with found more for bias be different depending on the particular affect dimension involved.We the score differences across across be small ( INLINEFORM0 is the). some differences reached as high as 0.34 (34%).  What impact a consistent bias, even with an average magnitude INLINEFORM2, might have in downstream applications merits further investigation.We plan to extend the EEC with sentences associated with country names, professions (e.g., doctors, police officers, janitors, teachers, etc.), fields of study (e.g., arts vs. sciences), as well as races (e.g., Asian, mixed, etc.) and genders (e.g., agender, androgyne, trans, queer, etc.) not included in the current study.  We can then use the corpus to examine biases across each of those variables as well.  We are also interested in exploring which systems (or what techniques) accentuate inappropriate biases in the data and which systems mitigate such biases. we are in the of analysis predictions varies when to text different as and.  The Equity Evaluation Corpus the proposed methodology to are not, using several and datasets such as the one proposed here can bring about a more thorough examination of inappropriate biases in modern machine learning systems.Appendix Figures FIGREF31 and FIGREF32 show box plots of the score differences for each system on the four emotion intensity regression tasks on the gender and race sentence pairs, respectively.  Each point on a plot corresponds to the difference in scores predicted by the system on one sentence pair.The systems are ordered by their performance rank (from first to last) on the task as per the official evaluation metric on the tweets test sets.... [The rest of the article is omitted]

Question:
What criteria are used to select the 8,640 English sentences?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

