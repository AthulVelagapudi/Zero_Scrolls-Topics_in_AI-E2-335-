<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction  Writing errors can occur in many different forms – from relatively simple punctuation and determiner errors, to mistakes including word tense and form, incorrect collocations and erroneous idioms.  Automatically identifying all of these errors is a challenging task, especially as the amount of available annotated data is very limited.Rei2016 showed that while some error detection algorithms perform better than others, it is additional training data that has the biggest impact on improving performance.  Being able to generate realistic artificial data would allow for any grammatically correct text to be transformed into annotated examples containing writing errors, producing large amounts of additional training examples.  Supervised error generation systems would also provide an efficient method for anonymising the source corpus – error statistics from a private corpus can be aggregated and applied to a different target text, obscuring sensitive information in the original examination scripts.However, the task of creating incorrect data is somewhat more difficult than might initially appear – naive methods for error generation can create data that does not resemble natural errors, thereby making downstream systems learn misleading or uninformative patterns.  Previous work on artificial error generation (AEG) has focused on specific error types, such as prepositions and determiners BIBREF0 , BIBREF1 , or noun number errors BIBREF2 .  Felice2014a investigated the use of linguistic information when generating artificial data for error correction, but also restricting the approach to only five error types.There been very on generating artificial for types, is for generalpurpose error detection. , the error types by Felice2014a only 35. errors theLL 2014 training, providing no additional information for the majority of errors.  In this paper, we investigate two supervised approaches for generating all types of artificial errors.We propose a framework for generating errors based on statistical machine translation (SMT), training a model to translate from correct into incorrect sentences.  In addition, we describe a method for learning error patterns from an annotated corpus and transplanting them into error-free text.  We evaluate the effect of introducing artificial data on two error detection benchmarks.  Our results show that each method provides significant improvements over using only the available training set, and a combination of both gives an absolute improvement of 4.3% in INLINEFORM0 , without requiring any additional annotated data.Error Generation Methods  We investigate two alternative methods for AEG.The models receive grammatically correct text as input and modify certain tokens to produce incorrect sequences.  The alternative versions of each sentence are aligned using Levenshtein distance, allowing us to identify specific words that need to be marked as errors.  While these alignments are not always perfect, we found them to be sufficient for practical purposes, since alternative alignments of similar sentences often result in the same binary labeling.  Future work could explore more advanced alignment methods, such as proposed by felice-bryant-briscoe.In Section SECREF4, this automatically labeled data is then used for training error detection models.  Machine TranslationWe treat AEG as a translation task – given a correct sentence as input, the system would learn to translate it to contain likely errors, based on a training corpus of parallel data.  Existing SMT approaches are already optimised for identifying context patterns that correspond to specific output sequences, which is also required for generating human-like errors.  The reverse of this idea, translating from incorrect to correct sentences, has been shown to work well for error correction tasks BIBREF2 , BIBREF3 , and round-trip translation has also been shown to be promising for correcting grammatical errors BIBREF4 .Following previous work BIBREF2REF, we abasedT error. ,- sentences in the training data are treated as the source, and the original sentences written by language learners as the target.  Pialign BIBREF6 is used to create a phrase translation table directly from model probabilities.In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table, as proposed by Felice:2014-CoNLL.  Decoding is performed using Moses BIBREF7 and the language model used during decoding is built from the original erroneous sentences in the learner corpus.  The IRSTLM Toolkit BIBREF8 is used for building a 5-gram language model with modified Kneser-Ney smoothing BIBREF9 .Patternraction  We also a for AEG using patterns over and, extracting known incorrect ofated the best method identified by Felice2014a, using error type distributions; while they covered only 5 error types, we relax this restriction and learn patterns for generating all types of errors.The original and corrected sentences the are aligned and to identify short transformation patterns the of (orrect).The of is the affected phrase, plus one of context on both sides.  If a word form changes between the incorrect and correct text, it is fully saved in the pattern, otherwise the POS tags are used for matching.For example, the original sentence `We went shop on Saturday' and the corrected version `We went shopping on Saturday' would produce the following pattern:  (VVD shop_VV0 II, VVD shopping_VVG II)After collecting statistics from the background corpus, errors can be inserted into error-free text.  The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence.  We only use patterns with frequency INLINEFORM0 , which yields a total of 35,625 patterns from our training data.For each input sentence, we first decide how many errors will be generated (using probabilities from the background corpus) and attempt to create them by sampling from the collection of applicable patterns.  This process is repeated until all the required errors have been generated or the sentence is exhausted.  During generation, we try to balance the distribution of error types as well as keeping the same proportion of incorrect and correct sentences as in the background corpus BIBREF10 .  The required POS tags were generated with RASP BIBREF11 , using the CLAWS2 tagset.Error Detection Model  We construct a neural sequence labeling model for error detection, following the previous work BIBREF12, BIBREF13.The model receives a sequence of tokens as input and outputs a prediction for each position, indicating whether the token is correct or incorrect in the current context.  The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings.  Next, the embeddings are given as input to a bidirectional LSTM BIBREF14 , in order to create context-dependent representations for every token.  The hidden states from forward- and backward-LSTMs are concatenated for each word position, resulting in representations that are conditioned on the whole sequence.Thisated through an additional layer, andmax the two possible labels ( is to distribution each.The isised byisingropy the  We use AdaDelta BIBREF for calculating rate training, which accounts for a higher baseline performance compared to previous results.Evaluation  We trained our error generation models on the public FCE training set BIBREF16 and used them to generate additional artificial training data.  Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources:1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens)..  While there are other text corpora that could be used (e.g., Wikipedia and news articles), our development experiments showed that keeping the writing style and vocabulary close to the target domain gives better results compared to simply including more data.  We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens)BIBREF3 .  Each artificial error generation system was used to generate 3 different versions of the artificial data, which were then combined with the original annotated dataset and used for training an error detection system.  Table TABREF1 contains example sentences from the error generation systems, highlighting each of the edits that are marked as errors.The error detection results can be seen in Table TABREF4 .  We use INLINEFORM0 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task BIBREF3 .  INLINEFORM1 calculates a weighted harmonic mean of precision and recall, which assigns twice as much importance to precision – this is motivated by practical applications, where accurate predictions from an error detection system are more important compared to coverage.  For comparison, we also report the performance of the error detection system by Rei2016, trained using the same FCE dataset.The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods.  When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance.  While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. the pattern the machine system (Ann+AT) performance all.The two frameworks to generate of and of both to in  We theimate Random Test BIBREF17,18 to calculate significance the improvement for the systems using significant over only annotation. the final combination also significantly better the Felice2014, on all three.  While Rei2016 also report separate experiments that achieve these a larger proprietary  In we compare error detection frameworks trained on the same publicly available FCE dataset, thereby removing the confounding factor of dataset size and only focusing on the model architectures.The error generation methods can generate alternative versions of the same input text – the pattern-based method randomly samples the error locations, and the SMT system can provide an n-best list of alternative translations.  Therefore, we also investigated the combination of multiple error-generated versions of the input files when training error detection models.  Figure FIGREF6 shows the INLINEFORM0 score on the development set, as the training data is increased by using more translations from the n-best list of the SMT system.  These results reveal that allowing the model to see multiple alternative versions of the same file gives a distinct improvement – showing the model both correct and incorrect variations of the same sentences likely assists in learning a discriminative model.Related Work  Our work builds on prior research into AEG.Brockett2006 constructed for transforming correct to contain noun number.  Rozovkaya2010a learned confusion from corpus generate pre  F2009 a generating errors for types using patterns user or , their method uses a limited number of edit operations and is thus unable to generate complex errors.Cill2013 compared different training and that artificial errors helped correct pre.  2014a learned error type for generating and the system in Section SECREF3 is this  While specific subset of error types, we explored two holistic approaches to AEG and showed that they are able to significantly improve error detection performance.Conclusion paper investigated two AEG, to create additional for.   we explored a method using textual patterns learned from an annotated corpus, which are used for inserting errors into correct input text.  In addition, we proposed formulating error generation as an MT framework, learning to translate from grammatically correct to incorrect sentences.The of artificial process was three error detection annotations, using the FCE and CoNLL 2014 datasets.  Making use of artificial data provided improvements for all data generation methods.  By relaxing the type restrictions and generating all types of errors, our pattern-based method consistently outperformed the system by Felice2014a.The combination of the pattern-based method with the machine translation approach gave further substantial improvements and the best performance on all datasets.... [The rest of the article is omitted]

Question:
What are their results on both datasets?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

