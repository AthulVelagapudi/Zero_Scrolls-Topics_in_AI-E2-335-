<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction This work is licensed under a Creative Commons Attribution 4.0 International License.  License details: http://creativecommons.org/licenses/by/4.0/.Sentence pair modeling is a fundamental technique underlying many NLP tasks, including the following:  Traditionally, researchers had to develop different methods specific for each task.  Now neural networks can perform all the above tasks with the same architecture by training end to end.Various neural models  BIBREF1, BIBREF0, BIBREF10, BIBREF11, BIBREF12,  BIBREF13,  BIBREF14,BIBREF15  have declared state-of-the-art results for sentence pair modeling tasks; however, they were carefully designed and evaluated on selected (often one or two) datasets that can demonstrate the superiority of the model.  The research questions are as follows:  Do they perform well on other tasks and datasets? much performance is to certain system and hyperparameter optimizations?  To answer these questions and better understand different network designs, we systematically analyze and compare the state-of-the-art neural models across multiple tasks and multiple domains.  Namely, we implement five models and their variations on the same PyTorch platform:InferSent model BIBREF16,  Shortcut-stacked Sentence Encoder Model BIBREF17,  Pairwise Word Interaction Model BIBREF1,  Decomposable Attention Model BIBREF10, and Enhanced Sequential Inference Model BIBREF0.They are representative of the two most common approaches:  sentence encoding models that learn vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance and sentence pair interaction models that use some sorts of word alignment mechanisms (e.g., attention) then aggregate inter-sentence interactions.  We focus on identifying important network designs and present a series of findings with quantitative measurements and in-depth analyses, including (i) incorporating inter-sentence interactions is critical;  (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter data;() Enhancedentialference Model the most consistent performance for whilewise Word datasets and Shortcut-Stacked Sentence Encoder Model is the best performaning model on the Quora corpus.  We release our implementations as a toolkit to the research community.  General Framework for Sentence Pair ModelingVarious neural networks for sentence pair modeling, all of which fall into two types of approaches.  The sentence encoding approach encodes each sentence into a fixed-length vector and then computes sentence similarity directly.  The model of this type has advantages in the simplicity of the network design and generalization to other NLP tasks.The sentence pair interaction approach takes word alignment and interactions between the sentence pair into account and often show better performance when trained on in-domain data.  Here we outline the two types of neural networks under the same general framework:  Representative Models for Sentence Pair Modeling 1 of typical for sentence pair modeling in.In we investigate in: two are the sentence encoding type of the interaction-   models have reported state-or-the-art results with varied architecture design (this section) and implementation details (Section "Implementation Details" ).The Bi-LSTM Max-pooling Network (InferSent)  We choose the simple Bi-LSTM max-pooling network from InferSent  BIBREF16 :$$&\overarrow{\mathbf {h}}_{i}  = BiLSTM(\mathbf {x}_{i}, \overleftrightarrow{\mathbf {h}}_{i-1}) \\  &\mathbf {v}=max(\overleftrightarrow{\mathbf {h}}_{1}, \overleftrightarrow{\mathbf {h}}_{2},..., \overleftrightarrow{\mathbf {h}}_{n})$$    (Eq. 17)  where $\overleftrightarrow{\mathbf {h}}_{i}$ represents the concatenation of hidden states in both directons.It has shown better transfer learning capabilities than several other sentence embedding models, including SkipThought BIBREF31 and FastSent BIBREF32 , when trained on the natural language inference datasets.  The Shortcut-Stacked Sentence Encoder Model (SSE)The Shortcut-Stacked Sentence Encoder model  BIBREF17 sentence-based embedding model, which enhances multi-layer Bi-LSTM with skip connection to avoid training error accumulation, and calculates each layer as follows:  $$&\overleftrightarrow{\mathbf {h}}_{i}^{k} = BiLSTM(\mathbf {x}_{i}^{k}, \overleftrightarrow{\mathbf {h}}_{i-1}^{k}) \\&\mathbf {x}_{i}^{1}=\mathbf {w}_{i} \quad (k=1), \qquad \mathbf {x}_{i}^{k}=[\mathw}_{i}, \overleftrightarrow{\mathbf {h}}_{i}^{k-1}, \overleftrightarrow{\mathbf {h}}_{i}^{k-2}, ...,  \overleftrightarrow{\mathbf {h}}_{i}^{1}]  \quad (k>1) \\&\=mathbf {h}}_{1}^{m}, \overleftrightarrow{\mathbf {h}}_{2}^{m},...,  \overleftrightarrow{\mathbf {h}}_{n}^{m})$$    (Eq. 19)where $\mathbf {x}_{i}^{k}$  is the input of the $k$  th Bi-LSTM layer at time step $i$  ,which is the combination of outputs from all previous layers,  $\overleftrightarrow{\mathbf {h}}_{i}^{k}$  represents the hidden state of the $k$  th Bi-LSTM layer in both directions.The final sentencedingbf { is the-based the the last, where $$ of words sentence$ is the number of Bi-LSTM layers ( $m = 3$ in SSE).  The Pairwise Word Interaction Model (PWIM)In the Pairwise Word Interaction model BIBREF1 , each word vector $\mathbf {w}_{i}$ is encoded with context through forward and backward LSTMs:  $\overrightarrow{\mathbf {h}}_{i} = LSTM^{f}(\mathbf {w}_{i}, \overrightarrow{\mathbf {h}}_{i-1})$ and $\overleftarrow{\mathbf {h}}_{i} = LSTM^{b}(\mathbf {w}_{i}, \overleftarrow{\mathbf {h}}_{i+1})$  .  For every word pair $(\mathbf {w}^a_{i}, \mathbf {w}^b_{j})$ across sentences, the model directly calculates word pair interactions using cosine similarity,E, and product the the layer:(\bfarrow{\mathbf {h}}_{j})  & = [cos(\overrightarrow{\mathbf {h}}_{i}, \overrightarrow{\mathbf {h}}_{j}),\ \bf {Vert \i} \cdot \overrightarrow{\mathbf {h}}_{j}]$$    (Eq. 21)  The above equation not only applies to forward hidden state$\overrightarrow{\mathbf {h}}_{i}$  and backward hidden state $\overleftarrow{\mathbf {h}}_{i}$  , but also to the concatenation $\overleftrightarrow{\mathbf {h}}_{i}= [\overrightarrow{\mathbf {h}}_{i}, \overleftarrow{\mathbf {h}}_{i}]$  and summation $\mathbf {h}^{+}_{i}= \overrightarrow{\mathbf {h}}_{i} + \overleftarrow{\mathbf {h}}_{i}$, aorbf {^{13times |sent$  after padding one extra.  Aï¿½ the to build word alignment: the most and the by   a-layer deep CNN is applied to aggregate the word interaction features for final classification.The Decposable Attention (Dec)   model BIBREF is the attentionbased alignment for pair itof-the-art results on the SNLI dataset with about an order of magnitude fewer parameters than other models (see more in Table 5 ) without relying on word order information.It computes the word pair interaction between $\mathbf {w}_{i}^{a}$ and $\mathbf {w}_{j}^{b}$ (from input sentences $s_a$ and $s_b$ , each with $m$ and $n$ words, respectively) as ${e}_{ij} = {F(\mathbf {w}_{i}^{a})}^{T}  F(\mathbf {w}_{j}^{b})$  , where $F$ is a feedforward network; then alignment is determined as follows:$$&\mathbf {\beta }_{i} = \sum _{j=1}^{n} \frac{exp({e}_{ij})}{\sum _{k=1}^{n} exp({e}_{ik})}  \mathbf {w}_{j}^{b} \quad &\mathbf {\alpha }_{j} = \sum _{i=1}^{m} \frac{exp({e}_{ij})}{\sum _{k=1}^{m} exp({e}_{kj})}  \mathbf {w}_{i}^{a}  $$(Eq.)  wherebf {\ the soft^{a}$ and subphrases $\mathbf {w}_{j}^{b}$  in sentence $s_b$, and vice versa for $\mathbf {\alpha }_{j}$.  The aligned phrases are fed into another feedforward network  $G$  :bf^{ = G {a};bf {\ $\mathbf {v}_{j}^{b} = G([\mathbf {w}_{j}^{b};  \mathbf {\alpha }_{j}])$  to generate sets $\bf {i}^{a}\rbrace $ and $\lbrace \mathbf {v}_{j}^{b}\rbrace $, which are aggregated by summation and then concatenated together for classification.  The Enhanced Sequential Inference Model (ESIM)The Enhanced Sequential Inference Model BIBREF0 is closely related to the DecAtt model, but it differs in a few aspects.  First, Chen et al.  Chen-Qian:2017:ACL demonstrated that using Bi-LSTM to encode sequential contexts is important for performance improvement. They used the concatenation $\overline{\mathbf {w}}_{i} = \overleftrightarrow{\mathbf {h}}_{i} = [\overrightarrow{\mathbf {h}}_{i}, \overleftarrow{\mathbf {h}}_{i}]$of both directions as in the PWIM model.  The word alignment $\mathbf {\beta }_{i}$ and $\mathbf {\alpha }_{j}$ between $\overline{\mathbf {w}}^{a}$ and $\overline{\mathbf {w}}^{b}$ are calculated the same way as in DecAtt.  Second, they showed the competitive performance of recursive architecture with constituency parsing, which complements with sequential LSTM.  The feedforward function $G$ in DecAtt is replaced with Tree-LSTM:$$^{ = TreeLSTM([\overline{\mathbf {w}}_{i}^{a};  \mathbf {\beta }_{i};  \overline{\mathbf {w}}_{i}^{a}-\mathbf {\beta }_{i};\overline{\mathbf {w}}_{i}^{a} \odot \mathbf {\beta }_{i}])\\  &\mathbf {v}_{j}^{b} = TreeLSTM([\overline{\mathbf {w}}_{j}^{b};  \mathbf {\alpha }_{j};  \overline{\mathbf {w}}_{j}^{b}-\mathbf {\alpha }_{j};\bf^{ \}$$. , using in aggregation, ESIM adapts the average and max pooling and concatenation $\mathbf {v}= [\mathbf {v}_{ave}^{a};\mathbf {v}_{max}^{a};  \mathbf {v}_{ave}^{b};  \mathbf {v}_{max}^{b}]$  before passing through multi-layer perceptron (MLP)for classification:  $$bf {_{^{_{ = \max _{i=1}^{m}\mathbf {v}_{i}^{a},\ \bf_{^{}_{_{j}^{b}$$    (Eq. 26)Datasets  We conducted sentence pair modeling experiments on eight popular datasets:two NLI datasets, three PI datasets, one STS dataset and two QA datasets.  Table 2 gives a comparison of these datasets:  SNLI BIBREF7 contains 570k hypotheses written by crowdsourcing workers given the premises.It focuses on three semantic relations:  the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are unrelated (neutral).  Multi-NLI BIBREF33 extends the SNLI corpus to multiple genres of written and spoken texts with 433k sentence pairs.Quora BIBREF34 contains 400k question pairs collected from the Quora website.  This dataset has balanced positive and negative labels indicating whether the questions are duplicated or not.Twitter-URL BIBREF35 includes 50k sentence pairs collected from tweets that share the same URL of news articles.  This dataset contains both formal and informal language.  PIT-2015 BIBREF5 comes from SemEval-2015 and was collected from tweets under the same trending topic.It contains naturally occurred. by independent users) and-paraphrases with varied topics and language styles.  STS-2014 BIBREF36 is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes BIBREF37.WikiQA  BIBREF8 is an open-domain question-answering dataset.  Following He and Lin he-lin:2016:N16-1, questions without correct candidate answer sentences are excluded, and answer sentences are truncated to 40 tokens, resulting in 12k question-answer pairs for our experiments.TrecQA BIBREF38 is an answer selection task of 56k question-answer pairs and created in Text Retrieval Conferences (TREC).  For both WikiQA and TrecQA datasets, the best answer is selected according to the semantic relatedness with the question.  Implementation DetailsWe implement all the models with the same PyTorch framework.  Below, we summarize the implementation details that are key for reproducing results for each model:SSE:  This model can converge very fast, for example, 2 or 3 epochs for the SNLI dataset.  We control the convergence speed by updating the learning rate for each epoch:  specifically,$lr=\frac{1}{2^{\frac{epoch\_i}{2}}}*{init\_lr}$  , where $init\_lr$ is the initial learning rate and $epoch\_i$ is the index of current epoch.  DecAtt: to use gradient clipping for this model:  for each gradient we the all the gradient values, if it is greater than a threshold $b$, we scale the gradient by a factor $\alpha = b/L2\_norm$ .  Another useful procedure is to assemble batches of sentences with similar length.ES:  Similar from Dec,IM with uses to padding the trees withinMursion, we follow Bowman et al.'s bowman-EtAl:2016:P16-1 procedure that converts tree structures into the linear sequential structure of a shift reduce parser.Two additional masks are used for producing left and right children of a tree node.  PWIM:  The cosine and Euclidean distances used in the word interaction layer have smaller values for similar vectors while dot products have larger values.The performance increases if we add a negative sign to make all the vector similarity measurements behave consistently.  Analysis Table 3 and 4 show the results reported in the original papers and the replicated results with our implementation.We use accuracy, F1 score, Pearson's $r$ , Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR) for evaluation on different datasets following the literature.  Our reproduced results are slightly lower than the original results by 0.5 $\sim $ 1.5 points on accuracy.  We suspect the following potential reasons:  (i) less extensive hyperparameter tuning for each individual dataset; (ii) only one run with random seeding to report results; and (iii) use of different neural network toolkits:for example, the original ESIM model was implemented with Theano, and PWIM model was in Torch.  Herein, we examine the main components that account for performance in sentence pair modeling.How important is LSTM encoded context information for sentence pair modeling?  Regarding DecAtt, Parikh et al.  parikh-EtAl:2016:EMNLP2016 mentioned that âintra-sentence attention is optional"; they can achieve competitive results without considering context information., not, our experiments that encoding sequential information with is critical.   to Dec,IM shows on every dataset ( Table 4 3  TheAtt that to improvement, we ofM and Tree-LSTM for sentence encoding, rather than the different choices of aggregation functions. does-L with?    henian:L simple combination (IM $_{seq+ ) by the predictionIM sequential and suggestedï¿½arsing information complements very well with ESIM and further improves the performance".However, we found that adding Tree-LSTM only helps slightly or not at all for most datasets, but it helps noticably with the two Twitter paraphrase datasets.  We hypothesize the reason is that these two datasets come from real-world tweets which often contain extraneous text fragments, in contrast to SNLI and other datasets that have sentences written by crowdsourcing workers.  For example, the segment âever wondered ,â in the sentence pair ever wondered , why your recorded #voice sounds weird to you?  and why do our recorded voices sound so weird to us?introduces a disruptive context into the Bi-LSTM encoder, while Tree-LSTM can put it in a less important position after constituency parsing.  How important is attentive interaction for sentence pair modeling?  Why does SSE excel on Quora?Both ESIM and DecAtt (Eq. 23 ) calculate an attention-based soft alignment between a sentence pair, which was also proposed in BIBREF27 and BIBREF29 for sentence pair modeling, whereas PWIM utilizes a hard attention mechanism.  Both attention strategies are critical for model performance.  In PWIM model BIBREF1 , we observed a 1 $\sim $ 2 point performance drop after removing the hard attention, 0 $\sim $ 3 point performance drop and $\sim $ 25% training time reduction after removing the 19-layer CNN aggregation.Likely without even the authors of SSE knowing, the SSE model performs extraordinarily well on the Quora corpus, perhaps because Quora contains many sentence pairs with less complicated inter-sentence interactions (e.g., many identical words in the two sentences) and incorrect ground truth labels (e.g., What is your biggest regret in life? and What's the biggest regret you've had in life?  are labeled as non-duplicate questions by mistake).  Figure 3 shows the learning curves.The DecAtt model converges quickly and performs well on large NLI datasets due to its design simplicity.  PWIM is the slowest model (see time comparison in Table 5 ) but shows very strong performance on semantic similarity and paraphrase identification datasets.  ESIM and SSE keep a good balance between training time and performance.[3] This number was reported in BIBREF12 by co-authors of DecAtt BIBREF10 .  [4] This number was reproduced by Williams et al. williams2017broad.[5]This number was generated by InferSent traind on SNLI and Multi-NLI datasets.  As shown in Figure 4 , we experimented with different training sizes of the largest SNLI dataset.  All the models show improved performance as we increase the training size.ESIM and SSE have similar and clearlyform PW on the SNLI dataset.  DecAtt shows a performance jump when the training size exceeds a threshold.  We conducted an in-depth analysis of model performance on the Multi-domain NLI dataset based on different categories: text genre, sentence pair overlap, and sentence length.As shown in Table 6 , all models have comparable performance between matched genre and unmatched genre.  Sentence length and overlap turn out to be two important factors â the longer the sentences and the fewer tokens in common, the more challenging it is to determine their semantic relationship.  These phenomena shared by the state-of-the-art systems reflect their similar design framework which is symmetric at processing both sentences in the pair, while question answering and natural language inference tasks are directional BIBREF30 .  How to incorporate asymmetry into model design will be worth more exploration in future research. the-domain study (Table 6 conducted transfer on threeraseTable ).The most noteworthy phenomenon is that the SSE model performs better on Twitter-URL and PIT-2015 when trained on the large out-of-domain Quora data than the small in-domain training data.  Two likely reasons are:(i) the SSE model with over 29 million parameters is data hungry and (ii) SSE model is a sentence encoding model, which generalizes better across domains/tasks than sentence pair interaction models.  Sentence pair interaction models may encounter difficulties on Quora, which contains sentence pairs with the highest word overlap (51.5%) among all datasets and often causes the interaction patterns to focus on a few key words that differ.  In contrast, the Twitter-URL dataset has the lowest overlap (23.0%) with a semantic relationship that is mainly based on the intention of the tweets.Conclusion We analyzed different neural models ( their for sentence pair and experiments with eight datasets for different.We the theM and attentive forence the learning of sentence encoding basedWe showed that the SNLI corpus of over 550k sentence pairs cannot saturate the learning curve.We systematically compared the strengths and weaknesses of different network designs and provided insights for future work.  AcknowledgementsWe thank Ohio Center BIBREF for computing. byF CRII award (RI-17558 andPA the ARO (--00). of the information not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred.Pretrained Word Embeddings  We used the 200-dimensional GloVe word vectors BIBREF18 , trained on 27 billion words from Twitter (vocabulary size of 1.2 milion words) for Twitter URL BIBREF35 and PIT-2015 BIBREF5 datasets, and the 300-dimensional GloVe vectors, trained on 840 billion words (vocabulary size of 2.2 milion words) from Common Crawl for all other datasets.  For out-of-vocabulary words, we initialized the word vectors using normal distribution with mean 0 and deviation 1.Hyper-parameter Settings  We followed original papers or code implementations to set hyper-parameters for these models.In Infersent model BIBREF16 , the hidden dimension size for Bi-LSTM is 2048, and the fully connected layers have 512 hidden units.  In SSE model BIBREF17 , the hidden size for three Bi-LSTMs is 512, 2014 and 2048, respectively.  The fully connected layers have 1600 units.  PWIM BIBREF1 and ESIM BIBREF0 both use Bi-LSTM for context encoding, having 200 hidden units and 300 hidden units respectively.The DecAtt model BIBREF10 uses three kinds of feed forward networks, all of which have 300 hidden units.  Other parameters like learning rate, batch size, dropout rate, and all of them use the same settings as in original papers.  Fine-tuning the ModelsIt is not practical to fine tune every hyper-parameter in every model and every dataset, since we want to show how these models can generalize well on other datasets, we need try to avoid fine-tuning these parameters on some specific datasets, otherwise we can easily get over-fitted models.  Therefore, we keep the hyper-parameters unchanged across different datasets, to demonstrate the generalization capability of each model.  The default number of epochs for training these models is set to 20, if some models could converge earlier (no more performance gain on development set), we would stop running them before they approached epoch 20.  The 20 epochs can guarantee every model get converged on every dataset.... [The rest of the article is omitted]

Question:
Do the authors also analyze transformer-based architectures?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

