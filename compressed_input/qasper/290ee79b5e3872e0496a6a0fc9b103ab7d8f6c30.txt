<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction  Offensive content has become pervasive in social media and a reason of concern for government organizations, online communities, and social media platforms.  One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which then can be deleted or set aside for human moderation. the the of to.Most a of offensive language as language BIBREF0, BIBREF1, (cyber-)aggression BIBREF2, (cyber-)bullying  BIBREF3,  BIBREF4,toxic comments  INLINEFORM0, hate speech  BIBREF5,  BIBREF6,BREF7, BIBREF9, BIBREF10,  and offensive language BIBREF11.  Prior work has focused on these aspects of offensive language in Twitter  BIBREF3, BIBREF7, BIBREF8, BIBREF11, Wikipedia comments, and Facebook postsBIBREF2 .  Recently, Waseem et. al.  ( BIBREF12 ) acknowledged the similarities among prior work and discussed the need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity or towards a generalized group and whether the abusive content is explicit or implicit.Wand.  ( BIBREF ) this trend well on German. their they a detect offensive vs a task for tweets as , no has the when studying hate speech with respect to a specific target.Therefore, we expand on these ideas by proposing a a hierarchical three-level annotation model that encompasses:  Using this annotation model, we create a new large publicly available dataset of English tweets.The key contributions of this paper are as follows:  Related WorkDifferent abusive and offense language identification sub-tasks have been explored in the past few years including aggression identification, bullying detection, hate speech, toxic comments, and offensive language.  Aggression identification:  The TRAC shared task on Aggression IdentificationBIBREF2 provided a containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation.  For testing, two different sets, one from Facebook and one from Twitter were provided.  Systems were trained to discriminate between three classes: non-aggressive, covertly aggressive, and overtly aggressive.Bullying detection:  Several studies have been published on bullying detection.  One of them is the one by xu2012learning which apply sentiment analysis to detect bullying in tweets.  xu2012learning use topic models to to identify relevant topics in bullying.Another related study is the one by dadvar2013improving which use user-related features such as the frequency of profanity in previous messages to improve bullying detection.  Hate speech identification:  It is perhaps the most widespread abusive language detection sub-task. this-task asok2013l and2015hate who a between `',  d2017ated the speech detection dataset containing over 24,000 English tweets labeled as non offensive, hate speech, and profanity.Offensive:The Germval BIBREF11 task on language over500 for a-ained task which to offensive and non-offensive tweets and a second task where the organizers broke down the offensive class into three classes: profanity, insult, and abuse.Toxic:  The Comment Challenge competition atle which Wikipedia labeled in:,,,, hate.  While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model proposed in this paper aims to capture this.Considering that, for example, an insult targeted at an individual is commonly known as cyberbulling and that insults targeted at a group are known as hate speech, we pose that OLID's hierarchical annotation model makes it a useful resource for various offensive language identification sub-tasks.  Hierarchically Modelling Offensive Content the OLID a hierarchical model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language.  Each level is described in more detail in the following subsections and examples are shown in Table TABREF10.  Level A:Offensive language Detection Level A discriminates between offensive (OFF) and non-offensive (NOT) tweets.  Not Offensive (NOT):Posts that do not contain offense or profanity;  Offensive (OFF):  We label a post as offensive if it contains any form of non-acceptable language (profanity) or a targeted offense, which can be veiled or direct.This category includes insults, threats, and posts containing profane language or swear words.  Level B:  Categorization of Offensive LanguageLevel B categorizes the type of offense and two labels are used:  targeted (TIN) and untargeted (INT) insults and threats.Targeted Insult (TIN):  Posts which contain an insult/threat to an individual, group, or others (see next layer);  Untargeted (UNT):Posts containing non-targeted profanity and swearing.  Posts with general profanity are not targeted, but they contain non-acceptable language.  Level C:Offensive Language  Target Identification  Level C categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH).Individual (IND):  Posts targeting an individual.  It can be a a famous person, a named individual or an unnamed participant in the conversation.Insults and threats targeted at individuals are often defined as cyberbulling.  Group (GRP):  The target of these offensive posts is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or other common characteristic.Many of the insults and threats targeted at a group correspond to what is commonly understood as hate speech.  Other (OTH):  The target of these offensive posts does not belong to any of the previous two categories (e.g. an organization, a situation, an event, or an issue).Data Collection  The data included in OLID has been collected from Twitter.We retrieved the data using the Twitter API by searching for keywords and constructions that are often included in offensive messages, such as `she is' or `to:BreitBartNews'.  We carried out a first round of trial annotation of 300 instances with six experts.  The goal of the trial annotation was to 1) evaluate the proposed tagset; 2) evaluate the data retrieval method; and 3) create a gold standard with instances that could be used as test questions in the training and test setting annotation which was carried out using crowdsourcing.  The breakdown of keywords and their offensive content in the trial data of 300 tweets is shown in Table TABREF14 .We included a left (@NewYorker) and far-right (@BreitBartNews) news accounts because there tends to be political offense in the comments.  One of the best offensive keywords was tweets that were flagged as not being safe by the Twitter `safe' filter (the `-' indicates `not safe').  The vast majority of content on Twitter is not offensive so we tried different strategies to keep a reasonable number of tweets in the offensive class amounting to around 30% of the dataset including excluding some keywords that were not high in offensive content such as `they are` and `to:NewYorker`.  Although `he is' is lower in offensive content we kept it as a keyword to avoid gender bias.In addition to the keywords in the trial set, we searched for more political keywords which tend to be higher in offensive content, and sampled our dataset such that 50% of the the tweets come from political keywords and 50% come from non-political keywords.  In addition to the keywords `gun control', and `to:BreitbartNews', political keywords used to collect these tweets are `MAGA', `antifa', `conservative' and `liberal'.  We computed Fliess'  INLINEFORM0 on the trial set for the five annotators on 21 of the tweets.INLINEFORM1 is. for (OFF vs) to and no user or andWe follow in (2015, and using platform Figure Eight.  We ensure data quality by:) we only received from were experienced in the platform; and  2) we used test questions to discard annotations of individuals who did not reach a certain threshold.  Each instance in the dataset was annotated by multiple annotators and inter-annotator agreement has been calculated.  We first acquired two annotations for each instance.In of% we considered as acceptable we requested above%.  After the crowdsourcing annotation, we used expert adjudication to guarantee the quality of the annotation.  The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15.Experiments and Evaluation  We assess our dataset using traditional and deep learning methods.  Our simplest model is a linear SVM trained on word unigrams.SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 .  We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead.  It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features.  The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function.We set two input inputding:  trainedings BIBREF14 as   also a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15, using the same multi-channel inputs as the above BiLSTM.Our are and evaluated by the.The distribution is in TableABREFWe evaluate the the label  class Precision1-score (F1), also with other averaged metrics are also reported.The models are compared against baselines of predicting all labels as the majority or minority classes.  Offensive Language DetectionThe performance on between offensive () is Table TABREF18.  We can see that all systems perform significantly better than chance, with the neural models being substantially better than the SVM.  The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80.Categorization of Offensive Language  In this experiment, the two systems were trained to discriminate between insults and threats (TIN) and untargeted (UNT) offenses, which generally refer to profanity.  The results are shown in Table TABREF19 .The CNN system achieved higher performance in this experiment compared to the BiLSTM, with a macro-F1 score of 0.69.  All systems performed better at identifying target and threats (TIN) than untargeted offenses (UNT).Offensive Language Target Identification  The results of the offensive target identification experiment are reported in Table TABREF20 .  Here the systems were trained to distinguish between three targets: a group (GRP), an individual (IND), or others (OTH).All three models achieved similar results far surpassing the random baselines, with a slight performance edge for the neural models.  The performance of all systems for the OTH class is 0.  This poor performances can be explained by two main factors., the other, O a of includes offensive tweets at,,. for learnative of. contains  There are only 395 instances in OTH, and 1,075 in GRP, and 2,407 in IND.Conclusion and Future Work  This paper presents OLID, a new dataset with annotation of type and target of offensive language.  OLID is the official dataset of the shared task SemEval 2019 Task 6:Identifying and Categorizing Offensive Language in Social Media (OffensEval)  BIBREF16 .  In OffensEval, each annotation level in OLID is an independent sub-task.  The dataset contains 14,100 tweets and is released freely to the research community.To the best of our knowledge, this is the first dataset to contain annotation of type and target of offenses in social media and it opens several new avenues for research in this area.  We present baseline experiments using SVMs and neural networks to identify the offensive tweets, discriminate between insults, threats, and profanity, and finally to identify the target of the offensive messages.  The results show that this is a challenging task.  A CNN-based sentence classifier achieved the best results in all three sub-tasks.In future work, we would like to make a cross-corpus comparison of OLID and datasets annotated for similar tasks such as aggression identification BIBREF2 and hate speech detection BIBREF8 .  This comparison is, however, far from trivial as the annotation of OLID is different.Acknowledgments  The research presented in this paper was partially supported by an ERAS fellowship awarded by the University of Wolverhampton.... [The rest of the article is omitted]

Question:
What are the three layers of the annotation scheme?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

