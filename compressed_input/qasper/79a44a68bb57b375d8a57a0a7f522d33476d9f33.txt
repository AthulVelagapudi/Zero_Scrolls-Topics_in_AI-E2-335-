<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant. Always follow the task instruction carefully. The first paragraph before the first double line break contains the task instruction. Generate text as a natural continuation of the user message. Do not include any meta-commentary or explanations.<|eot_id|>
<|start_header_id|>user<|end_header_id|>

You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write "unanswerable". If the question is a yes/no question, answer "yes", "no", or "unanswerable".

Article:
Introduction  Knowledge and/or data is often modeled in a structure, such as indexes, tables, key-value pairs, or triplets.  These data, by their nature (e.g., raw data or long time-series data), are not easily usable by humans; outlining their crucial need to be synthesized.Recently, numerous works have focused on leveraging structured data in various applications, such as question answering BIBREF0, BIBREF1 or table retrieval BIBREF2, BIBREF3.  One emerging research field consists in transcribing data-structures into natural language in order to ease their understandablity and their usablity.  This field is referred to as “data-to-text" BIBREF4 and has its place in several application domains (such as journalism BIBREF5 or medical diagnosis BIBREF6) or wide-audience applications (such as financial BIBREF7 and weather reports BIBREF8, or sport broadcasting BIBREF9, BIBREF10).  As an example, Figure FIGREF1 shows a data-structure containing statistics on NBA basketball games, paired with its corresponding journalistic description.Designing data-to-text models gives rise to two main challenges: 1) understanding structured data and 2) generating associated descriptions.  Recent data-to-text models BIBREF11, BIBREF12, BIBREF13, BIBREF10 mostly rely on an encoder-decoder architecture BIBREF14 in which the data-structure is first encoded sequentially into a fixed-size vectorial representation by an encoder. Then, a decoder generates words conditioned on this representation.  With the introduction of the attention mechanism BIBREF15 on one hand, which computes a context focused on important elements from the input at each decoding step and, on the other hand, the copy mechanism BIBREF16, BIBREF17 to deal with unknown or rare words, these systems produce fluent and domain comprehensive texts.For instance, Roberti et al.  BIBREF18 train a character-wise encoder-decoder to generate descriptions of restaurants based on their attributes, while Puduppully et al.  BIBREF12 design a more complex two-step decoder: they first generate a plan of elements to be mentioned, and then condition text generation on this plan.  Although previous work yield overall good results, we identify two important caveats, that hinder precision (i.e. factual mentions) in the descriptions:ear most works on introducingating decoding and still a the table Figure FIG1 be [(H H ( H/V, V),...], effectively leading to losing distinction between rows, and therefore entities.To the best of our knowledge, only Liu et al.  BIBREF19, BIBREF11 propose encoders constrained by the structure but these approaches are designed for single-entity structures.  Arbitrary ordering of unordered collections in recurrent networks (RNN).Most- use as (), these have.   require practice beThis encoding. of) an as by  BIBREF20, significantly impacts the learning performance. these propose a new structured-data encoder assuming that structures should be hierarchically captured.  Our contribution focuses on the encoding of the data-structure, thus the decoder is chosen to be a classical module as used in BIBREF12, BIBREF10.  Our contribution is threefold:We thelevel, encoding all entities their entities;We introduce the Transformer encoder BIBREF21 in data-to-text models to ensure robust encoding of each element/entities in comparison to all others, no matter their initial positioning;We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder.  We report experiments on the RotoWire benchmark BIBREF10 which contains around $5K$ statistical tables of NBA basketball games paired with human-written descriptions.Our is to several.   proposedforms on and is on qualitative metrics.  In the following, we first present a state-of-the art of data-to-text literature (Section 2), and then describe our proposed hierarchical data encoder (Section 3).The evaluation protocol is presented in Section 4, followed by the results (Section 5).  Section 6 concludes the paper and presents perspectives.  Related Work, bring out from structureddata knowledge BIBREFREF. to betterribe numerical data to a textual forecast, Reiter et al.  BIBREF8 devise complex template schemes in collaboration with weather experts to build a consistent set of data-to-word rules.Modern approaches to the wide range of tasks based on structured-data (e.g. table retrieval BIBREF2, BIBREF23, table classification BIBREF24, question answering BIBREF25) now propose to leverage progress in deep learning to represent these data into a semantic vector space (also called embedding space).  In parallel, an emerging task, called “data-to-text", aims at describing structured data into a natural language description.  This task stems from the neural machine translation (NMT) domain, and early work BIBREF26, BIBREF27, BIBREF10 represent the data records as a single sequence of facts to be entirely translated into natural language.W  BREF10 the traditional NMT larger structured extract workREF BIBREF12, BIBREF29 proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions.For example, Puduppully et al.  BIBREF12 propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation.  Similarly, Li et al.  BIBREF28 proposed a delayed copy mechanism where their decoder also acts in two steps:) a classicalM generate network BIBREF to by the , work BIBREF19, BIBREF13 have proposed to take into account the data structure.  More particularly, Puduppully et al.BIBREF13 follow-centric theoriesREF and on entity representation at. in each decoding step.  On the other hand, Liu et al.  BIBREF11, BIBREF19 rather focus on introducing structure into the encoder.For instance, they propose a dual encoder BIBREF19 which encodes separately the sequence of element names and the sequence of element values.  These approaches are however designed for single-entity data structures and do not account for delimitation between entities.  Our contribution differs from previous work in several aspects.First, instead of flatly concatenating elements from the data-structure and encoding them as a sequence BIBREF11, BIBREF12, BIBREF10, we constrain the encoding to the underlying structure of the input data, so that the delimitation between entities remains clear throughout the process.  Second, unlike all works in the domain, we exploit the Transformer architecture BIBREF21 and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering.  Finally, in contrast to BIBREF33, BIBREF13 that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities, we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention.Hierarchical Encoder Model for Data-to-Text  In this section we introduce our proposed hierarchical model taking into account the data structure.  We outline that the decoding component aiming to generate descriptions is considered as a black-box module so that our contribution is focused on the encoding module.We first describe the model overview, before detailing the hierarchical encoder and the associated hierarchical attention.  Hierarchical Encoder Model for Data-to-Text :::  Notation and General OverviewLet's consider the following notations:  $\bullet $An entity $e$ is a set of $J_i$ unordered records  $\lbrace r_{i,1},..., r_{i,j},..., r_{i,J_i}\rbrace $;  where record $r_{i,j}$ is defined as a pair of key $k_{i,j}$ and value $v_{i,j}$.  We outline that $J_i$ might differ between entities.$\bullet $  A data-structure $s$ is an unordered set of $I$ entities $e_i$.  We thus denote $s \lbrace e_1,..., e_i,..., e_I\rbrace $.$\bullet $  For each data-structure, a textual description $y$ is associated.  We refer to the first $t$ words of a description $y$ as $y_{1:t}$.Thus, the full sequence of words can be noted as $y = y_{1:T}$.  $\bullet $  The dataset $\mathcal {D}$ is a collection of $N$ aligned (data-structure, description) pairs $(s,y)$., Figure FIG aructure a. (H,, Al Horford, Jeff Teague,...).  The entity Jeff Teague is modeled as a set of records {(PTS, 17), (REB, 0), (AST, 7)...}in which, the record (PTS, is a). each-ructure the function to a{$ function following theD}$:whereta $ the $y\ s the model to generate the adequate description $y$ for table $s$.  During inference, we generate the sequence $\hat{y}^*$ with the maximum a posteriori probability conditioned on table $s$.Using the chain rule, we get:  This equation is intractable in practice, we approximate a solution using beam search, as in BIBREF11, BIBREF19, BIBREF12, BIBREF13, BIBREF10.Our model follows the encoder-decoder architecture BIBREF14.  Because our contribution focuses on the encoding process, we chose the decoding module used in BIBREF12, BIBREF10:  a two-layers LSTM network with a copy mechanism.  In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input.We now describe the hierarchical encoder and the hierarchical attention.  Hierarchical Encoder Model for Data-to-Text :::  Hierarchical Encoding ModelAs outlined in Section SECREF2, most previous work BIBREF28, BIBREF12, BIBREF13, BIBREF10, BIBREF29 make use of flat encoders that do not exploit the data structure.  To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules.  The first one (module A in Figure FIGREF11) is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities. thelevel the traditional a embedding layer as in BIBREF11, BIBREF12, BIBREF10.  We present in what follows the record embedding layer and introduce our two hierarchical modules.  Hierarchical Encoder Model for Data-to-Text :::Hierarchical Encoding Model :::  Record Embedding Layer.  The first layer of the network consists in learning two embedding matrices to embed the record keys and values.Keys $k_{i,j}$ are embedded to $\mathbf {k}_{i,j} \in \mathbb {R}^{d}$ and values $v_{i,j}$ to $\mathbf {v}_{i,j} \in \mathbb {R}^{d}$, with $d$ the size of the embedding.  As in previous work BIBREF11, BIBREF12, BIBREF10, each record embedding $\mathbf {r}_{i,j}$ is computed by a linear projection on the concatenation $[\mathbf {k}_{i,j}$;  $\mathbf {v}_{i,j}]$ followed by a non linearity:where $\mathbf {W}_r \in \mathbb {R}^{2d \times d}$ and $\mathbf {b}_r \in \mathbb {R}^{d}$ are learnt parameters.  The low-level encoder aims at encoding a collection of records belonging to the same entity while the high-level encoder encodes the whole set of entities.  Both the low-level and high-level encoders consider their input elements as unordered.We use the Transformer architecture from BIBREF21.  For each encoder, we have the following peculiarities:  the Low-level encoder encodes each entity $e_i$ on the basis of its record embeddings $\mathbf {r}_{i,j}$.Each record embedding $\mathbf {r}_{i,j}$ is compared to other record embeddings to learn its final hidden representation $\mathbf {h}_{i,j}$.  Furthermore, we add a special record [ENT] for each entity, illustrated in Figure FIGREF11 as the last record.  Since entities might have a variable number of records, this token allows to aggregate final hidden record representations $\lbrace \mathbf {h}_{i,j}\rbrace _{j=1}^{J_i}$ in a fixed-sized representation vector $\mathbf {h}_{i}$.the High-level encoder encodes the data-structure on the basis of its entity representation $\mathbf {h}_{i}$.  Similarly to the Low-level encoder, the final hidden state $\mathbf {e_i}$ of an entity is computed by comparing entity representation $\mathbf {h}_{i}$ with each others.  The data-structure representation $\mathbf {z}$ is computed as the mean of these entity representations, and is used for the decoder initialization.Hierarchical Encoder Model for Data-to-Text :::  Hierarchical attention  To fully leverage the hierarchical structure of our encoder, we propose two variants of hierarchical attention mechanism to compute the context fed to the decoder module.$\bullet $  Traditional Hierarchical Attention.  As in BIBREF13, we hypothesize that a dynamic context should be computed in two steps:first attending to entities, then to records corresponding to these entities.  To implement this hierarchical attention, at each decoding step $t$, the model learns a first set of attention scores $\alpha _{i,t}$ over entities $e_i$ and a second set of attention scores $\beta _{i,j,t}$ over records $r_{i,j}$ belonging to entity $e_i$.  The $\alpha _{i,t}$ scores are normalized to form a distribution over all entities $e_i$, and $\beta _{i,j,t}$ scores are normalized to form a distribution over records $r_{i,j}$ of entity $e_i$.  Each entity is then represented as a weighted sum of its record embeddings, and the entire data structure is represented as a weighted sum of the entity representations.The dynamic context is computed as:  ct = i=1I (i,t ( j i,j,t ri,j )),t(dtdt,)wherebfd the\alpha } \in \mathbb {R}^{d\times d}$ and $\mathbf {W}_{\beta }\inbb are,$ \i\alpha _{i,t} = 1$, and for all $i \in \lbrace 1,...,I\rbrace $  $\sum _{j}\beta _{i,j,t} = 1$.\ $  -guided variant the that for ( to the type of records is important to determine the content of the description.  For example, when deciding to mention a player, all experts automatically report his score without consideration of its specific value.To this we the by thebeta (SECREF) the key full record}$:  Please note that the different embeddings and the model parameters presented in the model components are learnt using Equation 1.Experimental setup :::  The Rotowire dataset  To evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data-structure made of several types of entities, we used the RotoWire dataset BIBREF10.It includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of Figure FIGREF1.  The descriptions are professionally written and average 337 words with a vocabulary size of $11.3$K.  There are 39 different record keys, and the average number of records (resp. entities) in a single data-structure is 628 (resp. 28).  Entities are of two types, either team or player, and player descriptions depend on their involvement in the game.We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively $3,398$/727/728 (data-structure, description) pairs.  Experimental setup :::  Evaluation metricsWe evaluate our model through two types of metrics.  The BLEU score BIBREF34 aims at measuring to what extent the generated descriptions are literally closed to the ground truth.  The second category designed by BIBREF10 is more qualitative.Experimental setup :::  Evaluation metrics :::  BLEU Score.The B score BIBREF is in It the a machine and ofrencessn 3, 4}$) between the generated candidate and the ground truth.  We use the implementation code released by BIBREF35.Experimental setup :::  Evaluation metrics :::  Information extraction-oriented metrics.These metrics the of to integrate the its   they the measure the extracted are aligned or we the in BIBREF, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset.Entity-value pairs are extracted from the descriptions.  For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]. , an IE tool will extract the pair (Isaiah Thomas, 23, PTS).  Second, we compute three metrics on the extracted information:\ $  ation) generate text containing factual) and numberdenoted respectively RG-P% and RG-#) of unique relations $r$ extracted from $\hat{y}_{1:T}$ that also appear in $s$.\let $  Content () the the of mentionedoted respectively-P% and CS-R%) of unique relations $r$ extracted from $\hat{y}_{1:T}$ that are also extracted from $y_{1:T}$.\ $  Content () the records theWe measure the normalized Damerau-Levenshtein distance BIBREF36 between the sequences of records extracted from $\hat{y}_{1:T}$ that are also extracted from $y_{1:T}$.CS the� say of, CO and.   for RG- and BU; which is not true for RG-#.  The IE system used in the experiments is able to extract an average of 17 factual records from gold descriptions.In order to mimic a human expert, a generative system should approach this number and not overload generation with brute facts.  Experimental setup :::  BaselinesWe compare our hierarchical model against three systems.  For each of them, we report the results of the best performing models presented in each paper.$\bullet $  Wiseman BIBREF10 is a standard encoder-decoder system with copy mechanism.  $\bullet $Li  BIBREF28 is a standard encoder-decoder with a delayed copy mechanism:  text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network.$\bullet $  Puduppully-plan  BIBREF12 acts in two steps:  a first standard encoder-decoder generates a plan, i.e. a list of salient records from the table;a second standard encoder-decoder generates text from this plan.  $\bullet $  Puduppully-updt BIBREF13.It consists in a standard encoder-decoder, with an added module aimed at updating record representations during the generation process.  At each decoding step, a gated recurrent network computes which records should be updated and what should be their new representation.  Experimental setup :::Baselines :::  Model scenarios  We test the importance of the input structure by training different variants of the proposed architecture:$\bullet $  Flat, where we feed the input sequentially to the encoder, losing all notion of hierarchy.  As a consequence, the model uses standard attention.This is to that a   $  Hierarchical-kv is our full hierarchical model, with traditional hierarchical attention, i.e. where attention over records is computed on the full record encoding, as in equation (SECREF16).$\bullet $  Hierarchical-k is our full hierarchical model, with key-guided hierarchical attention, i.e. where attention over records is computed only on the record key representations, as in equation (DISPLAY_FORM17).Experimental setup :::  Implementation details  The decoder is the one used in BIBREF12, BIBREF13, BIBREF10 with the same hyper-parameters. theoder, the aayersention with two.  To fit the of record (), toThe the record and the to useout at 0.5.The models are trained with a batch size of 64.  We follow the training procedure in BIBREF21 and train the model for a fixed number of 25K updates, and average the weights of the last 5 checkpoints (at every 1K updates) to ensure more stability across runs.  All models were trained with the Adam optimizer BIBREF37; the initial learning rate is 0.001, and is reduced by half every 10K steps.  We used beam search with beam size of 5 during inference.All the models are implemented in OpenNMT-py BIBREF38.  All code is available at https://github.com/KaijuML/data-to-text-hierarchical ResultsOur results the RWire test TableABREF. each proposed over in   toelines BREFREF and our  We also report the result of the oracle (metrics on the gold descriptions).Please note that gold descriptions trivially obtain 100% on all metrics expect RG, as they are all based on comparison with themselves.  RG scores are different, as the IE system is imperfect and fails to extract accurate entities 4% of the time.  RG-# is an absolute count.Results :::  Ablation studies  To evaluate the impact of our model components, we first compare scenarios Flat, Hierarchical-k, and Hierarchical-kv.As shown in Table TABREF25, we can see the lower results obtained by the Flat scenario compared to the other scenarios (e.g. BLEU $16.7$ vs. $17.5$ for resp. Flat and Hierarchical-k), suggesting the effectiveness of encoding the data-structure using a hierarchy.  This is expected, as losing explicit delimitation between entities makes it harder a) for the encoder to encode semantics of the objects contained in the table and b) for the attention mechanism to extract salient entities/records.  Second, the comparison between scenario Hierarchical-kv and Hierarchical-k shows that omitting entirely the influence of the record values in the attention mechanism is more effective: this last variant performs slightly better in all metrics excepted CS-R%, reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section SECREF16:once an entity is selected, facts about this entity are relevant based on their key, not value which might add noise.  To illustrate this intuition, we depict in Figure FIGREF27 attention scores (recall $\alpha _{i,t}$ and $\beta _{i,j,t}$ from equations (SECREF16) and (DISPLAY_FORM17)) for both variants Hierarchical-kv and Hierarchical-k.  We particularly focus on the timestamp where the models should mention the number of points scored during the first quarter of the game.  Scores of Hierarchical-k are sharp, with all of the weight on the correct record (PTS_QTR1, 26) whereas scores of Hierarchical-kv are more distributed over all PTS_QTR records, ultimately failing to retrieve the correct one.Results :::  Comparison w.r.t. baselines.From a general point of view, we can see from Table TABREF25 that our scenarios obtain significantly higher results in terms of BLEU over all models; our best model Hierarchical-k reaching $17.5$ vs. $16.5$ against the best baseline.  This means that our models learns to generate fluent sequences of words, close to the gold descriptions, adequately picking up on domain lingo.  Qualitative metrics are either better or on par with baselines.  We show in Figure FIGREF29 a text generated by our best model, which can be directly compared to the gold description in Figure FIGREF1.Generation is fluent and contains domain-specific expressions.  As reflected in Table TABREF25, the number of correct mentions (in green) outweights the number of incorrect mentions (in red).  Please note that, as in previous work BIBREF28, BIBREF12, BIBREF13, BIBREF10, generated texts still contain a number of incorrect facts, as well hallucinations (in blue):  sentences that have no basis in the input data (e.g. “[...] he's now averaging 22 points [...].").While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.  Specifically, regarding all baselines, we can outline the following statements.$\bullet $  Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman, reinforcing the crucial role of structure in data semantics and saliency.  The analysis of RG metrics shows that Wiseman seems to be the more naturalistic in terms of number of factual mentions (RG#) since it is the closest scenario to the gold value (16.83 vs. 17.31 for resp. Wiseman and Hierarchical-k).  However, Wiseman achieves only $75.62$% of precision, effectively mentioning on average a total of $22.25$ records (wrong or accurate), where our model Hierarchical-k scores a precision of $89.46$%, leading to $23.66$ total mentions, just slightly above Wiseman.$\bullet $  The comparison between the Flat scenario and Wiseman is particularly interesting.  Indeed, these two models share the same intuition to flatten the data-structure.The difference theoder: . for and Flat  Results that our Flat scenario and generates fluent with accurate (P also ( that promising to for structure.$\bullet $  Our hierarchical models outperform the two-step decoders of Li and Puduppully-plan on both BLEU and all qualitative metrics, showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder (i.e., planning or templating).  While our models sensibly outperform in precision at factual mentions, the baseline Puduppully-plan reaches $34.28$ mentions on average, showing that incorporating modules dedicated to entity extraction leads to over-focusing on entities; contrasting with our models that learn to generate more balanced descriptions. $The with Puduppully-updt shows that dynamically updating the encoding across the generation process can lead to better Content Ordering (CO) and RG-P%.  However, this does not help with Content Selection (CS) since our best model Hierarchical-k obtains slightly better scores.Indeed, Puduppully-updt updates representations after each mention allowing to keep track of the mention history.  This guides the ordering of mentions (CO metric), each step limiting more the number of candidate mentions (increasing RG-P%).  In contrast, our model encodes saliency among records/entities more effectively (CS metric).  We note that while our model encodes the data-structure once and for all, Puduppully-updt recomputes, via the updates, the encoding at each step and therefore significantly increases computation complexity. their-# of $.11 we that is andtains fluent with accurate more.We to the of by those our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions).This outlines the effectiveness in the design of our model relying on a structure encoding, in contrast to other approach that try to learn the structure of data/descriptions from a linearized encoding.  Conclusion and future work this have a hierarchical for which) to form efficient of;) has strong the attention of itsThis and more   the RotoWireforms terms of BLEU score and is generally better on qualitative metrics.This of representing structured databases may to and by.This be by recent-guided networks BIBREF can to erroneous even interesting to constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions.Acknowledgements  We would like to thank the H2020 project AI4EU (825619) which partially supports Laure Soulier and Patrick Gallinari.... [The rest of the article is omitted]

Question:
Which qualitative metric are used for evaluation?

Answer:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

