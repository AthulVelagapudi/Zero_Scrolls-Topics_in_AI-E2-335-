{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import set_seed as hf_set_seed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path to LM-Infinite folder\n",
    "lm_infinite_root = os.path.abspath('..')\n",
    "\n",
    "# Prepend it so Python finds 'models/llama.py' first\n",
    "sys.path.insert(0, lm_infinite_root)\n",
    "\n",
    "# Now you can import the converter\n",
    "from models.llama import convert_llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee836408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_prompt(user_message):\n",
    "    BEGIN = \"<|begin_of_text|>\"\n",
    "    START = \"<|start_header_id|>\"\n",
    "    END = \"<|end_header_id|>\"\n",
    "    EOT = \"<|eot_id|>\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"Always follow the task instruction carefully.\"\n",
    "        \"The first paragraph before the first double line break contains the task instruction.\"\n",
    "        \"Generate text as a natural continuation of the user message.\"\n",
    "        \"Do not include any meta-commentary or explanations or your own thoughts.\"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{BEGIN}\"\n",
    "        f\"{START}system{END}\\n\\n{system_prompt}{EOT}\\n\"\n",
    "        f\"{START}user{END}\\n\\n{user_message}{EOT}\\n\"\n",
    "        f\"{START}assistant{END}\\n\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "model_to_chat_template = {\n",
    "    \"/assets/models/meta-llama-3.2-instruct-3b\": llama3_prompt \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2993c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =[\"gov_report\", \"summ_screen_fd\", \"qmsum\", \"qasper\",\"narrative_qa\", \"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1568739",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_max_input_tokens = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfa16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc_keeping_suffix(tokenizer, tokenized_input_full, example, suffix_index, max_tokens, device):\n",
    "    seperator_and_suffix = f\"{example['truncation_seperator'].strip()}\\n\\n{example['input'][suffix_index:].strip()}\\n\"\n",
    "    tokenized_seperator_and_suffix = tokenizer(seperator_and_suffix, return_tensors=\"pt\").input_ids.to(device)\n",
    "    tokenized_input_trimmed = tokenized_input_full[:, :max_tokens - tokenized_seperator_and_suffix.shape[1]]\n",
    "    tokenized_input = torch.cat([tokenized_input_trimmed, tokenized_seperator_and_suffix], dim=1)\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/assets/models/meta-llama-3.2-instruct-3b\"\n",
    "model_print_name = \"llama-basic_4096\"\n",
    "max_examples_per_task = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_input(tokenizer, example, max_tokens, device):\n",
    "    instruction = example[\"input\"][:example['document_start_index']]\n",
    "    truncation_seperator = example['truncation_seperator']\n",
    "\n",
    "    query = example[\"input\"][example['query_start_index']:]\n",
    "    if len(query) == 0:\n",
    "        query = None\n",
    "    doc = example[\"input\"][example['document_start_index']\n",
    "        :example['document_end_index']]\n",
    "    \n",
    "    input_text = f\"{instruction}{doc}{truncation_seperator}{query or ''}\"\n",
    "    input = model_to_chat_template.get(model_name, lambda x: x)(input_text)\n",
    "    # print(f\"Input: {input}\")\n",
    "    tokenized_input = tokenizer(\n",
    "        input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed80d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n",
    "\n",
    "generations_dir = \"generations/ipynb\"\n",
    "seed = 43\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "hf_set_seed(seed)\n",
    "print(\"Params:\")\n",
    "print(f\"model: {model_name}\")\n",
    "generations_dir = os.path.join(generations_dir, model_print_name.replace(\"/\", \"_\"))\n",
    "print(f\"generations_dir: {generations_dir}\")\n",
    "print(f\"max_examples_per_task: {max_examples_per_task}\")\n",
    "print(\"=\" * 50)\n",
    "time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "print(f\"time as start: {time}\")\n",
    "\n",
    "print(\"Loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(f\"Loading model: {model_name}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "max_input_length = model_to_max_input_tokens\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_flash_attention_2=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model.past_key_values = None\n",
    "\n",
    "#model = convert_llama_model(model, local_branch=8192, global_branch=256, safe_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86112e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "\n",
    "print(f\"{model} model loaded!, device:{model.device}\")\n",
    "\n",
    "print(\"Will write to:\", generations_dir)\n",
    "os.makedirs(generations_dir, exist_ok=True)\n",
    "for dataset in datasets:\n",
    "    generations = dict()\n",
    "    input_task = dict()\n",
    "    output_task = dict()\n",
    "    print(f\"Processing {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time as start {dataset}: {time}\")\n",
    "    print(f\"Loading {dataset}\")\n",
    "    data = load_dataset(\"tau/zero_scrolls\", dataset, cache_dir=\"/home/athul/datasets_cache\")\n",
    "    print(f\"Loaded {dataset}\")\n",
    "\n",
    "    for i, example in tqdm(enumerate(data[\"validation\"])):\n",
    "        print(\"Processing example:\", example[\"id\"])\n",
    "\n",
    "        if 0 < max_examples_per_task == i:\n",
    "            print(f\"Reached {max_examples_per_task} for {dataset}. Breaking\")\n",
    "            break\n",
    "\n",
    "        model_input = process_model_input(tokenizer, example, max_input_length, device)\n",
    "\n",
    "        prediction_token_ids = model.generate(model_input,\n",
    "                                                  max_new_tokens=512,\n",
    "                                                  do_sample=False,\n",
    "                                                  top_p=0,\n",
    "                                                  top_k=0,\n",
    "                                                  temperature=1,\n",
    "                                                  pad_token_id=tokenizer.eos_token_id, )\n",
    "        model.past_key_values = None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        predicted_text = tokenizer.decode(prediction_token_ids[0][model_input.shape[1]:], skip_special_tokens=True)\n",
    "        generations[example[\"id\"]] = predicted_text\n",
    "        input_task[example[\"id\"]] = example[\"input\"]\n",
    "        output_task[example[\"id\"]] = example[\"output\"]\n",
    "        #break\n",
    "\n",
    "    out_file_path_pred = os.path.join(generations_dir, f\"{dataset}.json\")\n",
    "    with open(out_file_path_pred, 'w') as f_out:\n",
    "        json.dump(generations, f_out, indent=4)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Done generating {len(generations)} examples from {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time at end: {time}\")\n",
    "    print(f\"Look for predictions in {generations_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84721087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    out_file_path_pred = os.path.join(generations_dir, f\"{dataset}.json\")\n",
    "    with open(out_file_path_pred, 'w') as f_out:\n",
    "        json.dump(generations, f_out, indent=4)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"Done generating {len(generations)} examples from {dataset}\")\n",
    "    time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    print(f\"time at end: {time}\")\n",
    "    print(f\"Look for predictions in {generations_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8bf65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scrolls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
